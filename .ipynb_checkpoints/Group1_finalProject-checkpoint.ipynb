{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fbb218c-3c95-479d-87eb-b38ac4101ba9",
   "metadata": {},
   "source": [
    "# Group 1 Final Project Work "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b79343-85ed-4a30-b48f-4475c77a98e1",
   "metadata": {},
   "source": [
    "#### Specific data on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3387b813-213e-4b8d-b13c-01d8e44ecc7e",
   "metadata": {},
   "source": [
    "### Maternal Health Risk Dataset Summary\n",
    "\n",
    "**Shape:** 808 records × 7 columns  \n",
    "\n",
    "**Columns:**\n",
    "- `Age`\n",
    "- `SystolicBP` (Systolic Blood Pressure)\n",
    "- `DiastolicBP` (Diastolic Blood Pressure)\n",
    "- `BS` (Blood Sugar level)\n",
    "- `BodyTemp` (Body Temperature, °F)\n",
    "- `HeartRate` (Heart Rate, bpm)\n",
    "- `RiskLevel` (Target: maternal health risk category)\n",
    "\n",
    "---\n",
    "\n",
    "#### First 5 Records\n",
    "| Age | SystolicBP | DiastolicBP | BS   | BodyTemp | HeartRate | RiskLevel  |\n",
    "|-----|------------|--------------|------|----------|-----------|------------|\n",
    "| 25  | 130        | 80           | 15.0 | 98.0     | 86        | high risk  |\n",
    "| 35  | 140        | 90           | 13.0 | 98.0     | 70        | high risk  |\n",
    "| 29  | 90         | 70           | 8.0  | 100.0    | 80        | high risk  |\n",
    "| 30  | 140        | 85           | 7.0  | 98.0     | 70        | high risk  |\n",
    "| 35  | 120        | 60           | 6.1  | 98.0     | 76        | low risk   |\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary Statistics\n",
    "- **Age:** 10–70 years (mean = 30.6, std = 13.9)  \n",
    "- **SystolicBP:** 70–160 mmHg (mean = 113, std = 19.9)  \n",
    "- **DiastolicBP:** 49–100 mmHg (mean = 77.5, std = 14.8)  \n",
    "- **BS:** 6–19 mmol/L (mean = 9.26, std = 3.62)  \n",
    "- **BodyTemp:** 98–103 °F (mean = 98.6, std = 1.39)  \n",
    "- **HeartRate:** 7–90 bpm (mean = 74.3, std = 8.82)  \n",
    "\n",
    "---\n",
    "\n",
    "#### Target Variable: RiskLevel\n",
    "- **Low risk:** 478 records (~59.2%)  \n",
    "- **High risk:** 330 records (~40.8%)  \n",
    "- **Medium risk:** Not present in this dataset version  \n",
    "\n",
    " Note: The dataset is binary-labeled (low vs. high risk), so if a 3-class model (low/mid/high) is needed, additional data preprocessing or augmentation may be required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ac998-df87-4fa8-a23c-c32dd6250ae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T00:29:58.488110Z",
     "iopub.status.busy": "2025-09-20T00:29:58.487852Z",
     "iopub.status.idle": "2025-09-20T00:29:58.491576Z",
     "shell.execute_reply": "2025-09-20T00:29:58.490728Z",
     "shell.execute_reply.started": "2025-09-20T00:29:58.488088Z"
    }
   },
   "source": [
    "### Week 3 - Training and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d938a2-5f7a-4a04-b9a7-c4dd14f5aa77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T00:33:39.970066Z",
     "iopub.status.busy": "2025-09-20T00:33:39.969690Z",
     "iopub.status.idle": "2025-09-20T00:33:39.973857Z",
     "shell.execute_reply": "2025-09-20T00:33:39.972733Z",
     "shell.execute_reply.started": "2025-09-20T00:33:39.970039Z"
    }
   },
   "source": [
    "#### Environment (auto role + auto bucket) and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "204e533a-2bf3-413f-b03d-2bd34a912eea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:51:42.723774Z",
     "iopub.status.busy": "2025-09-30T20:51:42.723471Z",
     "iopub.status.idle": "2025-09-30T20:51:43.207946Z",
     "shell.execute_reply": "2025-09-30T20:51:43.207323Z",
     "shell.execute_reply.started": "2025-09-30T20:51:42.723751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Role:   arn:aws:iam::533267301342:role/LabRole\n",
      "S3:     s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143\n"
     ]
    }
   ],
   "source": [
    "# NO MANUAL SETTINGS: bucket/role are auto-detected from your Studio kernel.\n",
    "\n",
    "import os, io, json, time\n",
    "from pathlib import Path\n",
    "import boto3, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "# AWS context (auto from Studio kernel)\n",
    "boto_sess  = boto3.session.Session()\n",
    "region     = boto_sess.region_name\n",
    "sm_session = Session(boto_sess)\n",
    "role       = get_execution_role()\n",
    "bucket     = sm_session.default_bucket()\n",
    "\n",
    "RUN_ID    = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "S3_PREFIX = f\"aai540/maternal-risk/week3/{RUN_ID}\"\n",
    "\n",
    "# Paths\n",
    "DATA_CSV      = Path(\"Maternal_Risk.csv\")    # Using our Kaggle CSV here\n",
    "ARTIFACTS_DIR = Path(\"week3_outputs\"); ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:  \", role)\n",
    "print(\"S3:    \", f\"s3://{bucket}/{S3_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d4adc-f878-43ba-8088-6b1ae27302c2",
   "metadata": {},
   "source": [
    "#### Load data + lightweight EDA (plots + JSON summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8831eee7-cacf-4d1d-9ab8-722e439b677d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:51:43.682874Z",
     "iopub.status.busy": "2025-09-30T20:51:43.682424Z",
     "iopub.status.idle": "2025-09-30T20:51:44.635348Z",
     "shell.execute_reply": "2025-09-30T20:51:44.634567Z",
     "shell.execute_reply.started": "2025-09-30T20:51:43.682843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA done --> week3_outputs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 768x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert DATA_CSV.exists(), f\"Missing dataset at {DATA_CSV.resolve()}\"\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "\n",
    "# Keep a small EDA summary to feed the team tracker\n",
    "eda_summary = {\n",
    "    \"rows\": int(df.shape[0]),\n",
    "    \"cols\": int(df.shape[1]),\n",
    "    \"columns\": df.columns.tolist(),\n",
    "    \"dtypes\": {c: str(t) for c, t in df.dtypes.items()},\n",
    "    \"missing_counts\": df.isna().sum().to_dict(),\n",
    "    \"class_counts\": df[\"RiskLevel\"].value_counts().to_dict(),\n",
    "}\n",
    "json.dump(eda_summary, open(ARTIFACTS_DIR/\"eda_summary.json\",\"w\"), indent=2)\n",
    "\n",
    "# A few simple plots for the design doc\n",
    "(df[\"RiskLevel\"].value_counts()\n",
    "   .plot(kind=\"bar\", title=\"Class Distribution\")\n",
    "   .get_figure().savefig(ARTIFACTS_DIR/\"chart_class_distribution.png\")); plt.clf()\n",
    "\n",
    "df[\"Age\"].plot(kind=\"hist\", bins=20, title=\"Age Distribution\").get_figure().savefig(\n",
    "    ARTIFACTS_DIR/\"chart_age_hist.png\"); plt.clf()\n",
    "\n",
    "plt.boxplot([df[\"SystolicBP\"], df[\"DiastolicBP\"]], tick_labels=[\"SystolicBP\",\"DiastolicBP\"])\n",
    "plt.title(\"Blood Pressure Boxplots\"); plt.tight_layout()\n",
    "plt.savefig(ARTIFACTS_DIR/\"chart_bp_box.png\"); plt.clf()\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "corr = df[num_cols].corr()\n",
    "plt.imshow(corr, interpolation=\"nearest\"); plt.colorbar()\n",
    "plt.xticks(range(len(num_cols)), num_cols, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(len(num_cols)), num_cols)\n",
    "plt.title(\"Correlation Heatmap\"); plt.tight_layout()\n",
    "plt.savefig(ARTIFACTS_DIR/\"chart_corr_heatmap.png\"); plt.clf()\n",
    "\n",
    "print(\"EDA done -->\", ARTIFACTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d720b-3cbf-4645-81e5-cda60c4bf8c6",
   "metadata": {},
   "source": [
    "#### Feature engineering (clinically-motivated features + z-scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0b953f3-132d-4faf-9042-56c570bb7df2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:51:44.787636Z",
     "iopub.status.busy": "2025-09-30T20:51:44.787354Z",
     "iopub.status.idle": "2025-09-30T20:51:44.841856Z",
     "shell.execute_reply": "2025-09-30T20:51:44.841202Z",
     "shell.execute_reply.started": "2025-09-30T20:51:44.787616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering done.\n"
     ]
    }
   ],
   "source": [
    "# We derive simple vitals-based features and also add z-scaled versions for linear models.\n",
    "X = df.copy()\n",
    "# simple clinically meaningful features\n",
    "X[\"PulsePressure\"]    = X[\"SystolicBP\"] - X[\"DiastolicBP\"]\n",
    "X[\"SBP_to_DBP\"]       = X[\"SystolicBP\"] / (X[\"DiastolicBP\"].replace(0, np.nan))\n",
    "X[\"Fever\"]            = (X[\"BodyTemp\"] > 99.5).astype(int)\n",
    "X[\"Tachycardia\"]      = (X[\"HeartRate\"] >= 100).astype(int)\n",
    "X[\"HypertensionFlag\"] = ((X[\"SystolicBP\"] >= 140) | (X[\"DiastolicBP\"] >= 90)).astype(int)\n",
    "\n",
    "# z-scaling for linear models\n",
    "cont = [\"Age\",\"SystolicBP\",\"DiastolicBP\",\"BS\",\"BodyTemp\",\"HeartRate\",\"PulsePressure\"]\n",
    "X[[f\"z_{c}\" for c in cont]] = StandardScaler().fit_transform(X[cont])\n",
    "\n",
    "# binary labels (in this dataset: \"low risk\" / \"high risk\")\n",
    "label_map = {\"low risk\": 0, \"high risk\": 1}\n",
    "y = X[\"RiskLevel\"].map(label_map)\n",
    "engineered = pd.concat([X.drop(columns=[\"RiskLevel\"]), y.rename(\"label\")], axis=1)\n",
    "engineered.to_csv(ARTIFACTS_DIR/\"maternal_features_full.csv\", index=False)\n",
    "json.dump(label_map, open(ARTIFACTS_DIR/\"label_map.json\",\"w\"), indent=2)\n",
    "\n",
    "# Stratified splits (train/val/test/prod = 40/10/10/40)\n",
    "X_no_target = engineered.drop(columns=[\"label\"])\n",
    "y_only      = engineered[\"label\"]\n",
    "\n",
    "X_tmp, X_prod, y_tmp, y_prod = train_test_split(\n",
    "    X_no_target, y_only, test_size=0.40, random_state=42, stratify=y_only\n",
    ")\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=(1/3), random_state=42, stratify=y_tmp\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_rem, y_rem, test_size=0.5, random_state=42, stratify=y_rem\n",
    ")\n",
    "\n",
    "def save_split(name, Xd, yd):\n",
    "    out = Xd.copy(); out[\"label\"] = yd.values\n",
    "    out.to_csv(ARTIFACTS_DIR/f\"{name}.csv\", index=False)\n",
    "    return out\n",
    "\n",
    "train_df = save_split(\"train\", X_train, y_train)\n",
    "val_df   = save_split(\"val\",   X_val,   y_val)\n",
    "test_df  = save_split(\"test\",  X_test,  y_test)\n",
    "prod_df  = save_split(\"production\", X_prod, y_prod)\n",
    "\n",
    "print(\"Feature engineering done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d0b66-2582-4e76-9aba-50b1c9193047",
   "metadata": {},
   "source": [
    "#### Stratified splits: 40% prod, 40% train, 10% val, 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "882288ee-155b-42ff-a6a4-9583a26635cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:51:46.875250Z",
     "iopub.status.busy": "2025-09-30T20:51:46.874962Z",
     "iopub.status.idle": "2025-09-30T20:51:46.903058Z",
     "shell.execute_reply": "2025-09-30T20:51:46.902252Z",
     "shell.execute_reply.started": "2025-09-30T20:51:46.875230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 322, 'val': 81, 'test': 81, 'production': 324}\n"
     ]
    }
   ],
   "source": [
    "# We first carve out 40% as \"production\" holdout for future batch inference/monitoring.\n",
    "# The remaining 60% --> train (40%), val (10%), test (10%) of the original dataset.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 40% set aside for future batch inference/monitoring\n",
    "X_tmp, X_prod, y_tmp, y_prod = train_test_split(\n",
    "    X_no_target, y, test_size=0.40, random_state=42, stratify=y\n",
    ")\n",
    "# remaining 60% -> 40/10/10\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=(1/3), random_state=42, stratify=y_tmp\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_rem, y_rem, test_size=0.5, random_state=42, stratify=y_rem\n",
    ")\n",
    "\n",
    "def _save(name, Xd, yd):\n",
    "    out = Xd.copy(); out[\"label\"] = yd.values\n",
    "    out.to_csv(ARTIFACTS_DIR / f\"{name}.csv\", index=False)\n",
    "    return out\n",
    "\n",
    "train_df = _save(\"train\",      X_train, y_train)\n",
    "val_df   = _save(\"val\",        X_val,   y_val)\n",
    "test_df  = _save(\"test\",       X_test,  y_test)\n",
    "prod_df  = _save(\"production\", X_prod,  y_prod)\n",
    "\n",
    "print({\"train\":len(train_df), \"val\":len(val_df), \"test\":len(test_df), \"production\":len(prod_df)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818f682-6948-49c9-89ab-29fda6b786fa",
   "metadata": {},
   "source": [
    "#### Upload artifacts to S3 (no manual bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee9c2544-d4f1-4f72-948b-2b27a0defd9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:51:47.979435Z",
     "iopub.status.busy": "2025-09-30T20:51:47.979163Z",
     "iopub.status.idle": "2025-09-30T20:51:48.414787Z",
     "shell.execute_reply": "2025-09-30T20:51:48.413913Z",
     "shell.execute_reply.started": "2025-09-30T20:51:47.979415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143/train.csv\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143/val.csv\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143/test.csv\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143/production.csv\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143/maternal_features_full.csv\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143/label_map.json\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143/eda_summary.json\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143/figures/chart_class_distribution.png\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143/figures/chart_age_hist.png\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143/figures/chart_bp_box.png\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143/figures/chart_corr_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "# Upload the CSVs, label map, EDA summary, and figures to your default bucket/prefix.\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def s3_upload(local: Path, key: str):\n",
    "    s3.upload_file(str(local), bucket, f\"{S3_PREFIX}/{key}\")\n",
    "    print(\"Uploaded\", f\"s3://{bucket}/{S3_PREFIX}/{key}\")\n",
    "\n",
    "# CSVs + summaries\n",
    "for fname in [\"train.csv\",\"val.csv\",\"test.csv\",\"production.csv\",\n",
    "              \"maternal_features_full.csv\",\"label_map.json\",\"eda_summary.json\"]:\n",
    "    s3_upload(ARTIFACTS_DIR / fname, fname)\n",
    "\n",
    "# Plots\n",
    "for fname in [\"chart_class_distribution.png\",\"chart_age_hist.png\",\"chart_bp_box.png\",\"chart_corr_heatmap.png\"]:\n",
    "    s3_upload(ARTIFACTS_DIR / fname, f\"figures/{fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0280c5-9725-46f2-8231-3995920e9b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T15:14:52.533587Z",
     "iopub.status.busy": "2025-09-20T15:14:52.533071Z",
     "iopub.status.idle": "2025-09-20T15:14:52.537175Z",
     "shell.execute_reply": "2025-09-20T15:14:52.536233Z",
     "shell.execute_reply.started": "2025-09-20T15:14:52.533545Z"
    }
   },
   "source": [
    "#### Sanitize column names (Feature Store regex) & write sanitized splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80d6fae3-05c2-4908-b3a2-30b588255b4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:51:49.841215Z",
     "iopub.status.busy": "2025-09-30T20:51:49.840938Z",
     "iopub.status.idle": "2025-09-30T20:51:49.869221Z",
     "shell.execute_reply": "2025-09-30T20:51:49.868476Z",
     "shell.execute_reply.started": "2025-09-30T20:51:49.841194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized splits saved.\n"
     ]
    }
   ],
   "source": [
    "# FS rules: names must be letters/numbers/hyphens only; must start with alnum; <=64 chars.\n",
    "\n",
    "def sanitize_col(name: str) -> str:\n",
    "    if name == \"SBP_to_DBP\": name = \"SBPtoDBP\"   # preserve meaning\n",
    "    if name.startswith(\"z_\"): name = \"z\" + name[2:]\n",
    "    name = name.replace(\"_\", \"\")\n",
    "    name = \"\".join(ch for ch in name if ch.isalnum() or ch == \"-\")\n",
    "    if not name or not name[0].isalnum(): name = \"f\" + name\n",
    "    return name[:64]\n",
    "\n",
    "def sanitize_df_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    newcols, seen = [], set()\n",
    "    for c in df.columns:\n",
    "        s = sanitize_col(c)\n",
    "        if s in seen:\n",
    "            i, base = 2, s\n",
    "            while f\"{base}{i}\" in seen: i += 1\n",
    "            s = f\"{base}{i}\"\n",
    "        newcols.append(s); seen.add(s)\n",
    "    out = df.copy(); out.columns = newcols\n",
    "    return out\n",
    "\n",
    "label_col = \"label\"\n",
    "def sanitize_split(df):\n",
    "    feats = df.drop(columns=[label_col])\n",
    "    feats = sanitize_df_cols(feats)\n",
    "    feats[label_col] = df[label_col].values\n",
    "    return feats\n",
    "\n",
    "train_s = sanitize_split(train_df); train_s.to_csv(ARTIFACTS_DIR/\"train_sanitized.csv\", index=False)\n",
    "val_s   = sanitize_split(val_df);   val_s.to_csv(ARTIFACTS_DIR/\"val_sanitized.csv\",   index=False)\n",
    "test_s  = sanitize_split(test_df);  test_s.to_csv(ARTIFACTS_DIR/\"test_sanitized.csv\", index=False)\n",
    "prod_s  = sanitize_split(prod_df);  prod_s.to_csv(ARTIFACTS_DIR/\"production_sanitized.csv\", index=False)\n",
    "\n",
    "print(\"Sanitized splits saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a97e7b-0caa-44c8-bf22-b0937dc35491",
   "metadata": {},
   "source": [
    "#### Create & ingest Feature Store (OFFLINE, unique names per run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41bbe8b7-6ef8-4721-b6be-35b82e2dc8e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:51:51.741273Z",
     "iopub.status.busy": "2025-09-30T20:51:51.740998Z",
     "iopub.status.idle": "2025-09-30T20:53:41.812001Z",
     "shell.execute_reply": "2025-09-30T20:53:41.811151Z",
     "shell.execute_reply.started": "2025-09-30T20:51:51.741252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status mhr-train-fg-20250930-205143: Creating\n",
      "[READY] mhr-train-fg-20250930-205143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 81 to 162\n",
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 243 to 322\n",
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 0 to 81\n",
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 162 to 243\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 0 to 81\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 162 to 243\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 243 to 322\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 81 to 162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Ingested 322 rows → mhr-train-fg-20250930-205143\n",
      "Status mhr-val-fg-20250930-205143: Creating\n",
      "[READY] mhr-val-fg-20250930-205143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 0 to 21\n",
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 63 to 81\n",
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 42 to 63\n",
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 21 to 42\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 63 to 81\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 42 to 63\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 21 to 42\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 0 to 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Ingested 81 rows → mhr-val-fg-20250930-205143\n",
      "Status mhr-batch-fg-20250930-205143: Creating\n",
      "[READY] mhr-batch-fg-20250930-205143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 81 to 162\n",
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 0 to 81\n",
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 243 to 324\n",
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 162 to 243\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 243 to 324\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 81 to 162\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 162 to 243\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 0 to 81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Ingested 324 rows → mhr-batch-fg-20250930-205143\n",
      "Feature Store complete: mhr-train-fg-20250930-205143 mhr-val-fg-20250930-205143 mhr-batch-fg-20250930-205143\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.feature_definition import FeatureDefinition, FeatureTypeEnum\n",
    "\n",
    "sm      = boto3.client(\"sagemaker\")\n",
    "session = Session(boto3.session.Session(region_name=region))\n",
    "\n",
    "def ensure_id_time(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_in.copy()\n",
    "    if \"recordid\" not in df.columns:\n",
    "        df[\"recordid\"] = range(1, len(df)+1)\n",
    "    if \"eventtime\" not in df.columns:\n",
    "        df[\"eventtime\"] = pd.Timestamp.utcnow().isoformat()\n",
    "    return df\n",
    "\n",
    "def to_boto_feature_defs(df: pd.DataFrame):\n",
    "    out = []\n",
    "    for c, d in df.dtypes.items():\n",
    "        if c == \"eventtime\":\n",
    "            t = \"String\"\n",
    "        elif pd.api.types.is_integer_dtype(d):\n",
    "            t = \"Integral\"\n",
    "        elif pd.api.types.is_float_dtype(d):\n",
    "            t = \"Fractional\"\n",
    "        else:\n",
    "            t = \"String\"\n",
    "        out.append({\"FeatureName\": c, \"FeatureType\": t})\n",
    "    return out\n",
    "\n",
    "def create_fg_boto3(name: str, df_local: pd.DataFrame, s3_uri: str):\n",
    "    fdefs = to_boto_feature_defs(df_local)\n",
    "    try:\n",
    "        resp = sm.create_feature_group(\n",
    "            FeatureGroupName=name,\n",
    "            RecordIdentifierFeatureName=\"recordid\",\n",
    "            EventTimeFeatureName=\"eventtime\",\n",
    "            FeatureDefinitions=fdefs,\n",
    "            OfflineStoreConfig={\"S3StorageConfig\": {\"S3Uri\": s3_uri}},\n",
    "            OnlineStoreConfig={\"EnableOnlineStore\": False},\n",
    "            RoleArn=role,\n",
    "            Description=f\"Maternal Health Risk – {name}\",\n",
    "        )\n",
    "        return resp\n",
    "    except sm.exceptions.ResourceInUse:\n",
    "        # Already exists --> safe to reuse after we confirm it's Created\n",
    "        return {\"FeatureGroupArn\": f\"arn:aws:sagemaker:{region}:{boto3.client('sts').get_caller_identity()['Account']}:feature-group/{name}\"}\n",
    "\n",
    "def wait_fg_created(name: str, timeout_s: int = 900, poll_s: int = 10):\n",
    "    start = time.time()\n",
    "    last = \"\"\n",
    "    while True:\n",
    "        desc = sm.describe_feature_group(FeatureGroupName=name)\n",
    "        status = desc.get(\"FeatureGroupStatus\", \"\")\n",
    "        if status == \"Created\":\n",
    "            print(f\"[READY] {name}\")\n",
    "            return desc\n",
    "        if status == \"CreateFailed\":\n",
    "            raise RuntimeError(f\"{name} failed: {desc.get('FailureReason')}\")\n",
    "        if time.time() - start > timeout_s:\n",
    "            raise TimeoutError(f\"Timeout waiting for {name} (last status={status})\")\n",
    "        if status != last:\n",
    "            print(f\"Status {name}: {status}\")\n",
    "            last = status\n",
    "        time.sleep(poll_s)\n",
    "\n",
    "def create_and_ingest(name_base: str, df_local: pd.DataFrame):\n",
    "    # unique FG names per run to avoid collisions\n",
    "    name = f\"{name_base}-{RUN_ID}\"             # e.g., mhr-train-fg-20250920-154301\n",
    "    assert \"_\" not in name, \"FG name must not contain underscores.\"\n",
    "    df_local = ensure_id_time(df_local)\n",
    "    s3_uri   = f\"s3://{bucket}/{S3_PREFIX}/feature-store/{name}\"\n",
    "\n",
    "    create_fg_boto3(name, df_local, s3_uri)\n",
    "    wait_fg_created(name)\n",
    "\n",
    "    fg = FeatureGroup(name=name, sagemaker_session=session)\n",
    "    fg.load_feature_definitions(data_frame=df_local)    # make sure SDK knows schema\n",
    "    fg.ingest(data_frame=df_local, max_workers=4, wait=True)\n",
    "    print(f\"[OK] Ingested {len(df_local)} rows → {name}\")\n",
    "    return name\n",
    "\n",
    "# Load sanitized splits\n",
    "train_s = pd.read_csv(ARTIFACTS_DIR/\"train_sanitized.csv\")\n",
    "val_s   = pd.read_csv(ARTIFACTS_DIR/\"val_sanitized.csv\")\n",
    "prod_s  = pd.read_csv(ARTIFACTS_DIR/\"production_sanitized.csv\")\n",
    "\n",
    "# Create OFFLINE FGs with unique names\n",
    "FG_TRAIN = create_and_ingest(\"mhr-train-fg\", train_s)\n",
    "FG_VAL   = create_and_ingest(\"mhr-val-fg\",   val_s)\n",
    "FG_BATCH = create_and_ingest(\"mhr-batch-fg\", prod_s)\n",
    "\n",
    "print(\"Feature Store complete:\", FG_TRAIN, FG_VAL, FG_BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91005583-4b6c-4ecc-8829-abe82eb153cd",
   "metadata": {},
   "source": [
    "#### Tracker update (JSON + Markdown) and upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cba8c17-f8e7-43f6-98c9-526e5b7e5fae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:54:01.823261Z",
     "iopub.status.busy": "2025-09-30T20:54:01.821084Z",
     "iopub.status.idle": "2025-09-30T20:54:02.206360Z",
     "shell.execute_reply": "2025-09-30T20:54:02.205353Z",
     "shell.execute_reply.started": "2025-09-30T20:54:01.823220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracker written & uploaded.\n"
     ]
    }
   ],
   "source": [
    "tracker = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"s3_prefix\": f\"s3://{bucket}/{S3_PREFIX}\",\n",
    "    \"dataset\": {\n",
    "        \"rows\": eda_summary[\"rows\"], \"cols\": eda_summary[\"cols\"],\n",
    "        \"class_counts\": eda_summary[\"class_counts\"],\n",
    "        \"dtypes\": eda_summary[\"dtypes\"],\n",
    "        \"missing\": eda_summary[\"missing_counts\"],\n",
    "    },\n",
    "    \"splits\": {\n",
    "        \"train_rows\": len(train_df), \"val_rows\": len(val_df),\n",
    "        \"test_rows\": len(test_df), \"prod_rows\": len(prod_df),\n",
    "    },\n",
    "    \"feature_store_groups\": [FG_TRAIN, FG_VAL, FG_BATCH],\n",
    "}\n",
    "with open(ARTIFACTS_DIR / \"team_tracker_update_week3.json\", \"w\") as f:\n",
    "    json.dump(tracker, f, indent=2)\n",
    "\n",
    "md = f\"\"\"# Week 3 Tracker — Maternal Health Risk (RUN: {RUN_ID})\n",
    "\n",
    "**S3 prefix:** s3://{bucket}/{S3_PREFIX}\n",
    "\n",
    "## Dataset\n",
    "- Rows: {eda_summary['rows']} | Cols: {eda_summary['cols']}\n",
    "- Classes: {eda_summary['class_counts']}\n",
    "\n",
    "## Splits\n",
    "- Train: {len(train_df)} (~40%)\n",
    "- Val:   {len(val_df)} (~10%)\n",
    "- Test:  {len(test_df)} (~10%)\n",
    "- Prod:  {len(prod_df)} (~40%)\n",
    "\n",
    "## Feature Store (offline)\n",
    "- {FG_TRAIN}\n",
    "- {FG_VAL}\n",
    "- {FG_BATCH}\n",
    "\"\"\"\n",
    "with open(ARTIFACTS_DIR / \"team_tracker_update_week3.md\", \"w\") as f:\n",
    "    f.write(md)\n",
    "\n",
    "# Upload tracker docs\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(str(ARTIFACTS_DIR/\"team_tracker_update_week3.json\"), bucket, f\"{S3_PREFIX}/team_tracker_update_week3.json\")\n",
    "s3.upload_file(str(ARTIFACTS_DIR/\"team_tracker_update_week3.md\"),   bucket, f\"{S3_PREFIX}/team_tracker_update_week3.md\")\n",
    "\n",
    "print(\"Tracker written & uploaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2c762b5-98d8-4628-85c1-c35c454f3fd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:54:03.888857Z",
     "iopub.status.busy": "2025-09-30T20:54:03.888577Z",
     "iopub.status.idle": "2025-09-30T20:54:03.958676Z",
     "shell.execute_reply": "2025-09-30T20:54:03.957988Z",
     "shell.execute_reply.started": "2025-09-30T20:54:03.888835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_id': '20250930-205143',\n",
       " 's3_prefix': 's3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143',\n",
       " 'dataset': {'rows': 808,\n",
       "  'cols': 7,\n",
       "  'class_counts': {'low risk': 478, 'high risk': 330},\n",
       "  'dtypes': {'Age': 'int64',\n",
       "   'SystolicBP': 'int64',\n",
       "   'DiastolicBP': 'int64',\n",
       "   'BS': 'float64',\n",
       "   'BodyTemp': 'float64',\n",
       "   'HeartRate': 'int64',\n",
       "   'RiskLevel': 'object'},\n",
       "  'missing': {'Age': 0,\n",
       "   'SystolicBP': 0,\n",
       "   'DiastolicBP': 0,\n",
       "   'BS': 0,\n",
       "   'BodyTemp': 0,\n",
       "   'HeartRate': 0,\n",
       "   'RiskLevel': 0}},\n",
       " 'splits': {'train_rows': 322,\n",
       "  'val_rows': 81,\n",
       "  'test_rows': 81,\n",
       "  'prod_rows': 324},\n",
       " 'feature_store_groups': ['mhr-train-fg-20250930-205143',\n",
       "  'mhr-val-fg-20250930-205143',\n",
       "  'mhr-batch-fg-20250930-205143']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View\n",
    "\n",
    "import boto3, json\n",
    "s3 = boto3.client(\"s3\")\n",
    "obj = s3.get_object(Bucket=bucket, Key=f\"{S3_PREFIX}/team_tracker_update_week3.json\")\n",
    "tracker = json.load(obj[\"Body\"])\n",
    "tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dcc032-351e-4674-a70b-409d5bf9c472",
   "metadata": {},
   "source": [
    "## Week 4, Model Development and Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26772f-4388-4d0e-bd69-797a9f5d678b",
   "metadata": {},
   "source": [
    "#### Auto settings; continues from Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d69f10eb-4545-4fa0-9684-1230a160bee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:54:05.918712Z",
     "iopub.status.busy": "2025-09-30T20:54:05.918422Z",
     "iopub.status.idle": "2025-09-30T20:54:05.931452Z",
     "shell.execute_reply": "2025-09-30T20:54:05.930568Z",
     "shell.execute_reply.started": "2025-09-30T20:54:05.918688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Week-3: s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143\n",
      "Writing Week-4: s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20250930-205405\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AAI-540 — Week 4: Benchmark (LogReg), Main Model (XGBoost), Evaluation, Batch Deploy\n",
    "This script:\n",
    "  1) Reuses Week-3 splits from the latest run (or pass WEEK3_PREFIX env var)\n",
    "  2) Trains a *benchmark* Logistic Regression (Age + SystolicBP) via SKLearn Estimator\n",
    "  3) Trains a full-feature **XGBoost** model (built-in container)\n",
    "  4) Compares metrics on the test set and writes confusion matrices\n",
    "  5) Runs **Batch Transform** on Week-3 production data\n",
    "  6) Writes a Week-4 tracker + design-doc snippet with S3 artifact links\n",
    "\"\"\"\n",
    "\n",
    "import os, io, json, time, tarfile\n",
    "from pathlib import Path\n",
    "import boto3, sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Reuse Week-3 objects if they exist; otherwise, auto-init (no manual config)\n",
    "try:\n",
    "    bucket\n",
    "    sm_session\n",
    "    role\n",
    "except NameError:\n",
    "    boto_sess  = boto3.session.Session()\n",
    "    sm_session = Session(boto_sess)\n",
    "    role       = get_execution_role()\n",
    "    bucket     = sm_session.default_bucket()\n",
    "\n",
    "# Use the Week-3 S3 prefix if it’s still in memory; otherwise pick the latest run\n",
    "s3 = boto3.client(\"s3\")\n",
    "try:\n",
    "    WEEK3_PREFIX = S3_PREFIX  # from Week 3 cells\n",
    "except NameError:\n",
    "    base = \"aai540/maternal-risk/week3/\"\n",
    "    resp = s3.list_objects_v2(Bucket=bucket, Prefix=base, Delimiter=\"/\")\n",
    "    runs = [cp[\"Prefix\"].rstrip(\"/\") for cp in resp.get(\"CommonPrefixes\", [])]\n",
    "    assert runs, f\"No Week-3 artifacts found under s3://{bucket}/{base}\"\n",
    "    WEEK3_PREFIX = sorted(runs)[-1]\n",
    "\n",
    "# Create a unique Week-4 prefix\n",
    "RUN_ID = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "W4_PREFIX = f\"aai540/maternal-risk/week4/{RUN_ID}\"\n",
    "\n",
    "print(\"Using Week-3:\", f\"s3://{bucket}/{WEEK3_PREFIX}\")\n",
    "print(\"Writing Week-4:\", f\"s3://{bucket}/{W4_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5bb9a5-d6f2-4a3a-8089-bd8cd2d6bec2",
   "metadata": {},
   "source": [
    "#### Load Week-3 splits (train/val/test) from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a57735c-7195-4ac6-bb29-0220e395e7a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:54:08.308481Z",
     "iopub.status.busy": "2025-09-30T20:54:08.308199Z",
     "iopub.status.idle": "2025-09-30T20:54:08.495692Z",
     "shell.execute_reply": "2025-09-30T20:54:08.494864Z",
     "shell.execute_reply.started": "2025-09-30T20:54:08.308459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (322, 19) (81, 19) (81, 19)\n",
      "Train label balance: {0: 190, 1: 132}\n"
     ]
    }
   ],
   "source": [
    "# LOAD SPLITS\n",
    "\n",
    "def read_csv_from_s3(key: str) -> pd.DataFrame:\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "train = read_csv_from_s3(f\"{WEEK3_PREFIX}/train.csv\")\n",
    "val   = read_csv_from_s3(f\"{WEEK3_PREFIX}/val.csv\")\n",
    "test  = read_csv_from_s3(f\"{WEEK3_PREFIX}/test.csv\")\n",
    "\n",
    "label_col = \"label\"\n",
    "X_train, y_train = train.drop(columns=[label_col]), train[label_col]\n",
    "X_val,   y_val   = val.drop(columns=[label_col]),   val[label_col]\n",
    "X_test,  y_test  = test.drop(columns=[label_col]),  test[label_col]\n",
    "\n",
    "print(\"Loaded:\", train.shape, val.shape, test.shape)\n",
    "print(\"Train label balance:\", y_train.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da29fc7-8a77-4ae7-93ab-214810086a8f",
   "metadata": {},
   "source": [
    "#### Benchmark model in SageMaker (very simple: Logistic Regression on 2 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92889a8b-9ca5-418b-bd45-217acbc0e9f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T20:54:09.775256Z",
     "iopub.status.busy": "2025-09-30T20:54:09.774990Z",
     "iopub.status.idle": "2025-09-30T20:57:27.567144Z",
     "shell.execute_reply": "2025-09-30T20:57:27.566519Z",
     "shell.execute_reply.started": "2025-09-30T20:54:09.775236Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-scikit-learn-2025-09-30-20-54-09-915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-30 20:54:11 Starting - Starting the training job...\n",
      "2025-09-30 20:54:26 Starting - Preparing the instances for training...\n",
      "2025-09-30 20:54:49 Downloading - Downloading input data...\n",
      "2025-09-30 20:55:20 Downloading - Downloading the training image......\n",
      "2025-09-30 20:56:36 Training - Training image download completed. Training in progress.\n",
      "2025-09-30 20:56:36 Uploading - Uploading generated training model\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m2025-09-30 20:56:30,597 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2025-09-30 20:56:30,602 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-09-30 20:56:30,605 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-09-30 20:56:30,623 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-09-30 20:56:30,865 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-09-30 20:56:30,869 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-09-30 20:56:30,889 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-09-30 20:56:30,891 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-09-30 20:56:30,910 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-09-30 20:56:30,912 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-09-30 20:56:30,928 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.large\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.large\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2025-09-30-20-54-09-915\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-533267301342/sagemaker-scikit-learn-2025-09-30-20-54-09-915/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"baseline_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"baseline_train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=baseline_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m5.large\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=baseline_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-533267301342/sagemaker-scikit-learn-2025-09-30-20-54-09-915/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.large\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"sagemaker-scikit-learn-2025-09-30-20-54-09-915\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-533267301342/sagemaker-scikit-learn-2025-09-30-20-54-09-915/source/sourcedir.tar.gz\",\"module_name\":\"baseline_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"baseline_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python39.zip:/miniconda3/lib/python3.9:/miniconda3/lib/python3.9/lib-dynload:/miniconda3/lib/python3.9/site-packages:/miniconda3/lib/python3.9/site-packages/setuptools/_vendor\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python baseline_train.py\u001b[0m\n",
      "\u001b[34m2025-09-30 20:56:30,929 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2025-09-30 20:56:30,929 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m2025-09-30 20:56:31,902 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-09-30 20:56:54 Completed - Training job completed\n",
      "Training seconds: 125\n",
      "Billable seconds: 125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7654320987654321,\n",
       " 'precision': 0.71875,\n",
       " 'recall': 0.696969696969697,\n",
       " 'f1': 0.7076923076923077,\n",
       " 'roc_auc': 0.790719696969697}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why: have a simple, interpretable baseline for comparison (MVP).\n",
    "from sagemaker.sklearn import SKLearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "bm_feats = [\"Age\", \"SystolicBP\"]\n",
    "assert all(f in X_train.columns for f in bm_feats), \"Expected baseline features missing.\"\n",
    "\n",
    "# Stage small CSVs (filenames don't matter inside channels)\n",
    "w4_local = Path(\"w4_benchmark\"); w4_local.mkdir(exist_ok=True)\n",
    "pd.concat([X_train[bm_feats], y_train], axis=1).to_csv(w4_local/\"train_benchmark.csv\", index=False)\n",
    "pd.concat([X_val[bm_feats],   y_val],   axis=1).to_csv(w4_local/\"val_benchmark.csv\",   index=False)\n",
    "\n",
    "bm_train_s3 = sm_session.upload_data(str(w4_local/\"train_benchmark.csv\"), key_prefix=f\"{W4_PREFIX}/benchmark\")\n",
    "bm_val_s3   = sm_session.upload_data(str(w4_local/\"val_benchmark.csv\"),   key_prefix=f\"{W4_PREFIX}/benchmark\")\n",
    "\n",
    "# Entry script: read first *.csv in each channel, fit LR, write metrics + model\n",
    "with open(\"baseline_train.py\",\"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "import os, glob, json, pathlib\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "def first_csv_in(d):\n",
    "    files = sorted(glob.glob(os.path.join(d, '*.csv')))\n",
    "    assert files, f'No CSV found in {d}'\n",
    "    return files[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_dir = os.environ.get('SM_CHANNEL_TRAIN', '/opt/ml/input/data/train')\n",
    "    val_dir   = os.environ.get('SM_CHANNEL_VAL',   '/opt/ml/input/data/val')\n",
    "    model_dir = os.environ.get('SM_MODEL_DIR',     '/opt/ml/model')\n",
    "\n",
    "    df_tr = pd.read_csv(first_csv_in(train_dir))\n",
    "    df_va = pd.read_csv(first_csv_in(val_dir))\n",
    "\n",
    "    Xtr, ytr = df_tr[['Age','SystolicBP']], df_tr['label']\n",
    "    Xva, yva = df_va[['Age','SystolicBP']], df_va['label']\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000).fit(Xtr, ytr)\n",
    "\n",
    "    pred  = clf.predict(Xva)\n",
    "    proba = clf.predict_proba(Xva)[:,1]\n",
    "    acc = accuracy_score(yva, pred)\n",
    "    p,r,f1,_ = precision_recall_fscore_support(yva, pred, average='binary', zero_division=0)\n",
    "    try: auc = roc_auc_score(yva, proba)\n",
    "    except: auc = float('nan')\n",
    "\n",
    "    pathlib.Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "    import joblib\n",
    "    joblib.dump(clf, os.path.join(model_dir, 'model.joblib'))\n",
    "    with open(os.path.join(model_dir, 'metrics.json'), 'w') as f:\n",
    "        json.dump({'accuracy':acc,'precision':p,'recall':r,'f1':f1,'roc_auc':auc}, f)\n",
    "\"\"\")\n",
    "\n",
    "bm_est = SKLearn(\n",
    "    entry_point=\"baseline_train.py\",\n",
    "    framework_version=\"1.2-1\",     # use a tag compatible with your Studio image\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    sagemaker_session=sm_session,\n",
    ")\n",
    "bm_est.fit({\"train\": bm_train_s3, \"val\": bm_val_s3})\n",
    "\n",
    "# Also compute baseline metrics on our held-out TEST for a clean comparison\n",
    "bm_clf   = LogisticRegression(max_iter=1000).fit(train[bm_feats], y_train)\n",
    "bm_proba = bm_clf.predict_proba(test[bm_feats])[:,1]\n",
    "bm_pred  = (bm_proba >= 0.5).astype(int)\n",
    "\n",
    "bm_acc = accuracy_score(y_test, bm_pred)\n",
    "bm_p, bm_r, bm_f1, _ = precision_recall_fscore_support(y_test, bm_pred, average='binary', zero_division=0)\n",
    "try:    bm_auc = roc_auc_score(y_test, bm_proba)\n",
    "except: bm_auc = float(\"nan\")\n",
    "\n",
    "baseline_metrics = {\"accuracy\":bm_acc,\"precision\":bm_p,\"recall\":bm_r,\"f1\":bm_f1,\"roc_auc\":bm_auc}\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07094141-33e0-4b5b-b255-92e41e2e46ec",
   "metadata": {},
   "source": [
    "#### MAIN MODEL in SageMaker (Built-in XGBoost, CSV mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bda5ca46-6455-4069-846a-0ff0dcbb7cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T21:05:50.970431Z",
     "iopub.status.busy": "2025-09-30T21:05:50.970134Z",
     "iopub.status.idle": "2025-09-30T21:09:08.648226Z",
     "shell.execute_reply": "2025-09-30T21:09:08.647269Z",
     "shell.execute_reply.started": "2025-09-30T21:05:50.970409Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-09-30-21-05-51-135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-30 21:05:51 Starting - Starting the training job...\n",
      "2025-09-30 21:06:11 Starting - Preparing the instances for training...\n",
      "2025-09-30 21:06:33 Downloading - Downloading input data...\n",
      "2025-09-30 21:07:18 Downloading - Downloading the training image......\n",
      "2025-09-30 21:08:24 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:08:28.746 ip-10-2-96-218.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:08:28.818 ip-10-2-96-218.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] File path /opt/ml/input/data/train of input files\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Making smlinks from folder /opt/ml/input/data/train to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] creating symlink between Path /opt/ml/input/data/train/w4_xgb_train.csv and destination /tmp/sagemaker_xgboost_input_data/w4_xgb_train.csv-5956959047836998103\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] File path /opt/ml/input/data/validation of input files\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Making smlinks from folder /opt/ml/input/data/validation to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] creating symlink between Path /opt/ml/input/data/validation/w4_xgb_val.csv and destination /tmp/sagemaker_xgboost_input_data/w4_xgb_val.csv-1168102882775757718\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Train matrix has 322 rows and 18 columns\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Validation matrix has 81 rows\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:08:29.203 ip-10-2-96-218.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:08:29.203 ip-10-2-96-218.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:08:29.204 ip-10-2-96-218.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:08:29.204 ip-10-2-96-218.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:08:29:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:08:29.207 ip-10-2-96-218.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:08:29.211 ip-10-2-96-218.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.96216#011validation-auc:0.92771\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.96427#011validation-auc:0.92803\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.97984#011validation-auc:0.94760\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.98591#011validation-auc:0.95896\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.98706#011validation-auc:0.95896\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.99187#011validation-auc:0.98295\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.99155#011validation-auc:0.98169\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.99193#011validation-auc:0.98169\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.99288#011validation-auc:0.98232\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.99330#011validation-auc:0.97822\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.99400#011validation-auc:0.97727\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.99448#011validation-auc:0.97917\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.99474#011validation-auc:0.97980\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.99466#011validation-auc:0.97980\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.99498#011validation-auc:0.98106\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.99520#011validation-auc:0.98043\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.99524#011validation-auc:0.98106\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.99607#011validation-auc:0.98295\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.99635#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.99659#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.99703#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.99743#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.99735#011validation-auc:0.98359\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.99715#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.99743#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.99755#011validation-auc:0.98485\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.99767#011validation-auc:0.98359\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.99767#011validation-auc:0.98359\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.99807#011validation-auc:0.98485\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.99807#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.99803#011validation-auc:0.98359\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.99858#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.99862#011validation-auc:0.98485\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.99819#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.99815#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.99823#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.99850#011validation-auc:0.98927\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.99850#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.99866#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.99866#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.99862#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.99870#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:0.99870#011validation-auc:0.98864\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:0.99874#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:0.99878#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:0.99878#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:0.99890#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:0.99882#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:0.99886#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:0.99886#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:0.99886#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:0.99874#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:0.99894#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:0.99890#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:0.99894#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:0.99894#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:0.99894#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:0.99914#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[83]#011train-auc:0.99914#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[84]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:0.99894#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[93]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:0.99906#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:0.99906#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[100]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[101]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[102]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[103]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[104]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[105]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[106]#011train-auc:0.99906#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[107]#011train-auc:0.99906#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[108]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[109]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[110]#011train-auc:0.99906#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[111]#011train-auc:0.99906#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[112]#011train-auc:0.99906#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[113]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[114]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[115]#011train-auc:0.99902#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[116]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[117]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[118]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[119]#011train-auc:0.99894#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[120]#011train-auc:0.99894#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[121]#011train-auc:0.99894#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[122]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[123]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[124]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[125]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[126]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[127]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[128]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[129]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[130]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[131]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[132]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[133]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[134]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[135]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[136]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[137]#011train-auc:0.99930#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[138]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[139]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[140]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[141]#011train-auc:0.99930#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[142]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[143]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[144]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[145]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[146]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[147]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[148]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[149]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[150]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[151]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[152]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[153]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[154]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[155]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[156]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[157]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[158]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[159]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[160]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[161]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[162]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[163]#011train-auc:0.99942#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[164]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[165]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[166]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[167]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[168]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[169]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[170]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[171]#011train-auc:0.99942#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[172]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[173]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[174]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[175]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[176]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[177]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[178]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[179]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[180]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[181]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[182]#011train-auc:0.99942#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[183]#011train-auc:0.99942#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[184]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[185]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[186]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[187]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[188]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[189]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[190]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[191]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[192]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[193]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[194]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[195]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[196]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[197]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[198]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[199]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\n",
      "2025-09-30 21:08:48 Uploading - Uploading generated training model\n",
      "2025-09-30 21:08:48 Completed - Training job completed\n",
      "Training seconds: 135\n",
      "Billable seconds: 135\n",
      "XGBoost model artifact: s3://sagemaker-us-east-1-533267301342/sagemaker-xgboost-2025-09-30-21-05-51-135/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Why built-in? No entry_point needed; just CSV with label first (no header).\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import sagemaker\n",
    "\n",
    "# Helper to reorder columns to [label, features...] for CSV training\n",
    "def reorder_for_xgb(df):\n",
    "    cols = [label_col] + [c for c in df.columns if c != label_col]\n",
    "    return df[cols]\n",
    "\n",
    "xgb_train_local = reorder_for_xgb(train)\n",
    "xgb_val_local   = reorder_for_xgb(val)\n",
    "\n",
    "xgb_train_path = Path(\"w4_xgb_train.csv\"); xgb_val_path = Path(\"w4_xgb_val.csv\")\n",
    "xgb_train_local.to_csv(xgb_train_path, index=False, header=False)\n",
    "xgb_val_local.to_csv(xgb_val_path,   index=False, header=False)\n",
    "\n",
    "s3_xgb_train = sm_session.upload_data(str(xgb_train_path), key_prefix=f\"{W4_PREFIX}/xgb\")\n",
    "s3_xgb_val   = sm_session.upload_data(str(xgb_val_path),   key_prefix=f\"{W4_PREFIX}/xgb\")\n",
    "\n",
    "def get_xgb_image():\n",
    "    for ver in [\"1.7-1\", \"1.5-1\", \"1.3-1\"]:\n",
    "        try:\n",
    "            return sagemaker.image_uris.retrieve(\"xgboost\", sm_session.boto_region_name, version=ver)\n",
    "        except Exception as e:\n",
    "            print(f\"xgboost {ver} not available → trying next … ({e})\")\n",
    "    raise RuntimeError(\"No compatible built-in XGBoost image found.\")\n",
    "\n",
    "xgb_image_uri = get_xgb_image()\n",
    "\n",
    "xgb_est = Estimator(\n",
    "    image_uri=xgb_image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    sagemaker_session=sm_session,\n",
    "    hyperparameters={\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"eval_metric\":\"auc\",\n",
    "        \"max_depth\":5,\n",
    "        \"eta\":0.2,\n",
    "        \"min_child_weight\":1,\n",
    "        \"subsample\":0.8,\n",
    "        \"colsample_bytree\":0.8,\n",
    "        \"num_round\":200,\n",
    "        \"verbosity\":1,\n",
    "    },\n",
    ")\n",
    "\n",
    "# tell container the training data is CSV (otherwise it expects libsvm)\n",
    "train_input = TrainingInput(s3_data=s3_xgb_train, content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(s3_data=s3_xgb_val,   content_type=\"text/csv\")\n",
    "\n",
    "xgb_est.fit({\"train\": train_input, \"validation\": val_input}, wait=True)\n",
    "xgb_model_artifact = xgb_est.model_data\n",
    "print(\"XGBoost model artifact:\", xgb_model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b71be-2aac-4015-ab09-9a350ab6f71f",
   "metadata": {},
   "source": [
    "#### EVALUATE (compare main vs baseline on TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d01b9ed-7a61-488a-85b9-5514ac69af86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T21:09:45.099259Z",
     "iopub.status.busy": "2025-09-30T21:09:45.098980Z",
     "iopub.status.idle": "2025-09-30T21:09:45.380639Z",
     "shell.execute_reply": "2025-09-30T21:09:45.379984Z",
     "shell.execute_reply.started": "2025-09-30T21:09:45.099236Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_292/132528516.py:12: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  t.extractall(tmp_dir)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'baseline': {'accuracy': 0.7654320987654321,\n",
       "  'precision': 0.71875,\n",
       "  'recall': 0.696969696969697,\n",
       "  'f1': 0.7076923076923077,\n",
       "  'roc_auc': 0.790719696969697},\n",
       " 'xgboost': {'accuracy': 0.9876543209876543,\n",
       "  'precision': 1.0,\n",
       "  'recall': 0.9696969696969697,\n",
       "  'f1': 0.9846153846153847,\n",
       "  'roc_auc': 0.999368686868687}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def parse_s3_uri(uri: str):\n",
    "    assert uri.startswith(\"s3://\")\n",
    "    p = uri[5:]; b, k = p.split(\"/\", 1)\n",
    "    return b, k\n",
    "\n",
    "tmp_dir = Path(\"w4_tmp\"); tmp_dir.mkdir(exist_ok=True)\n",
    "bkt, key = parse_s3_uri(xgb_model_artifact)\n",
    "boto3.client(\"s3\").download_file(bkt, key, str(tmp_dir/\"model.tar.gz\"))\n",
    "with tarfile.open(tmp_dir/\"model.tar.gz\") as t:\n",
    "    t.extractall(tmp_dir)\n",
    "\n",
    "# Score test set with the trained booster\n",
    "dtest   = xgb.DMatrix(test.drop(columns=[label_col]), label=test[label_col])\n",
    "booster = xgb.Booster(); booster.load_model(str(tmp_dir/\"xgboost-model\"))\n",
    "xgb_proba = booster.predict(dtest)\n",
    "xgb_pred  = (xgb_proba >= 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "xgb_p, xgb_r, xgb_f1, _ = precision_recall_fscore_support(y_test, xgb_pred, average='binary', zero_division=0)\n",
    "xgb_auc = roc_auc_score(y_test, xgb_proba)\n",
    "\n",
    "metrics_compare = {\n",
    "    \"baseline\": baseline_metrics,\n",
    "    \"xgboost\":  {\"accuracy\":xgb_acc,\"precision\":xgb_p,\"recall\":xgb_r,\"f1\":xgb_f1,\"roc_auc\":xgb_auc},\n",
    "}\n",
    "metrics_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f1d73-a865-487d-8792-4f3cf2b7fd8f",
   "metadata": {},
   "source": [
    "#### Deploy via Batch Transform (score Week-3 production.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce2d23ae-613f-4512-ba37-4dcc5420d131",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T21:09:49.736160Z",
     "iopub.status.busy": "2025-09-30T21:09:49.735794Z",
     "iopub.status.idle": "2025-09-30T21:15:54.896746Z",
     "shell.execute_reply": "2025-09-30T21:15:54.896034Z",
     "shell.execute_reply.started": "2025-09-30T21:09:49.736125Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-09-30-21-09-49-897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature cols count: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2025-09-30-21-09-50-645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:04:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:15:04 +0000] [13] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:15:04 +0000] [13] [INFO] Listening at: unix:/tmp/gunicorn.sock (13)\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:15:04 +0000] [13] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:15:04 +0000] [16] [INFO] Booting worker with pid: 16\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:15:04 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:07:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:07:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:07:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:07:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:07:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:07:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [30/Sep/2025:21:15:13 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [30/Sep/2025:21:15:13 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [30/Sep/2025:21:15:13 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [30/Sep/2025:21:15:13 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:13:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [30/Sep/2025:21:15:13 +0000] \"POST /invocations HTTP/1.1\" 200 6571 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:13:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [30/Sep/2025:21:15:13 +0000] \"POST /invocations HTTP/1.1\" 200 6571 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2025-09-30T21:15:13.725:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:04:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:04:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:15:04 +0000] [13] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:15:04 +0000] [13] [INFO] Listening at: unix:/tmp/gunicorn.sock (13)\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:15:04 +0000] [13] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:15:04 +0000] [16] [INFO] Booting worker with pid: 16\u001b[0m\n",
      "\u001b[34m[2025-09-30 21:15:04 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2025-09-30 21:15:04 +0000] [13] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[35m[2025-09-30 21:15:04 +0000] [13] [INFO] Listening at: unix:/tmp/gunicorn.sock (13)\u001b[0m\n",
      "\u001b[35m[2025-09-30 21:15:04 +0000] [13] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2025-09-30 21:15:04 +0000] [16] [INFO] Booting worker with pid: 16\u001b[0m\n",
      "\u001b[35m[2025-09-30 21:15:04 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:07:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:07:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:07:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:07:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:07:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:07:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:07:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:07:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:07:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:07:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:07:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:07:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [30/Sep/2025:21:15:13 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [30/Sep/2025:21:15:13 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [30/Sep/2025:21:15:13 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [30/Sep/2025:21:15:13 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-09-30:21:15:13:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [30/Sep/2025:21:15:13 +0000] \"POST /invocations HTTP/1.1\" 200 6571 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-09-30:21:15:13:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [30/Sep/2025:21:15:13 +0000] \"POST /invocations HTTP/1.1\" 200 6571 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2025-09-30T21:15:13.725:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "Batch output: s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20250930-205405/batch/outputs\n"
     ]
    }
   ],
   "source": [
    "# Inference expects FEATURES ONLY (no label) in the SAME order as training.\n",
    "\n",
    "from sagemaker.inputs import TransformInput\n",
    "\n",
    "prod_df = read_csv_from_s3(f\"{WEEK3_PREFIX}/production.csv\")\n",
    "\n",
    "# Same feature order as used to create training CSVs\n",
    "FEATURE_COLS = [c for c in train.columns if c != label_col]\n",
    "print(\"Feature cols count:\", len(FEATURE_COLS))\n",
    "\n",
    "prod_features = prod_df[FEATURE_COLS].copy()\n",
    "bt_local = Path(\"w4_production_features_only.csv\")\n",
    "prod_features.to_csv(bt_local, index=False, header=False)\n",
    "\n",
    "s3_bt_input = sm_session.upload_data(str(bt_local), key_prefix=f\"{W4_PREFIX}/batch\")\n",
    "\n",
    "transformer = xgb_est.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/{W4_PREFIX}/batch/outputs\",\n",
    "    accept=\"text/csv\",\n",
    "    assemble_with=\"Line\",\n",
    ")\n",
    "\n",
    "transformer.transform(data=s3_bt_input, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()\n",
    "\n",
    "batch_output_s3 = transformer.output_path\n",
    "print(\"Batch output:\", batch_output_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dcfda8-a55b-44db-a67c-c98009fbf264",
   "metadata": {},
   "source": [
    "#### ARTIFACTS + DESIGN-DOC SNIPPET + TRACKER (upload to S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94aa204a-d7f5-4808-8c49-be0b851cfe97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T21:16:22.107831Z",
     "iopub.status.busy": "2025-09-30T21:16:22.107545Z",
     "iopub.status.idle": "2025-09-30T21:16:23.038388Z",
     "shell.execute_reply": "2025-09-30T21:16:23.037561Z",
     "shell.execute_reply.started": "2025-09-30T21:16:22.107809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Week 4 Findings — Model Development & Deployment\n",
      "\n",
      "**Benchmark (LogReg on Age + SystolicBP)**  \n",
      "Acc: 0.765 | Prec: 0.719 | Rec: 0.697 | F1: 0.708 | AUC: 0.791\n",
      "\n",
      "**XGBoost (full features)**  \n",
      "Acc: 0.988 | Prec: 1.000 | Rec: 0.970 | F1: 0.985 | AUC: 0.999\n",
      "\n",
      "**Artifacts**  \n",
      "- Metrics JSON: s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20250930-205405/metrics_compare.json  \n",
      "- Baseline CM:  s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20250930-205405/baseline_cm.png  \n",
      "- XGBoost CM:   s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20250930-205405/xgb_cm.png  \n",
      "- XGBoost Model Artifact: s3://sagemaker-us-east-1-533267301342/sagemaker-xgboost-2025-09-30-21-05-51-135/output/model.tar.gz  \n",
      "- Batch Transform Output: s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20250930-205405/batch/outputs\n",
      "\n",
      "Week-4 tracker written & uploaded → s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20250930-205405/team_tracker_update_week4.*\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_cm(cm, title, path):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest'); plt.title(title); plt.colorbar()\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\"); plt.tight_layout(); plt.savefig(path); plt.close()\n",
    "\n",
    "# Confusion matrices (test set)\n",
    "bm_pred = (LogisticRegression(max_iter=1000).fit(train[bm_feats], y_train)\n",
    "           .predict_proba(test[bm_feats])[:,1] >= 0.5).astype(int)\n",
    "bm_cm   = confusion_matrix(y_test, bm_pred)\n",
    "xgb_cm  = confusion_matrix(y_test, xgb_pred)\n",
    "\n",
    "art_dir = Path(\"w4_artifacts\"); art_dir.mkdir(exist_ok=True)\n",
    "plot_cm(bm_cm,  \"Baseline CM\", art_dir/\"baseline_cm.png\")\n",
    "plot_cm(xgb_cm, \"XGBoost CM\",  art_dir/\"xgb_cm.png\")\n",
    "with open(art_dir/\"metrics_compare.json\",\"w\") as f:\n",
    "    json.dump(metrics_compare, f, indent=2)\n",
    "\n",
    "def up(local, key):\n",
    "    boto3.client(\"s3\").upload_file(str(local), bucket, f\"{W4_PREFIX}/{key}\")\n",
    "    return f\"s3://{bucket}/{W4_PREFIX}/{key}\"\n",
    "\n",
    "metrics_s3 = up(art_dir/\"metrics_compare.json\", \"metrics_compare.json\")\n",
    "bm_cm_s3   = up(art_dir/\"baseline_cm.png\",      \"baseline_cm.png\")\n",
    "xgb_cm_s3  = up(art_dir/\"xgb_cm.png\",           \"xgb_cm.png\")\n",
    "\n",
    "design_doc_snippet = f\"\"\"\n",
    "### Week 4 Findings — Model Development & Deployment\n",
    "\n",
    "**Benchmark (LogReg on Age + SystolicBP)**  \n",
    "Acc: {baseline_metrics['accuracy']:.3f} | Prec: {baseline_metrics['precision']:.3f} | Rec: {baseline_metrics['recall']:.3f} | F1: {baseline_metrics['f1']:.3f} | AUC: {baseline_metrics['roc_auc']:.3f}\n",
    "\n",
    "**XGBoost (full features)**  \n",
    "Acc: {metrics_compare['xgboost']['accuracy']:.3f} | Prec: {metrics_compare['xgboost']['precision']:.3f} | Rec: {metrics_compare['xgboost']['recall']:.3f} | F1: {metrics_compare['xgboost']['f1']:.3f} | AUC: {metrics_compare['xgboost']['roc_auc']:.3f}\n",
    "\n",
    "**Artifacts**  \n",
    "- Metrics JSON: {metrics_s3}  \n",
    "- Baseline CM:  {bm_cm_s3}  \n",
    "- XGBoost CM:   {xgb_cm_s3}  \n",
    "- XGBoost Model Artifact: {xgb_model_artifact}  \n",
    "- Batch Transform Output: {batch_output_s3}\n",
    "\"\"\"\n",
    "print(design_doc_snippet)\n",
    "\n",
    "# Tracker (JSON + Markdown)\n",
    "w4_tracker_dir = Path(\"w4_tracker\"); w4_tracker_dir.mkdir(exist_ok=True)\n",
    "tracker_w4 = {\n",
    "    \"week\": \"4\",\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"week3_prefix\": f\"s3://{bucket}/{WEEK3_PREFIX}\",\n",
    "    \"week4_prefix\": f\"s3://{bucket}/{W4_PREFIX}\",\n",
    "    \"benchmark\": baseline_metrics,\n",
    "    \"xgboost\": metrics_compare[\"xgboost\"],\n",
    "    \"artifacts\": {\n",
    "        \"metrics_json\": metrics_s3,\n",
    "        \"baseline_cm\": bm_cm_s3,\n",
    "        \"xgb_cm\": xgb_cm_s3,\n",
    "        \"model_artifact\": xgb_model_artifact,\n",
    "        \"batch_output\": batch_output_s3\n",
    "    }\n",
    "}\n",
    "with open(w4_tracker_dir/\"team_tracker_update_week4.json\",\"w\") as f:\n",
    "    json.dump(tracker_w4, f, indent=2)\n",
    "\n",
    "md = f\"\"\"# Week 4 Tracker – Maternal Health Risk (RUN: {RUN_ID})\n",
    "\n",
    "**Week-3 prefix:** s3://{bucket}/{WEEK3_PREFIX}  \n",
    "**Week-4 prefix:** s3://{bucket}/{W4_PREFIX}\n",
    "\n",
    "## Benchmark (LogReg on Age + SystolicBP)\n",
    "Acc: {baseline_metrics['accuracy']:.3f} | Prec: {baseline_metrics['precision']:.3f} | Rec: {baseline_metrics['recall']:.3f} | F1: {baseline_metrics['f1']:.3f} | AUC: {baseline_metrics['roc_auc']:.3f}\n",
    "\n",
    "# XGBoost (full features)\n",
    "Acc: {metrics_compare['xgboost']['accuracy']:.3f} | Prec: {metrics_compare['xgboost']['precision']:.3f} | Rec: {metrics_compare['xgboost']['recall']:.3f} | F1: {metrics_compare['xgboost']['f1']:.3f} | AUC: {metrics_compare['xgboost']['roc_auc']:.3f}\n",
    "\n",
    "# Artifacts\n",
    "- Metrics JSON: {metrics_s3}\n",
    "- Baseline CM:  {bm_cm_s3}\n",
    "- XGBoost CM:   {xgb_cm_s3}\n",
    "- Model:        {xgb_model_artifact}\n",
    "- Batch Output: {batch_output_s3}\n",
    "\"\"\"\n",
    "with open(w4_tracker_dir/\"team_tracker_update_week4.md\",\"w\") as f:\n",
    "    f.write(md)\n",
    "\n",
    "boto3.client(\"s3\").upload_file(str(w4_tracker_dir/\"team_tracker_update_week4.json\"), bucket, f\"{W4_PREFIX}/team_tracker_update_week4.json\")\n",
    "boto3.client(\"s3\").upload_file(str(w4_tracker_dir/\"team_tracker_update_week4.md\"),   bucket, f\"{W4_PREFIX}/team_tracker_update_week4.md\")\n",
    "\n",
    "print(\"Week-4 tracker written & uploaded →\", f\"s3://{bucket}/{W4_PREFIX}/team_tracker_update_week4.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865223b-9aa9-41a0-9792-ed43e3e75df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "604ca106-1b79-4ace-beaa-b9609d5ee49c",
   "metadata": {},
   "source": [
    "## AAI-540 - Week 5: Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43fbd319-e179-4789-93a0-8de7cbd91d45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:23:00.415225Z",
     "iopub.status.busy": "2025-10-01T01:23:00.414904Z",
     "iopub.status.idle": "2025-10-01T01:23:01.020273Z",
     "shell.execute_reply": "2025-10-01T01:23:01.019423Z",
     "shell.execute_reply.started": "2025-10-01T01:23:00.415203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Region=us-east-1  Bucket=sagemaker-us-east-1-533267301342  Run=20251001-012300\n",
      "Using Week-4: s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20250930-205405\n",
      "Week-3:      s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20250930-205143\n",
      "Model:       s3://sagemaker-us-east-1-533267301342/sagemaker-xgboost-2025-09-30-21-05-51-135/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AAI-540 — Week 5 Monitoring (Maternal Health Risk Prediction)\n",
    "This notebook cell is a complete, reproducible, commented script that delivers:\n",
    "  1) Model monitors (Data Quality + Model Quality)\n",
    "  2) Data monitors (data capture + baseline)\n",
    "  3) Infrastructure monitors (CloudWatch alarms)\n",
    "  4) CloudWatch dashboard\n",
    "  5) S3-hosted Week-5 tracker (MD + JSON) for the Team Project Update\n",
    "\n",
    "\"\"\"\n",
    "import os, io, json, time\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.model_monitor import (\n",
    "    DataCaptureConfig,\n",
    "    DefaultModelMonitor,\n",
    "    ModelQualityMonitor,\n",
    "    CronExpressionGenerator,\n",
    "    DatasetFormat,\n",
    "    EndpointInput,      # older/newer SDK-friendly way to pass MQ attributes\n",
    ")\n",
    "\n",
    "# AWS context & run IDs\n",
    "boto_sess  = boto3.session.Session()\n",
    "region     = boto_sess.region_name\n",
    "sm_sess    = Session(boto_sess)\n",
    "role       = get_execution_role()\n",
    "s3c        = boto3.client(\"s3\")\n",
    "cw         = boto3.client(\"cloudwatch\")\n",
    "bucket     = sm_sess.default_bucket()\n",
    "\n",
    "RUN_ID    = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "W5_PREFIX = f\"aai540/maternal-risk/week5/{RUN_ID}\"\n",
    "\n",
    "print(f\"[INFO] Region={region}  Bucket={bucket}  Run={RUN_ID}\")\n",
    "\n",
    "# Load Week-4 tracker -> model artifact + Week-3 splits\n",
    "WEEK4_PREFIX = os.environ.get(\"WEEK4_PREFIX\")\n",
    "if not WEEK4_PREFIX:\n",
    "    base = \"aai540/maternal-risk/week4/\"\n",
    "    r = s3c.list_objects_v2(Bucket=bucket, Prefix=base, Delimiter=\"/\")\n",
    "    candidates = [cp[\"Prefix\"].rstrip(\"/\") for cp in r.get(\"CommonPrefixes\", [])]\n",
    "    assert candidates, f\"No Week-4 runs found in s3://{bucket}/{base}\"\n",
    "    WEEK4_PREFIX = sorted(candidates)[-1]\n",
    "\n",
    "w4_tracker_key = f\"{WEEK4_PREFIX}/team_tracker_update_week4.json\"\n",
    "w4 = json.loads(s3c.get_object(Bucket=bucket, Key=w4_tracker_key)[\"Body\"].read().decode(\"utf-8\"))\n",
    "MODEL_ARTIFACT = w4[\"artifacts\"][\"model_artifact\"]\n",
    "WEEK3_PREFIX   = w4[\"week3_prefix\"].replace(f\"s3://{bucket}/\", \"\")\n",
    "\n",
    "print(\"Using Week-4:\", f\"s3://{bucket}/{WEEK4_PREFIX}\")\n",
    "print(\"Week-3:     \", f\"s3://{bucket}/{WEEK3_PREFIX}\")\n",
    "print(\"Model:      \", MODEL_ARTIFACT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25626cca-b3fa-4d69-8387-8fbdc49545ca",
   "metadata": {},
   "source": [
    "#### Deploy endpoint with data capture (we'll send a few warm-up inferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6903e08c-1574-4275-944e-283c92abda31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:25:22.316924Z",
     "iopub.status.busy": "2025-10-01T01:25:22.316664Z",
     "iopub.status.idle": "2025-10-01T01:29:24.641609Z",
     "shell.execute_reply": "2025-10-01T01:29:24.640949Z",
     "shell.execute_reply.started": "2025-10-01T01:25:22.316905Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating model with name: mhr-xgb-model-20251001-012300\n",
      "INFO:sagemaker:Creating endpoint-config with name mhr-xgb-w5-20251001-012300\n",
      "INFO:sagemaker:Creating endpoint with name mhr-xgb-w5-20251001-012300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!Endpoint ready: mhr-xgb-w5-20251001-012300\n"
     ]
    }
   ],
   "source": [
    "xgb_image = retrieve(\"xgboost\", region, version=\"1.7-1\")\n",
    "ENDPOINT_NAME = f\"mhr-xgb-w5-{RUN_ID}\"\n",
    "\n",
    "model = Model(\n",
    "    image_uri=xgb_image,\n",
    "    model_data=MODEL_ARTIFACT,\n",
    "    role=role,\n",
    "    sagemaker_session=sm_sess,\n",
    "    name=f\"mhr-xgb-model-{RUN_ID}\",\n",
    ")\n",
    "\n",
    "data_capture_prefix = f\"{W5_PREFIX}/datacapture\"\n",
    "data_capture = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,                             # capture all early traffic\n",
    "    destination_s3_uri=f\"s3://{bucket}/{data_capture_prefix}\",\n",
    "    capture_options=[\"Input\", \"Output\"],\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    data_capture_config=data_capture,\n",
    ")\n",
    "\n",
    "# Re-attach safeguard (in case some Studio images return None)\n",
    "if predictor is None:\n",
    "    predictor = Predictor(endpoint_name=ENDPOINT_NAME, sagemaker_session=sm_sess)\n",
    "\n",
    "print(\"Endpoint ready:\", ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba9a78-551f-4c13-9254-3a70945c4abb",
   "metadata": {},
   "source": [
    "#### Warm up endpoint & upload matching ground-truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "505d306a-2aa6-42d4-a6c4-24c98e27deba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:30:28.127441Z",
     "iopub.status.busy": "2025-10-01T01:30:28.127157Z",
     "iopub.status.idle": "2025-10-01T01:30:28.541010Z",
     "shell.execute_reply": "2025-10-01T01:30:28.540212Z",
     "shell.execute_reply.started": "2025-10-01T01:30:28.127419Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_s3(key: str) -> pd.DataFrame:\n",
    "    obj = s3c.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "val = read_csv_s3(f\"{WEEK3_PREFIX}/val.csv\")\n",
    "feature_cols = [c for c in val.columns if c != \"label\"]\n",
    "\n",
    "# Send 25 rows to create captured traffic\n",
    "payload = \"\\n\".join(\",\".join(map(str, row)) for row in val[feature_cols].iloc[:25].values)\n",
    "_ = predictor.predict(payload, initial_args={\"ContentType\":\"text/csv\"})\n",
    "\n",
    "# Upload the corresponding labels (one per line, no header)\n",
    "gt_dir = Path(\"gt\"); gt_dir.mkdir(exist_ok=True)\n",
    "gt_file = gt_dir / \"val_labels.csv\"\n",
    "val[\"label\"].iloc[:25].to_csv(gt_file, index=False, header=False)\n",
    "s3c.upload_file(str(gt_file), bucket, f\"{W5_PREFIX}/ground-truth/val_labels.csv\")\n",
    "GROUND_TRUTH_S3 = f\"s3://{bucket}/{W5_PREFIX}/ground-truth/val_labels.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b21dc-af49-4218-af35-c1d7f24197da",
   "metadata": {},
   "source": [
    "#### Data Quality monitor - build baseline & schedule hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4e2cec84-ee67-4dc9-a87a-879219bd0f16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:30:30.772246Z",
     "iopub.status.busy": "2025-10-01T01:30:30.771854Z",
     "iopub.status.idle": "2025-10-01T01:35:55.787982Z",
     "shell.execute_reply": "2025-10-01T01:35:55.787292Z",
     "shell.execute_reply.started": "2025-10-01T01:30:30.772218Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2025-10-01-01-30-30-902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................\u001b[34m2025-10-01 01:33:21.694705: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:21.694756: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:23.453731: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:23.453775: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:23.453801: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-218-178.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:23.454180: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:25,433 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:533267301342:processing-job/baseline-suggestion-job-2025-10-01-01-30-30-902', 'ProcessingJobName': 'baseline-suggestion-job-2025-10-01-01-30-30-902', 'Environment': {'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week5/20251001-012300/baselines/xgb_train_inputs.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week5/20251001-012300/monitoring/data-quality', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.large', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::533267301342:role/LabRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 1800}}\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:25,433 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:25,433 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:25,434 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:25,434 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:25,434 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:25,753 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:25,754 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:25,754 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.large', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.large', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:25,767 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:25,767 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:25,767 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:27,500 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.218.178\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:\u001b[0m\n",
      "\u001b[34m/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:27,520 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:27,527 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-1962ccac-58f0-4f61-a675-216fa2319f68\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,567 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,591 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,595 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,600 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,621 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,621 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,621 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,621 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,682 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,707 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,707 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,712 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,716 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Oct 01 01:33:28\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,718 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,718 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,722 INFO util.GSet: 2.0% max memory 1.4 GB = 28.1 MB\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,723 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,781 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,785 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,786 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,786 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,786 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,786 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,786 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,786 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,786 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,786 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,786 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,787 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,827 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,827 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,827 INFO util.GSet: 1.0% max memory 1.4 GB = 14.0 MB\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,827 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,829 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,829 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,829 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,829 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,835 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,839 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,839 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,839 INFO util.GSet: 0.25% max memory 1.4 GB = 3.5 MB\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,840 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,849 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,850 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,850 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,853 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,854 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,857 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,857 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,857 INFO util.GSet: 0.029999999329447746% max memory 1.4 GB = 431.0 KB\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,857 INFO util.GSet: capacity      = 2^16 = 65536 entries\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,891 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1662128111-10.2.218.178-1759282408882\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,910 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:28,924 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:29,042 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:29,063 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:29,070 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.218.178\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:29,080 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:31,158 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:31,158 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:33,389 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:33,389 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:35,523 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:35,523 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:37,907 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:37,908 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:40,506 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:40,506 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:50,511 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:53,635 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:54,422 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:54,471 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:54,493 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:55,352 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:55,388 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:55,389 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:55,389 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:55,390 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:55,445 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 5602, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:55,463 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:55,465 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:55,566 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:55,567 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:55,567 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:55,567 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:55,568 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:56,224 INFO util.Utils: Successfully started service 'sparkDriver' on port 33417.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:56,292 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:56,355 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:56,392 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:56,393 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:56,441 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:56,502 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-00eada80-6a52-49cc-a088-a39b6567f8cf\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:56,531 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:56,600 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:56,659 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.2.218.178:33417/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1759282435342\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:57,546 INFO client.RMProxy: Connecting to ResourceManager at /10.2.218.178:8032\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:58,500 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:58,501 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:58,508 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (7640 MB per container)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:58,509 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:58,509 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:58,510 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:58,517 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-10-01 01:33:58,643 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:01,194 INFO yarn.Client: Uploading resource file:/tmp/spark-103949ec-dd84-4a7e-93fd-98b9282725b9/__spark_libs__9047612124861792077.zip -> hdfs://10.2.218.178/user/root/.sparkStaging/application_1759282417448_0001/__spark_libs__9047612124861792077.zip\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:04,197 INFO yarn.Client: Uploading resource file:/tmp/spark-103949ec-dd84-4a7e-93fd-98b9282725b9/__spark_conf__2258647185723994822.zip -> hdfs://10.2.218.178/user/root/.sparkStaging/application_1759282417448_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:04,264 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:04,265 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:04,265 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:04,265 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:04,266 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:04,314 INFO yarn.Client: Submitting application application_1759282417448_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:04,610 INFO impl.YarnClientImpl: Submitted application application_1759282417448_0001\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:05,617 INFO yarn.Client: Application report for application_1759282417448_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:05,624 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Wed Oct 01 01:34:05 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1759282444431\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1759282417448_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:06,635 INFO yarn.Client: Application report for application_1759282417448_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:07,639 INFO yarn.Client: Application report for application_1759282417448_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:08,643 INFO yarn.Client: Application report for application_1759282417448_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:09,647 INFO yarn.Client: Application report for application_1759282417448_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:10,651 INFO yarn.Client: Application report for application_1759282417448_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:11,655 INFO yarn.Client: Application report for application_1759282417448_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:12,659 INFO yarn.Client: Application report for application_1759282417448_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:13,456 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1759282417448_0001), /proxy/application_1759282417448_0001\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:13,676 INFO yarn.Client: Application report for application_1759282417448_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:13,677 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.218.178\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1759282444431\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1759282417448_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:13,679 INFO cluster.YarnClientSchedulerBackend: Application application_1759282417448_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:13,725 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44065.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:13,726 INFO netty.NettyBlockTransferService: Server created on 10.2.218.178:44065\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:13,729 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:13,760 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.218.178, 44065, None)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:13,774 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.218.178:44065 with 1458.6 MiB RAM, BlockManagerId(driver, 10.2.218.178, 44065, None)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:13,801 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.218.178, 44065, None)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:13,803 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.218.178, 44065, None)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:14,086 INFO util.log: Logging initialized @23299ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:16,374 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:21,658 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.218.178:47550) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:22,144 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:36939 with 2.7 GiB RAM, BlockManagerId(1, algo-1, 36939, None)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:27,370 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:28,031 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:28,280 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:28,292 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:30,849 INFO datasources.InMemoryFileIndex: It took 91 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:31,187 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:31,678 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:31,684 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.218.178:44065 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:31,695 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,257 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,260 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,265 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 57479\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,346 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,369 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,370 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,370 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,372 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,387 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,535 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,543 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,545 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.218.178:44065 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,546 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,576 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,578 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:32,660 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4629 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:33,151 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:36939 (size: 4.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:34,578 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:36939 (size: 39.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:35,309 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2683 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:35,313 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:35,321 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 2.860 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:35,674 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:35,676 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:35,679 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 3.332052 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,177 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,184 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,217 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 16 more fields>\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,539 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,559 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,560 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.218.178:44065 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,562 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,584 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,691 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,693 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,693 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,693 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,698 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,703 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,778 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,780 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,781 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.218.178:44065 (size: 8.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,783 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,784 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,784 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,789 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:39,897 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:36939 (size: 8.4 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:40,947 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:36939 (size: 39.1 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:41,115 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:36939 (size: 19.3 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:41,355 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1569 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:41,356 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:41,358 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 1.649 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:41,358 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:41,359 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:41,359 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 1.667573 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:42,070 INFO codegen.CodeGenerator: Code generated in 506.353762 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,029 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,228 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,232 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,236 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,237 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,240 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,248 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,290 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 115.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,294 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,295 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.218.178:44065 (size: 35.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,296 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,300 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,300 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,309 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:43,335 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:36939 (size: 35.1 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,493 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2186 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,499 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 2.242 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,500 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,501 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,501 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,502 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,509 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,723 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,727 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,727 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,728 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,728 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,731 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,774 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 167.6 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,777 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,780 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.218.178:44065 (size: 46.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,782 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,783 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,784 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,790 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,836 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:36939 (size: 46.1 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:45,920 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.218.178:47550\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:46,698 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 909 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:46,698 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:46,700 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.946 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:46,700 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:46,701 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:46,701 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.978025 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:46,776 INFO codegen.CodeGenerator: Code generated in 38.942674 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,232 INFO codegen.CodeGenerator: Code generated in 64.468818 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,329 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,331 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,331 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,331 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,332 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,336 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,358 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 38.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,365 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,367 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.218.178:44065 (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,369 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,370 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,372 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,375 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,405 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:36939 (size: 16.7 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,769 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 395 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,769 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,770 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.433 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,771 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,771 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:47,772 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.442019 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,490 INFO codegen.CodeGenerator: Code generated in 177.213315 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,550 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,551 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,552 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,552 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,621 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,627 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,667 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 75.8 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,672 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,675 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.218.178:44065 (size: 24.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,680 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,689 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,689 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,692 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:36939 in memory (size: 8.4 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,715 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,756 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:36939 (size: 24.5 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,780 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.218.178:44065 in memory (size: 8.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,864 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.218.178:44065 in memory (size: 46.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,869 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:36939 in memory (size: 46.1 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,925 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.218.178:44065 in memory (size: 35.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,935 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:36939 in memory (size: 35.1 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,981 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 265 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,981 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,982 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.350 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,983 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,984 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,986 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:48,986 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,004 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:36939 in memory (size: 16.7 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,007 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.2.218.178:44065 in memory (size: 16.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,487 INFO codegen.CodeGenerator: Code generated in 242.83157 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,520 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,525 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,525 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,525 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,525 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,533 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,536 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 67.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,541 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,550 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.218.178:44065 (size: 19.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,551 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,551 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,551 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,560 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,602 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:36939 (size: 19.7 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,613 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.218.178:47550\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,862 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 303 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,864 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.330 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,866 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,866 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,867 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,867 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.347182 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:49,981 INFO codegen.CodeGenerator: Code generated in 91.898058 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,264 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,278 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,279 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,279 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,279 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,280 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,284 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,310 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 31.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,315 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,317 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.218.178:44065 (size: 14.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,321 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,322 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,323 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,324 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:50,346 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:36939 (size: 14.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:51,988 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1664 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:51,989 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.704 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:51,990 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:51,990 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:51,991 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:51,991 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:51,991 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:51,992 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:51,995 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,000 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,002 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.2.218.178:44065 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,002 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,004 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,004 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,007 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,038 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:36939 (size: 3.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,052 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.2.218.178:47550\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,128 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 122 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,129 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,130 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.137 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,131 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,133 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,134 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.870087 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,657 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,657 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,657 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,657 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,678 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,684 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,701 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 84.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,703 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,705 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.2.218.178:44065 (size: 27.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,708 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,713 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,714 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,718 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:52,775 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:36939 (size: 27.5 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,419 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 700 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,419 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,420 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.735 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,420 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,420 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,421 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,421 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,543 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,545 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,545 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,545 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,546 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,547 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,562 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 168.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,571 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 46.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,574 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.2.218.178:44065 (size: 46.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,574 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,578 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,578 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,583 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,635 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:36939 (size: 46.3 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,690 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.2.218.178:47550\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,953 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 370 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,954 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,955 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.405 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,958 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,959 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:53,959 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.415696 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,374 INFO codegen.CodeGenerator: Code generated in 37.355603 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,462 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,464 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,465 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,465 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,466 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,467 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,478 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 38.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,480 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,484 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.2.218.178:44065 (size: 16.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,485 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,486 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,486 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,492 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,509 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:36939 (size: 16.6 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,641 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 149 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,641 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,643 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.173 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,645 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,645 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:54,645 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.182880 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,177 INFO codegen.CodeGenerator: Code generated in 124.428861 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,218 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,218 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,218 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,219 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,219 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,220 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,225 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 76.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,227 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 24.7 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,229 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.2.218.178:44065 (size: 24.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,233 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,234 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,235 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,236 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,260 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:36939 (size: 24.7 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,426 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 190 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,426 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,427 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.205 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,427 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,427 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,427 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,427 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,564 INFO codegen.CodeGenerator: Code generated in 72.104247 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,587 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,588 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,588 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,588 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,589 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,589 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,591 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 67.4 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,593 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,594 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.2.218.178:44065 (size: 19.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,597 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,597 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,597 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,599 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,614 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:36939 (size: 19.9 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,621 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.2.218.178:47550\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,738 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 139 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,738 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,739 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.149 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,740 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,740 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,741 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.153288 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,936 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,945 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.2.218.178:44065 in memory (size: 24.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,946 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:36939 in memory (size: 24.7 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,958 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,958 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,958 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,958 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,958 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:55,961 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,017 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.2.218.178:44065 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,021 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:36939 in memory (size: 19.9 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,034 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 31.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,037 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 14.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,038 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.2.218.178:44065 (size: 14.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,040 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,041 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,041 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,046 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,091 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:36939 (size: 14.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,121 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.2.218.178:44065 in memory (size: 27.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,122 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:36939 in memory (size: 27.5 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,158 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 113 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,159 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,160 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.198 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,161 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,161 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,162 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,162 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,162 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,170 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,171 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,172 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.2.218.178:44065 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,173 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,174 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,175 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,182 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,205 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:36939 in memory (size: 14.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,207 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:36939 (size: 3.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,212 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.2.218.178:44065 in memory (size: 14.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,213 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.2.218.178:47550\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,255 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 74 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,255 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,256 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.093 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,260 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,260 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.2.218.178:44065 in memory (size: 46.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,262 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,262 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.325251 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,271 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:36939 in memory (size: 46.3 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,336 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:36939 in memory (size: 19.7 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,339 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.2.218.178:44065 in memory (size: 19.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,356 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.218.178:44065 in memory (size: 24.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,361 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:36939 in memory (size: 24.5 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,448 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:36939 in memory (size: 16.6 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,475 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.2.218.178:44065 in memory (size: 16.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,565 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.2.218.178:44065 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,567 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:36939 in memory (size: 3.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,634 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,635 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,635 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,635 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,636 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,637 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,643 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 84.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,649 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,652 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.2.218.178:44065 (size: 27.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,653 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,653 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,654 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,655 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,674 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:36939 (size: 27.5 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,883 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 228 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,883 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,884 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.245 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,886 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,886 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,886 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,887 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,962 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,965 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,967 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,967 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,967 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,968 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,978 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 168.8 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,982 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 46.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,983 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.2.218.178:44065 (size: 46.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,983 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,984 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,984 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:56,986 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,002 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:36939 (size: 46.5 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,021 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.2.218.178:47550\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,136 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 150 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,136 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,137 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.167 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,137 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,138 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,138 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.174971 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,423 INFO codegen.CodeGenerator: Code generated in 38.346948 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,506 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,508 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,509 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,510 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,512 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,513 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,530 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 38.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,534 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,535 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.2.218.178:44065 (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,536 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,536 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,537 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,541 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,558 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:36939 (size: 16.7 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,646 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 105 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,647 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,648 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.133 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,650 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,650 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:57,651 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.143479 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,144 INFO codegen.CodeGenerator: Code generated in 164.919593 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,155 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,155 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,155 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,155 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,157 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,157 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,167 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 74.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,171 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,175 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.2.218.178:44065 (size: 24.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,176 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,195 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,195 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,198 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,233 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:36939 (size: 24.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,421 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 224 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,421 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,422 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.264 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,422 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,423 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,423 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,424 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,692 INFO codegen.CodeGenerator: Code generated in 141.786827 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,719 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,728 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,729 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,729 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,730 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,730 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,736 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 66.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,738 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,739 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.2.218.178:44065 (size: 19.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,740 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,741 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,741 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,743 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,755 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:36939 (size: 19.5 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,760 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.2.218.178:47550\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,908 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 166 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,908 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,909 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.175 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,910 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,910 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:58,910 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.183806 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,053 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,058 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,059 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,059 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,059 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,059 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,061 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,067 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 31.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,070 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 14.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,071 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.2.218.178:44065 (size: 14.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,072 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,072 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,072 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,074 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,087 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:36939 (size: 14.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,145 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 71 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,145 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,146 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.083 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,148 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,149 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,149 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,149 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,149 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,152 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,156 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,157 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.2.218.178:44065 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,158 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,158 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,158 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,160 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,171 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:36939 (size: 3.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,176 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.2.218.178:47550\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,225 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 65 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,226 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.076 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,227 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,229 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,229 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,229 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.172729 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,415 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,415 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,416 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,416 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,417 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,418 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,422 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 63.6 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,424 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 22.8 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,425 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.2.218.178:44065 (size: 22.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,431 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,431 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,432 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,433 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,451 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:36939 (size: 22.8 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,851 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 418 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,852 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,853 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.434 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,854 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,854 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,854 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,854 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,904 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,906 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,907 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,907 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,909 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,909 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,916 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 115.9 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,920 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,921 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.2.218.178:44065 (size: 35.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,923 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,924 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,924 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,927 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,946 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:36939 (size: 35.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:34:59,961 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.2.218.178:47550\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,189 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 263 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,189 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,191 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.281 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,191 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,191 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,192 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.287288 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,205 INFO codegen.CodeGenerator: Code generated in 9.851507 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,289 INFO codegen.CodeGenerator: Code generated in 20.815325 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,331 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,332 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,332 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,332 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,333 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,333 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,347 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 36.8 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,349 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,350 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.2.218.178:44065 (size: 16.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,350 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,351 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,351 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,353 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,366 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:36939 (size: 16.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,460 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 107 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,460 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,465 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 0.130 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,466 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,467 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,467 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 0.135662 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,583 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:36939 in memory (size: 16.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,589 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.2.218.178:44065 in memory (size: 16.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,642 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.2.218.178:44065 in memory (size: 19.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,655 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:36939 in memory (size: 19.5 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,706 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:36939 in memory (size: 3.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,713 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.2.218.178:44065 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,748 INFO codegen.CodeGenerator: Code generated in 76.447587 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,757 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,758 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,758 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,758 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,759 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,760 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,769 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 53.8 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,774 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 18.9 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,775 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.2.218.178:44065 (size: 18.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,775 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,790 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,790 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,792 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,813 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:36939 (size: 18.9 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,862 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:36939 in memory (size: 14.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,868 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.2.218.178:44065 in memory (size: 14.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,947 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 156 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,949 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,950 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.189 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,951 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,952 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,952 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:00,952 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,011 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:36939 in memory (size: 3.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,032 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.2.218.178:44065 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,092 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:36939 in memory (size: 14.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,095 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.2.218.178:44065 in memory (size: 14.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,105 INFO codegen.CodeGenerator: Code generated in 66.836013 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,118 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.2.218.178:44065 in memory (size: 24.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,121 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:36939 in memory (size: 24.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,129 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.2.218.178:44065 in memory (size: 35.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,139 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,141 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,141 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,141 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,141 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,142 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,144 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 43.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,146 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,146 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.2.218.178:44065 (size: 14.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,147 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:36939 in memory (size: 35.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,149 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,149 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,149 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,152 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,168 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:36939 (size: 14.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,178 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.2.218.178:47550\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,185 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:36939 in memory (size: 27.5 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,211 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.2.218.178:44065 in memory (size: 27.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,234 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:36939 in memory (size: 46.5 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,235 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.2.218.178:44065 in memory (size: 46.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,262 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.2.218.178:44065 in memory (size: 16.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,263 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:36939 in memory (size: 16.7 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,270 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.2.218.178:44065 in memory (size: 22.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,271 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:36939 in memory (size: 22.8 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,332 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 180 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,332 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,333 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.190 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,335 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,335 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,336 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.195728 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,396 INFO codegen.CodeGenerator: Code generated in 51.561487 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,484 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,486 INFO scheduler.DAGScheduler: Registering RDD 150 (countByKey at ColumnProfiler.scala:592) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,487 INFO scheduler.DAGScheduler: Got job 25 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,487 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,487 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,487 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,488 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,494 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 31.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,496 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 14.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,496 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.2.218.178:44065 (size: 14.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,497 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,498 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,498 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,499 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,514 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:36939 (size: 14.2 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,592 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 93 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,592 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,593 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (countByKey at ColumnProfiler.scala:592) finished in 0.104 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,594 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,594 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,594 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 37)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,594 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,595 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,596 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,598 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,599 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.2.218.178:44065 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,600 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,600 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,600 INFO cluster.YarnScheduler: Adding task set 37.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,602 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,614 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:36939 (size: 3.0 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,625 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.2.218.178:47550\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,643 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 29) in 42 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,643 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,644 INFO scheduler.DAGScheduler: ResultStage 37 (countByKey at ColumnProfiler.scala:592) finished in 0.049 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,644 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,645 INFO cluster.YarnScheduler: Killing all running tasks in stage 37: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:01,646 INFO scheduler.DAGScheduler: Job 25 finished: countByKey at ColumnProfiler.scala:592, took 0.161935 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,323 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,448 INFO codegen.CodeGenerator: Code generated in 16.819035 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,456 INFO scheduler.DAGScheduler: Registering RDD 156 (count at StatsGenerator.scala:66) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,457 INFO scheduler.DAGScheduler: Got map stage job 26 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,457 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 38 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,457 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,458 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,459 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[156] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,468 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 23.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,471 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 10.6 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,472 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.2.218.178:44065 (size: 10.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,473 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,473 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[156] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,473 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,477 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,495 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:36939 (size: 10.6 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,583 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 30) in 106 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,584 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,585 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (count at StatsGenerator.scala:66) finished in 0.124 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,587 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,587 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,588 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,588 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,623 INFO codegen.CodeGenerator: Code generated in 24.72916 ms\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,638 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,643 INFO scheduler.DAGScheduler: Got job 27 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,643 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,645 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,646 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,646 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[159] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,649 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 11.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,652 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,653 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.2.218.178:44065 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,655 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,655 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[159] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,655 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,657 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,670 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:36939 (size: 5.5 KiB, free: 2.7 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,677 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.2.218.178:47550\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,736 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 79 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,736 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,737 INFO scheduler.DAGScheduler: ResultStage 40 (count at StatsGenerator.scala:66) finished in 0.089 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,739 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,739 INFO cluster.YarnScheduler: Killing all running tasks in stage 40: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:02,740 INFO scheduler.DAGScheduler: Job 27 finished: count at StatsGenerator.scala:66, took 0.097294 s\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,117 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,139 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,188 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,191 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,200 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,257 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,306 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,315 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,331 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,343 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,384 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,384 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,384 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,430 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,439 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-e4df5c90-151c-4979-bcf6-445fb8c943ee\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,450 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-103949ec-dd84-4a7e-93fd-98b9282725b9\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,676 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2025-10-01 01:35:03,677 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: mhr-dq-sched-20251001-012300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality schedule: mhr-dq-sched-20251001-012300\n"
     ]
    }
   ],
   "source": [
    "# Baseline from Week-4 train CSV (drop label column 0 to reflect inference inputs)\n",
    "resp = s3c.list_objects_v2(Bucket=bucket, Prefix=f\"{WEEK4_PREFIX}/xgb/\")\n",
    "train_keys = [o[\"Key\"] for o in resp.get(\"Contents\", []) if o[\"Key\"].endswith(\"w4_xgb_train.csv\")]\n",
    "assert train_keys, \"Week-4 XGB train CSV not found.\"\n",
    "xgb_train_key = train_keys[0]\n",
    "\n",
    "raw = pd.read_csv(io.BytesIO(s3c.get_object(Bucket=bucket, Key=xgb_train_key)[\"Body\"].read()), header=None)\n",
    "raw.drop(columns=[0]).to_csv(\"xgb_train_inputs.csv\", index=False, header=False)\n",
    "baseline_inputs_key = f\"{W5_PREFIX}/baselines/xgb_train_inputs.csv\"\n",
    "s3c.upload_file(\"xgb_train_inputs.csv\", bucket, baseline_inputs_key)\n",
    "BASELINE_INPUTS_S3 = f\"s3://{bucket}/{baseline_inputs_key}\"\n",
    "\n",
    "dq_output = f\"s3://{bucket}/{W5_PREFIX}/monitoring/data-quality\"\n",
    "dqm = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,                 # < 1 hour cadence\n",
    "    sagemaker_session=sm_sess,\n",
    ")\n",
    "dqm.suggest_baseline(\n",
    "    baseline_dataset=BASELINE_INPUTS_S3,\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=dq_output,\n",
    "    wait=True,\n",
    ")\n",
    "\n",
    "DQ_SCHEDULE = f\"mhr-dq-sched-{RUN_ID}\"\n",
    "dqm.create_monitoring_schedule(\n",
    "    monitor_schedule_name=DQ_SCHEDULE,\n",
    "    endpoint_input=ENDPOINT_NAME,\n",
    "    output_s3_uri=dq_output,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")\n",
    "print(\"Data Quality schedule:\", DQ_SCHEDULE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9818e3f2-4808-40a8-ac8b-cc31d5078b71",
   "metadata": {},
   "source": [
    "#### Model Quality monitor - schedule hourly on previous full hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "98d10283-b224-4577-a185-e81333f8399c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:37:11.121119Z",
     "iopub.status.busy": "2025-10-01T01:37:11.120845Z",
     "iopub.status.idle": "2025-10-01T01:37:12.052867Z",
     "shell.execute_reply": "2025-10-01T01:37:12.052254Z",
     "shell.execute_reply.started": "2025-10-01T01:37:11.121098Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: mhr-mq-sched-20251001-012300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Quality schedule: mhr-mq-sched-20251001-012300\n"
     ]
    }
   ],
   "source": [
    "mq_output = f\"s3://{bucket}/{W5_PREFIX}/monitoring/model-quality\"\n",
    "mqm = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,                 # < 1 hour cadence\n",
    "    sagemaker_session=sm_sess,\n",
    ")\n",
    "\n",
    "ENDPOINT_DEST = \"/opt/ml/processing/input/endpoint\"\n",
    "\n",
    "endpoint_input = EndpointInput(\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    destination=ENDPOINT_DEST,\n",
    "    # previous full hour window (works reliably in us-east-1)\n",
    "    start_time_offset=\"-PT2H\",\n",
    "    end_time_offset=\"-PT1H\",\n",
    "    # model output is a single probability in CSV column 0\n",
    "    probability_attribute=\"0\",\n",
    "    probability_threshold_attribute=0.5,\n",
    ")\n",
    "\n",
    "MQ_SCHEDULE = f\"mhr-mq-sched-{RUN_ID}\"\n",
    "mqm.create_monitoring_schedule(\n",
    "    monitor_schedule_name=MQ_SCHEDULE,\n",
    "    endpoint_input=endpoint_input,\n",
    "    ground_truth_input=GROUND_TRUTH_S3,\n",
    "    problem_type=\"BinaryClassification\",\n",
    "    output_s3_uri=mq_output,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")\n",
    "print(\"Model Quality schedule:\", MQ_SCHEDULE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eed816-ac7a-4d86-a194-b3d4bffd1726",
   "metadata": {},
   "source": [
    "#### Infrastructure monitors  CloudWatch alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "05c08725-9290-4c6d-85fc-5d9d21f934f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:38:31.380624Z",
     "iopub.status.busy": "2025-10-01T01:38:31.380308Z",
     "iopub.status.idle": "2025-10-01T01:38:31.988805Z",
     "shell.execute_reply": "2025-10-01T01:38:31.988185Z",
     "shell.execute_reply.started": "2025-10-01T01:38:31.380597Z"
    }
   },
   "outputs": [],
   "source": [
    "ALARM_PREFIX = f\"MHR-W5-{RUN_ID}\"\n",
    "\n",
    "def put_alarm(metric, stat, comp, thresh, period=60, evals=1, unit=None):\n",
    "    dims=[{\"Name\":\"EndpointName\",\"Value\":ENDPOINT_NAME},\n",
    "          {\"Name\":\"VariantName\",\"Value\":\"AllTraffic\"}]\n",
    "    params=dict(\n",
    "        AlarmName=f\"{ALARM_PREFIX}-{metric}\",\n",
    "        Namespace=\"AWS/SageMaker\",\n",
    "        MetricName=metric,\n",
    "        Dimensions=dims,\n",
    "        Statistic=stat,\n",
    "        Period=period,\n",
    "        EvaluationPeriods=evals,\n",
    "        ComparisonOperator=comp,\n",
    "        Threshold=thresh,\n",
    "        ActionsEnabled=False,                # flip True + SNS if you want emails\n",
    "        TreatMissingData=\"notBreaching\",\n",
    "    )\n",
    "    if unit: params[\"Unit\"]=unit\n",
    "    cw.put_metric_alarm(**params)\n",
    "\n",
    "put_alarm(\"ModelLatency\",    \"Average\", \"GreaterThanThreshold\", 60000)\n",
    "put_alarm(\"OverheadLatency\", \"Average\", \"GreaterThanThreshold\", 60000)\n",
    "put_alarm(\"5XXErrors\",       \"Sum\",     \"GreaterThanThreshold\", 0)\n",
    "put_alarm(\"Invocations\",     \"Sum\",     \"GreaterThanThreshold\", 500)\n",
    "put_alarm(\"CPUUtilization\",  \"Average\", \"GreaterThanThreshold\", 95, unit=\"Percent\")\n",
    "put_alarm(\"MemoryUtilization\",\"Average\",\"GreaterThanThreshold\", 95, unit=\"Percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67671689-4bed-420f-987e-fa85c8a2a2d6",
   "metadata": {},
   "source": [
    "#### CloudWatch dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "65fc69d3-c918-4cd9-9372-32c636ba0874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:39:21.063034Z",
     "iopub.status.busy": "2025-10-01T01:39:21.062750Z",
     "iopub.status.idle": "2025-10-01T01:39:21.228668Z",
     "shell.execute_reply": "2025-10-01T01:39:21.227974Z",
     "shell.execute_reply.started": "2025-10-01T01:39:21.063012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CloudWatch dashboard: MHR-W5-Dashboard-20251001-012300\n"
     ]
    }
   ],
   "source": [
    "DASHBOARD = f\"MHR-W5-Dashboard-{RUN_ID}\"\n",
    "widgets = [\n",
    "  {\"type\":\"metric\",\"width\":12,\"height\":6,\"properties\":{\n",
    "      \"region\":region,\"title\":\"Latency (ms)\",\n",
    "      \"metrics\":[[\"AWS/SageMaker\",\"ModelLatency\",\"EndpointName\",ENDPOINT_NAME,\"VariantName\",\"AllTraffic\"],\n",
    "                 [\".\",\"OverheadLatency\",\".\",\".\",\".\",\".\"]],\n",
    "      \"period\":60,\"stat\":\"Average\",\"view\":\"timeSeries\"}},\n",
    "  {\"type\":\"metric\",\"width\":12,\"height\":6,\"properties\":{\n",
    "      \"region\":region,\"title\":\"Invocations & 5XX\",\n",
    "      \"metrics\":[[\"AWS/SageMaker\",\"Invocations\",\"EndpointName\",ENDPOINT_NAME,\"VariantName\",\"AllTraffic\"],\n",
    "                 [\".\",\"5XXErrors\",\".\",\".\",\".\",\".\"]],\n",
    "      \"period\":60,\"stat\":\"Sum\",\"view\":\"timeSeries\"}}\n",
    "]\n",
    "cw.put_dashboard(DashboardName=DASHBOARD, DashboardBody=json.dumps({\"widgets\":widgets}))\n",
    "print(\"CloudWatch dashboard:\", DASHBOARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c56d5-282d-4ee2-9865-164a53ebf20f",
   "metadata": {},
   "source": [
    "#### Week-5 tracker (JSON + MD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "459e8ed4-fb2c-45c9-806e-25a7353107b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:41:24.385859Z",
     "iopub.status.busy": "2025-10-01T01:41:24.385593Z",
     "iopub.status.idle": "2025-10-01T01:41:24.496841Z",
     "shell.execute_reply": "2025-10-01T01:41:24.496194Z",
     "shell.execute_reply.started": "2025-10-01T01:41:24.385839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week-5 monitoring setup complete.\n"
     ]
    }
   ],
   "source": [
    "tracker = {\n",
    "    \"week\": 5,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"week4_prefix\": f\"s3://{bucket}/{WEEK4_PREFIX}\",\n",
    "    \"week5_prefix\": f\"s3://{bucket}/{W5_PREFIX}\",\n",
    "    \"endpoint\": ENDPOINT_NAME,\n",
    "    \"datacapture_s3\": f\"s3://{bucket}/{data_capture_prefix}\",\n",
    "    \"schedules\": {\"data_quality\": DQ_SCHEDULE, \"model_quality\": MQ_SCHEDULE},\n",
    "    \"outputs\": {\"data_quality\": dq_output, \"model_quality\": mq_output, \"ground_truth\": GROUND_TRUTH_S3},\n",
    "    \"cloudwatch\": {\"alarms_prefix\": ALARM_PREFIX, \"dashboard\": DASHBOARD}\n",
    "}\n",
    "Path(\"w5_tracker\").mkdir(exist_ok=True)\n",
    "json.dump(tracker, open(\"w5_tracker/team_tracker_update_week5.json\",\"w\"), indent=2)\n",
    "open(\"w5_tracker/team_tracker_update_week5.md\",\"w\").write(\n",
    "    f\"# Week 5 Tracker – Maternal Health Risk (RUN: {RUN_ID})\\n\\n\"\n",
    "    f\"**Week-4:** s3://{bucket}/{WEEK4_PREFIX}\\n\"\n",
    "    f\"**Week-5:** s3://{bucket}/{W5_PREFIX}\\n\\n\"\n",
    "    f\"## Endpoint\\n- {ENDPOINT_NAME}\\n- Data Capture: s3://{bucket}/{data_capture_prefix}\\n\\n\"\n",
    "    f\"## Monitoring Schedules\\n- Data Quality: {DQ_SCHEDULE}\\n- Model Quality: {MQ_SCHEDULE}\\n\\n\"\n",
    "    f\"## Outputs\\n- DQ: {dq_output}\\n- MQ: {mq_output}\\n- GT: {GROUND_TRUTH_S3}\\n\\n\"\n",
    "    f\"## CloudWatch\\n- Alarms prefix: {ALARM_PREFIX}\\n- Dashboard: {DASHBOARD}\\n\"\n",
    ")\n",
    "s3c.upload_file(\"w5_tracker/team_tracker_update_week5.json\", bucket, f\"{W5_PREFIX}/team_tracker_update_week5.json\")\n",
    "s3c.upload_file(\"w5_tracker/team_tracker_update_week5.md\",   bucket, f\"{W5_PREFIX}/team_tracker_update_week5.md\")\n",
    "\n",
    "print(\"Week-5 monitoring setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd0818-60e0-4b84-87f8-0366d3dbf1bd",
   "metadata": {},
   "source": [
    "### week5_generate_reports.py <-- We'll run this after at least one schedule execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "606f0377-71b7-4350-81fc-bc473ed8baea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:41:34.663984Z",
     "iopub.status.busy": "2025-10-01T01:41:34.663708Z",
     "iopub.status.idle": "2025-10-01T01:41:35.106293Z",
     "shell.execute_reply": "2025-10-01T01:41:35.105665Z",
     "shell.execute_reply.started": "2025-10-01T01:41:34.663963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Report written → s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week5/20251001-012300/monitoring_report.md\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AAI-540 — Week 5 Reporting (student version)\n",
    "- Finds the latest Week-5 run\n",
    "- Lists Data/Model Quality outputs in S3\n",
    "- Writes compact MD + JSON report for Team Update\n",
    "\"\"\"\n",
    "\n",
    "import json, io\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "\n",
    "s3  = boto3.client(\"s3\")\n",
    "bkt = Session().default_bucket()\n",
    "\n",
    "# Find latest Week-5 folder\n",
    "base=\"aai540/maternal-risk/week5/\"\n",
    "resp=s3.list_objects_v2(Bucket=bkt, Prefix=base, Delimiter=\"/\")\n",
    "runs=[cp[\"Prefix\"].rstrip(\"/\") for cp in resp.get(\"CommonPrefixes\",[])]\n",
    "assert runs, f\"No Week-5 runs under s3://{bkt}/{base}\"\n",
    "w5=sorted(runs)[-1]\n",
    "\n",
    "# Load Week-5 tracker\n",
    "tracker=json.loads(s3.get_object(Bucket=bkt, Key=f\"{w5}/team_tracker_update_week5.json\")[\"Body\"].read())\n",
    "dq_out=tracker[\"outputs\"][\"data_quality\"]; mq_out=tracker[\"outputs\"][\"model_quality\"]\n",
    "\n",
    "def list_all(prefix_uri):\n",
    "    \"\"\"List all keys under a given s3://bucket/prefix and return as full s3 URIs.\"\"\"\n",
    "    prefix=prefix_uri.replace(f\"s3://{bkt}/\",\"\")\n",
    "    out=[]; token=None\n",
    "    while True:\n",
    "        kw=dict(Bucket=bkt, Prefix=prefix)\n",
    "        if token: kw[\"ContinuationToken\"]=token\n",
    "        r=s3.list_objects_v2(**kw)\n",
    "        out += [o[\"Key\"] for o in r.get(\"Contents\",[])]\n",
    "        token=r.get(\"NextContinuationToken\")\n",
    "        if not token: break\n",
    "    return [f\"s3://{bkt}/{k}\" for k in out]\n",
    "\n",
    "dq_files=list_all(dq_out)\n",
    "mq_files=list_all(mq_out)\n",
    "\n",
    "report={\n",
    "  \"week\":5,\n",
    "  \"run\":w5,\n",
    "  \"data_quality\":{\n",
    "    \"statistics\": next((f for f in dq_files if f.endswith(\"statistics.json\")), \"N/A\"),\n",
    "    \"constraints\": next((f for f in dq_files if f.endswith(\"constraints.json\")), \"N/A\"),\n",
    "    \"violations\": next((f for f in dq_files if \"constraint_violations.json\" in f), \"N/A\"),\n",
    "  },\n",
    "  \"model_quality\":{\n",
    "    \"files\": [f for f in mq_files if f.endswith((\".json\",\".csv\"))][-10:]\n",
    "  }\n",
    "}\n",
    "\n",
    "Path(\"w5_reports\").mkdir(exist_ok=True)\n",
    "open(\"w5_reports/monitoring_report.md\",\"w\").write(\n",
    "  \"# Week 5 Monitoring Report\\n\\n\"\n",
    "  f\"**Run:** {w5}\\n\\n\"\n",
    "  \"## Data Quality\\n\"\n",
    "  f\"- Statistics: {report['data_quality']['statistics']}\\n\"\n",
    "  f\"- Constraints: {report['data_quality']['constraints']}\\n\"\n",
    "  f\"- Violations: {report['data_quality']['violations']}\\n\\n\"\n",
    "  \"## Model Quality (latest files)\\n\" + \"\\n\".join(f\"- {p}\" for p in report[\"model_quality\"][\"files\"])\n",
    ")\n",
    "open(\"w5_reports/monitoring_report.json\",\"w\").write(json.dumps(report, indent=2))\n",
    "\n",
    "s3.upload_file(\"w5_reports/monitoring_report.md\",   bkt, f\"{w5}/monitoring_report.md\")\n",
    "s3.upload_file(\"w5_reports/monitoring_report.json\", bkt, f\"{w5}/monitoring_report.json\")\n",
    "\n",
    "print(\"📄 Report written →\", f\"s3://{bkt}/{w5}/monitoring_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047650ec-8836-4caa-afd2-85434a053941",
   "metadata": {},
   "source": [
    "### Week 5 cleanup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1ec85199-c14b-4bfb-add6-eda5e00debc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T01:42:05.960568Z",
     "iopub.status.busy": "2025-10-01T01:42:05.960272Z",
     "iopub.status.idle": "2025-10-01T01:42:07.554130Z",
     "shell.execute_reply": "2025-10-01T01:42:07.553383Z",
     "shell.execute_reply.started": "2025-10-01T01:42:05.960547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Cleanup complete for run: aai540/maternal-risk/week5/20251001-012300\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AAI-540 — Week 5 Cleanup\n",
    "Deletes: monitoring schedules, endpoint (+config), CW dashboard & alarms.\n",
    "Artifacts remain in S3 for grading.\n",
    "\"\"\"\n",
    "\n",
    "import json, boto3\n",
    "from sagemaker.session import Session\n",
    "\n",
    "s3 = boto3.client(\"s3\"); sm = boto3.client(\"sagemaker\"); cw=boto3.client(\"cloudwatch\")\n",
    "bkt=Session().default_bucket()\n",
    "\n",
    "# latest Week-5 run\n",
    "base=\"aai540/maternal-risk/week5/\"\n",
    "resp=s3.list_objects_v2(Bucket=bkt, Prefix=base, Delimiter=\"/\")\n",
    "w5=sorted([cp[\"Prefix\"].rstrip(\"/\") for cp in resp.get(\"CommonPrefixes\",[])])[-1]\n",
    "\n",
    "trk=json.loads(s3.get_object(Bucket=bkt, Key=f\"{w5}/team_tracker_update_week5.json\")[\"Body\"].read())\n",
    "ep=trk[\"endpoint\"]; dq=trk[\"schedules\"][\"data_quality\"]; mq=trk[\"schedules\"][\"model_quality\"]\n",
    "dash=trk[\"cloudwatch\"][\"dashboard\"]; prefix=trk[\"cloudwatch\"][\"alarms_prefix\"]\n",
    "\n",
    "# stop & delete schedules\n",
    "for name in [dq, mq]:\n",
    "    try: sm.stop_monitoring_schedule(MonitoringScheduleName=name)\n",
    "    except: pass\n",
    "    try: sm.delete_monitoring_schedule(MonitoringScheduleName=name)\n",
    "    except: pass\n",
    "\n",
    "# delete endpoint (+ config)\n",
    "try: sm.delete_endpoint(EndpointName=ep)\n",
    "except: pass\n",
    "try: sm.delete_endpoint_config(EndpointConfigName=ep)\n",
    "except: pass\n",
    "\n",
    "# delete dashboard\n",
    "try: cw.delete_dashboards(DashboardNames=[dash])\n",
    "except: pass\n",
    "\n",
    "# delete alarms with our prefix\n",
    "alarms=cw.describe_alarms(AlarmNamePrefix=prefix).get(\"MetricAlarms\",[])\n",
    "if alarms:\n",
    "    cw.delete_alarms(AlarmNames=[a[\"AlarmName\"] for a in alarms])\n",
    "\n",
    "print(\"🧹 Cleanup complete for run:\", w5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e971272-54e4-48f6-b832-9d5fc8a262d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

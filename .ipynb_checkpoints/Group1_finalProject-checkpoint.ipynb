{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fbb218c-3c95-479d-87eb-b38ac4101ba9",
   "metadata": {},
   "source": [
    "# Group 1 Final Project Work "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b79343-85ed-4a30-b48f-4475c77a98e1",
   "metadata": {},
   "source": [
    "#### Specific data on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3387b813-213e-4b8d-b13c-01d8e44ecc7e",
   "metadata": {},
   "source": [
    "### Maternal Health Risk Dataset Summary\n",
    "\n",
    "**Shape:** 808 records × 7 columns  \n",
    "\n",
    "**Columns:**\n",
    "- `Age`\n",
    "- `SystolicBP` (Systolic Blood Pressure)\n",
    "- `DiastolicBP` (Diastolic Blood Pressure)\n",
    "- `BS` (Blood Sugar level)\n",
    "- `BodyTemp` (Body Temperature, °F)\n",
    "- `HeartRate` (Heart Rate, bpm)\n",
    "- `RiskLevel` (Target: maternal health risk category)\n",
    "\n",
    "---\n",
    "\n",
    "#### First 5 Records\n",
    "| Age | SystolicBP | DiastolicBP | BS   | BodyTemp | HeartRate | RiskLevel  |\n",
    "|-----|------------|--------------|------|----------|-----------|------------|\n",
    "| 25  | 130        | 80           | 15.0 | 98.0     | 86        | high risk  |\n",
    "| 35  | 140        | 90           | 13.0 | 98.0     | 70        | high risk  |\n",
    "| 29  | 90         | 70           | 8.0  | 100.0    | 80        | high risk  |\n",
    "| 30  | 140        | 85           | 7.0  | 98.0     | 70        | high risk  |\n",
    "| 35  | 120        | 60           | 6.1  | 98.0     | 76        | low risk   |\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary Statistics\n",
    "- **Age:** 10–70 years (mean = 30.6, std = 13.9)  \n",
    "- **SystolicBP:** 70–160 mmHg (mean = 113, std = 19.9)  \n",
    "- **DiastolicBP:** 49–100 mmHg (mean = 77.5, std = 14.8)  \n",
    "- **BS:** 6–19 mmol/L (mean = 9.26, std = 3.62)  \n",
    "- **BodyTemp:** 98–103 °F (mean = 98.6, std = 1.39)  \n",
    "- **HeartRate:** 7–90 bpm (mean = 74.3, std = 8.82)  \n",
    "\n",
    "---\n",
    "\n",
    "#### Target Variable: RiskLevel\n",
    "- **Low risk:** 478 records (~59.2%)  \n",
    "- **High risk:** 330 records (~40.8%)  \n",
    "- **Medium risk:** Not present in this dataset version  \n",
    "\n",
    " Note: The dataset is binary-labeled (low vs. high risk), so if a 3-class model (low/mid/high) is needed, additional data preprocessing or augmentation may be required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ac998-df87-4fa8-a23c-c32dd6250ae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T00:29:58.488110Z",
     "iopub.status.busy": "2025-09-20T00:29:58.487852Z",
     "iopub.status.idle": "2025-09-20T00:29:58.491576Z",
     "shell.execute_reply": "2025-09-20T00:29:58.490728Z",
     "shell.execute_reply.started": "2025-09-20T00:29:58.488088Z"
    }
   },
   "source": [
    "### Week 3 - Training and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d938a2-5f7a-4a04-b9a7-c4dd14f5aa77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T00:33:39.970066Z",
     "iopub.status.busy": "2025-09-20T00:33:39.969690Z",
     "iopub.status.idle": "2025-09-20T00:33:39.973857Z",
     "shell.execute_reply": "2025-09-20T00:33:39.972733Z",
     "shell.execute_reply.started": "2025-09-20T00:33:39.970039Z"
    }
   },
   "source": [
    "#### Environment (auto role + auto bucket) and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204e533a-2bf3-413f-b03d-2bd34a912eea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:46:02.062550Z",
     "iopub.status.busy": "2025-10-04T12:46:02.062210Z",
     "iopub.status.idle": "2025-10-04T12:46:06.739239Z",
     "shell.execute_reply": "2025-10-04T12:46:06.738587Z",
     "shell.execute_reply.started": "2025-10-04T12:46:02.062520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Region: us-east-1\n",
      "Role:   arn:aws:iam::533267301342:role/LabRole\n",
      "S3:     s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606\n"
     ]
    }
   ],
   "source": [
    "# NO MANUAL SETTINGS: bucket/role are auto-detected from your Studio kernel.\n",
    "\n",
    "import os, io, json, time\n",
    "from pathlib import Path\n",
    "import boto3, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "# AWS context (auto from Studio kernel)\n",
    "boto_sess  = boto3.session.Session()\n",
    "region     = boto_sess.region_name\n",
    "sm_session = Session(boto_sess)\n",
    "role       = get_execution_role()\n",
    "bucket     = sm_session.default_bucket()\n",
    "\n",
    "RUN_ID    = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "S3_PREFIX = f\"aai540/maternal-risk/week3/{RUN_ID}\"\n",
    "\n",
    "# Paths\n",
    "DATA_CSV      = Path(\"Maternal_Risk.csv\")    # Using our Kaggle CSV here\n",
    "ARTIFACTS_DIR = Path(\"week3_outputs\"); ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:  \", role)\n",
    "print(\"S3:    \", f\"s3://{bucket}/{S3_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d4adc-f878-43ba-8088-6b1ae27302c2",
   "metadata": {},
   "source": [
    "#### Load data + lightweight EDA (plots + JSON summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8831eee7-cacf-4d1d-9ab8-722e439b677d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:46:08.904009Z",
     "iopub.status.busy": "2025-10-04T12:46:08.903551Z",
     "iopub.status.idle": "2025-10-04T12:46:09.869673Z",
     "shell.execute_reply": "2025-10-04T12:46:09.869018Z",
     "shell.execute_reply.started": "2025-10-04T12:46:08.903984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA done --> week3_outputs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 768x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert DATA_CSV.exists(), f\"Missing dataset at {DATA_CSV.resolve()}\"\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "\n",
    "# Keep a small EDA summary to feed the team tracker\n",
    "eda_summary = {\n",
    "    \"rows\": int(df.shape[0]),\n",
    "    \"cols\": int(df.shape[1]),\n",
    "    \"columns\": df.columns.tolist(),\n",
    "    \"dtypes\": {c: str(t) for c, t in df.dtypes.items()},\n",
    "    \"missing_counts\": df.isna().sum().to_dict(),\n",
    "    \"class_counts\": df[\"RiskLevel\"].value_counts().to_dict(),\n",
    "}\n",
    "json.dump(eda_summary, open(ARTIFACTS_DIR/\"eda_summary.json\",\"w\"), indent=2)\n",
    "\n",
    "# A few simple plots for the design doc\n",
    "(df[\"RiskLevel\"].value_counts()\n",
    "   .plot(kind=\"bar\", title=\"Class Distribution\")\n",
    "   .get_figure().savefig(ARTIFACTS_DIR/\"chart_class_distribution.png\")); plt.clf()\n",
    "\n",
    "df[\"Age\"].plot(kind=\"hist\", bins=20, title=\"Age Distribution\").get_figure().savefig(\n",
    "    ARTIFACTS_DIR/\"chart_age_hist.png\"); plt.clf()\n",
    "\n",
    "plt.boxplot([df[\"SystolicBP\"], df[\"DiastolicBP\"]], tick_labels=[\"SystolicBP\",\"DiastolicBP\"])\n",
    "plt.title(\"Blood Pressure Boxplots\"); plt.tight_layout()\n",
    "plt.savefig(ARTIFACTS_DIR/\"chart_bp_box.png\"); plt.clf()\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "corr = df[num_cols].corr()\n",
    "plt.imshow(corr, interpolation=\"nearest\"); plt.colorbar()\n",
    "plt.xticks(range(len(num_cols)), num_cols, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(len(num_cols)), num_cols)\n",
    "plt.title(\"Correlation Heatmap\"); plt.tight_layout()\n",
    "plt.savefig(ARTIFACTS_DIR/\"chart_corr_heatmap.png\"); plt.clf()\n",
    "\n",
    "print(\"EDA done -->\", ARTIFACTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d720b-3cbf-4645-81e5-cda60c4bf8c6",
   "metadata": {},
   "source": [
    "#### Feature engineering (clinically-motivated features + z-scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b953f3-132d-4faf-9042-56c570bb7df2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:46:13.692776Z",
     "iopub.status.busy": "2025-10-04T12:46:13.692480Z",
     "iopub.status.idle": "2025-10-04T12:46:13.755666Z",
     "shell.execute_reply": "2025-10-04T12:46:13.754745Z",
     "shell.execute_reply.started": "2025-10-04T12:46:13.692752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering done.\n"
     ]
    }
   ],
   "source": [
    "# We derive simple vitals-based features and also add z-scaled versions for linear models.\n",
    "X = df.copy()\n",
    "# simple clinically meaningful features\n",
    "X[\"PulsePressure\"]    = X[\"SystolicBP\"] - X[\"DiastolicBP\"]\n",
    "X[\"SBP_to_DBP\"]       = X[\"SystolicBP\"] / (X[\"DiastolicBP\"].replace(0, np.nan))\n",
    "X[\"Fever\"]            = (X[\"BodyTemp\"] > 99.5).astype(int)\n",
    "X[\"Tachycardia\"]      = (X[\"HeartRate\"] >= 100).astype(int)\n",
    "X[\"HypertensionFlag\"] = ((X[\"SystolicBP\"] >= 140) | (X[\"DiastolicBP\"] >= 90)).astype(int)\n",
    "\n",
    "# z-scaling for linear models\n",
    "cont = [\"Age\",\"SystolicBP\",\"DiastolicBP\",\"BS\",\"BodyTemp\",\"HeartRate\",\"PulsePressure\"]\n",
    "X[[f\"z_{c}\" for c in cont]] = StandardScaler().fit_transform(X[cont])\n",
    "\n",
    "# binary labels (in this dataset: \"low risk\" / \"high risk\")\n",
    "label_map = {\"low risk\": 0, \"high risk\": 1}\n",
    "y = X[\"RiskLevel\"].map(label_map)\n",
    "engineered = pd.concat([X.drop(columns=[\"RiskLevel\"]), y.rename(\"label\")], axis=1)\n",
    "engineered.to_csv(ARTIFACTS_DIR/\"maternal_features_full.csv\", index=False)\n",
    "json.dump(label_map, open(ARTIFACTS_DIR/\"label_map.json\",\"w\"), indent=2)\n",
    "\n",
    "# Stratified splits (train/val/test/prod = 40/10/10/40)\n",
    "X_no_target = engineered.drop(columns=[\"label\"])\n",
    "y_only      = engineered[\"label\"]\n",
    "\n",
    "X_tmp, X_prod, y_tmp, y_prod = train_test_split(\n",
    "    X_no_target, y_only, test_size=0.40, random_state=42, stratify=y_only\n",
    ")\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=(1/3), random_state=42, stratify=y_tmp\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_rem, y_rem, test_size=0.5, random_state=42, stratify=y_rem\n",
    ")\n",
    "\n",
    "def save_split(name, Xd, yd):\n",
    "    out = Xd.copy(); out[\"label\"] = yd.values\n",
    "    out.to_csv(ARTIFACTS_DIR/f\"{name}.csv\", index=False)\n",
    "    return out\n",
    "\n",
    "train_df = save_split(\"train\", X_train, y_train)\n",
    "val_df   = save_split(\"val\",   X_val,   y_val)\n",
    "test_df  = save_split(\"test\",  X_test,  y_test)\n",
    "prod_df  = save_split(\"production\", X_prod, y_prod)\n",
    "\n",
    "print(\"Feature engineering done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d0b66-2582-4e76-9aba-50b1c9193047",
   "metadata": {},
   "source": [
    "#### Stratified splits: 40% prod, 40% train, 10% val, 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882288ee-155b-42ff-a6a4-9583a26635cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:46:16.140148Z",
     "iopub.status.busy": "2025-10-04T12:46:16.139856Z",
     "iopub.status.idle": "2025-10-04T12:46:16.167199Z",
     "shell.execute_reply": "2025-10-04T12:46:16.166509Z",
     "shell.execute_reply.started": "2025-10-04T12:46:16.140126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 322, 'val': 81, 'test': 81, 'production': 324}\n"
     ]
    }
   ],
   "source": [
    "# We first carve out 40% as \"production\" holdout for future batch inference/monitoring.\n",
    "# The remaining 60% --> train (40%), val (10%), test (10%) of the original dataset.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 40% set aside for future batch inference/monitoring\n",
    "X_tmp, X_prod, y_tmp, y_prod = train_test_split(\n",
    "    X_no_target, y, test_size=0.40, random_state=42, stratify=y\n",
    ")\n",
    "# remaining 60% -> 40/10/10\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=(1/3), random_state=42, stratify=y_tmp\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_rem, y_rem, test_size=0.5, random_state=42, stratify=y_rem\n",
    ")\n",
    "\n",
    "def _save(name, Xd, yd):\n",
    "    out = Xd.copy(); out[\"label\"] = yd.values\n",
    "    out.to_csv(ARTIFACTS_DIR / f\"{name}.csv\", index=False)\n",
    "    return out\n",
    "\n",
    "train_df = _save(\"train\",      X_train, y_train)\n",
    "val_df   = _save(\"val\",        X_val,   y_val)\n",
    "test_df  = _save(\"test\",       X_test,  y_test)\n",
    "prod_df  = _save(\"production\", X_prod,  y_prod)\n",
    "\n",
    "print({\"train\":len(train_df), \"val\":len(val_df), \"test\":len(test_df), \"production\":len(prod_df)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818f682-6948-49c9-89ab-29fda6b786fa",
   "metadata": {},
   "source": [
    "#### Upload artifacts to S3 (no manual bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9c2544-d4f1-4f72-948b-2b27a0defd9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:46:17.289372Z",
     "iopub.status.busy": "2025-10-04T12:46:17.289084Z",
     "iopub.status.idle": "2025-10-04T12:46:17.856178Z",
     "shell.execute_reply": "2025-10-04T12:46:17.855584Z",
     "shell.execute_reply.started": "2025-10-04T12:46:17.289349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606/train.csv\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606/val.csv\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606/test.csv\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606/production.csv\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606/maternal_features_full.csv\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606/label_map.json\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606/eda_summary.json\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606/figures/chart_class_distribution.png\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606/figures/chart_age_hist.png\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606/figures/chart_bp_box.png\n",
      "Uploaded s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606/figures/chart_corr_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "# Upload the CSVs, label map, EDA summary, and figures to your default bucket/prefix.\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def s3_upload(local: Path, key: str):\n",
    "    s3.upload_file(str(local), bucket, f\"{S3_PREFIX}/{key}\")\n",
    "    print(\"Uploaded\", f\"s3://{bucket}/{S3_PREFIX}/{key}\")\n",
    "\n",
    "# CSVs + summaries\n",
    "for fname in [\"train.csv\",\"val.csv\",\"test.csv\",\"production.csv\",\n",
    "              \"maternal_features_full.csv\",\"label_map.json\",\"eda_summary.json\"]:\n",
    "    s3_upload(ARTIFACTS_DIR / fname, fname)\n",
    "\n",
    "# Plots\n",
    "for fname in [\"chart_class_distribution.png\",\"chart_age_hist.png\",\"chart_bp_box.png\",\"chart_corr_heatmap.png\"]:\n",
    "    s3_upload(ARTIFACTS_DIR / fname, f\"figures/{fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0280c5-9725-46f2-8231-3995920e9b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T15:14:52.533587Z",
     "iopub.status.busy": "2025-09-20T15:14:52.533071Z",
     "iopub.status.idle": "2025-09-20T15:14:52.537175Z",
     "shell.execute_reply": "2025-09-20T15:14:52.536233Z",
     "shell.execute_reply.started": "2025-09-20T15:14:52.533545Z"
    }
   },
   "source": [
    "#### Sanitize column names (Feature Store regex) & write sanitized splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d6fae3-05c2-4908-b3a2-30b588255b4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:46:22.510340Z",
     "iopub.status.busy": "2025-10-04T12:46:22.508517Z",
     "iopub.status.idle": "2025-10-04T12:46:22.549432Z",
     "shell.execute_reply": "2025-10-04T12:46:22.548704Z",
     "shell.execute_reply.started": "2025-10-04T12:46:22.510306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized splits saved.\n"
     ]
    }
   ],
   "source": [
    "# FS rules: names must be letters/numbers/hyphens only; must start with alnum; <=64 chars.\n",
    "\n",
    "def sanitize_col(name: str) -> str:\n",
    "    if name == \"SBP_to_DBP\": name = \"SBPtoDBP\"   # preserve meaning\n",
    "    if name.startswith(\"z_\"): name = \"z\" + name[2:]\n",
    "    name = name.replace(\"_\", \"\")\n",
    "    name = \"\".join(ch for ch in name if ch.isalnum() or ch == \"-\")\n",
    "    if not name or not name[0].isalnum(): name = \"f\" + name\n",
    "    return name[:64]\n",
    "\n",
    "def sanitize_df_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    newcols, seen = [], set()\n",
    "    for c in df.columns:\n",
    "        s = sanitize_col(c)\n",
    "        if s in seen:\n",
    "            i, base = 2, s\n",
    "            while f\"{base}{i}\" in seen: i += 1\n",
    "            s = f\"{base}{i}\"\n",
    "        newcols.append(s); seen.add(s)\n",
    "    out = df.copy(); out.columns = newcols\n",
    "    return out\n",
    "\n",
    "label_col = \"label\"\n",
    "def sanitize_split(df):\n",
    "    feats = df.drop(columns=[label_col])\n",
    "    feats = sanitize_df_cols(feats)\n",
    "    feats[label_col] = df[label_col].values\n",
    "    return feats\n",
    "\n",
    "train_s = sanitize_split(train_df); train_s.to_csv(ARTIFACTS_DIR/\"train_sanitized.csv\", index=False)\n",
    "val_s   = sanitize_split(val_df);   val_s.to_csv(ARTIFACTS_DIR/\"val_sanitized.csv\",   index=False)\n",
    "test_s  = sanitize_split(test_df);  test_s.to_csv(ARTIFACTS_DIR/\"test_sanitized.csv\", index=False)\n",
    "prod_s  = sanitize_split(prod_df);  prod_s.to_csv(ARTIFACTS_DIR/\"production_sanitized.csv\", index=False)\n",
    "\n",
    "print(\"Sanitized splits saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a97e7b-0caa-44c8-bf22-b0937dc35491",
   "metadata": {},
   "source": [
    "#### Create & ingest Feature Store (OFFLINE, unique names per run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41bbe8b7-6ef8-4721-b6be-35b82e2dc8e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:46:29.196697Z",
     "iopub.status.busy": "2025-10-04T12:46:29.196414Z",
     "iopub.status.idle": "2025-10-04T12:48:18.224906Z",
     "shell.execute_reply": "2025-10-04T12:48:18.223817Z",
     "shell.execute_reply.started": "2025-10-04T12:46:29.196676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status mhr-train-fg-20251004-124606: Creating\n",
      "[READY] mhr-train-fg-20251004-124606\n",
      "[OK] Ingested 322 rows → mhr-train-fg-20251004-124606\n",
      "Status mhr-val-fg-20251004-124606: Creating\n",
      "[READY] mhr-val-fg-20251004-124606\n",
      "[OK] Ingested 81 rows → mhr-val-fg-20251004-124606\n",
      "Status mhr-batch-fg-20251004-124606: Creating\n",
      "[READY] mhr-batch-fg-20251004-124606\n",
      "[OK] Ingested 324 rows → mhr-batch-fg-20251004-124606\n",
      "Feature Store complete: mhr-train-fg-20251004-124606 mhr-val-fg-20251004-124606 mhr-batch-fg-20251004-124606\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.feature_definition import FeatureDefinition, FeatureTypeEnum\n",
    "\n",
    "sm      = boto3.client(\"sagemaker\")\n",
    "session = Session(boto3.session.Session(region_name=region))\n",
    "\n",
    "def ensure_id_time(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_in.copy()\n",
    "    if \"recordid\" not in df.columns:\n",
    "        df[\"recordid\"] = range(1, len(df)+1)\n",
    "    if \"eventtime\" not in df.columns:\n",
    "        df[\"eventtime\"] = pd.Timestamp.utcnow().isoformat()\n",
    "    return df\n",
    "\n",
    "def to_boto_feature_defs(df: pd.DataFrame):\n",
    "    out = []\n",
    "    for c, d in df.dtypes.items():\n",
    "        if c == \"eventtime\":\n",
    "            t = \"String\"\n",
    "        elif pd.api.types.is_integer_dtype(d):\n",
    "            t = \"Integral\"\n",
    "        elif pd.api.types.is_float_dtype(d):\n",
    "            t = \"Fractional\"\n",
    "        else:\n",
    "            t = \"String\"\n",
    "        out.append({\"FeatureName\": c, \"FeatureType\": t})\n",
    "    return out\n",
    "\n",
    "def create_fg_boto3(name: str, df_local: pd.DataFrame, s3_uri: str):\n",
    "    fdefs = to_boto_feature_defs(df_local)\n",
    "    try:\n",
    "        resp = sm.create_feature_group(\n",
    "            FeatureGroupName=name,\n",
    "            RecordIdentifierFeatureName=\"recordid\",\n",
    "            EventTimeFeatureName=\"eventtime\",\n",
    "            FeatureDefinitions=fdefs,\n",
    "            OfflineStoreConfig={\"S3StorageConfig\": {\"S3Uri\": s3_uri}},\n",
    "            OnlineStoreConfig={\"EnableOnlineStore\": False},\n",
    "            RoleArn=role,\n",
    "            Description=f\"Maternal Health Risk – {name}\",\n",
    "        )\n",
    "        return resp\n",
    "    except sm.exceptions.ResourceInUse:\n",
    "        # Already exists --> safe to reuse after we confirm it's Created\n",
    "        return {\"FeatureGroupArn\": f\"arn:aws:sagemaker:{region}:{boto3.client('sts').get_caller_identity()['Account']}:feature-group/{name}\"}\n",
    "\n",
    "def wait_fg_created(name: str, timeout_s: int = 900, poll_s: int = 10):\n",
    "    start = time.time()\n",
    "    last = \"\"\n",
    "    while True:\n",
    "        desc = sm.describe_feature_group(FeatureGroupName=name)\n",
    "        status = desc.get(\"FeatureGroupStatus\", \"\")\n",
    "        if status == \"Created\":\n",
    "            print(f\"[READY] {name}\")\n",
    "            return desc\n",
    "        if status == \"CreateFailed\":\n",
    "            raise RuntimeError(f\"{name} failed: {desc.get('FailureReason')}\")\n",
    "        if time.time() - start > timeout_s:\n",
    "            raise TimeoutError(f\"Timeout waiting for {name} (last status={status})\")\n",
    "        if status != last:\n",
    "            print(f\"Status {name}: {status}\")\n",
    "            last = status\n",
    "        time.sleep(poll_s)\n",
    "\n",
    "def create_and_ingest(name_base: str, df_local: pd.DataFrame):\n",
    "    # unique FG names per run to avoid collisions\n",
    "    name = f\"{name_base}-{RUN_ID}\"             # e.g., mhr-train-fg-20250920-154301\n",
    "    assert \"_\" not in name, \"FG name must not contain underscores.\"\n",
    "    df_local = ensure_id_time(df_local)\n",
    "    s3_uri   = f\"s3://{bucket}/{S3_PREFIX}/feature-store/{name}\"\n",
    "\n",
    "    create_fg_boto3(name, df_local, s3_uri)\n",
    "    wait_fg_created(name)\n",
    "\n",
    "    fg = FeatureGroup(name=name, sagemaker_session=session)\n",
    "    fg.load_feature_definitions(data_frame=df_local)    # make sure SDK knows schema\n",
    "    fg.ingest(data_frame=df_local, max_workers=4, wait=True)\n",
    "    print(f\"[OK] Ingested {len(df_local)} rows → {name}\")\n",
    "    return name\n",
    "\n",
    "# Load sanitized splits\n",
    "train_s = pd.read_csv(ARTIFACTS_DIR/\"train_sanitized.csv\")\n",
    "val_s   = pd.read_csv(ARTIFACTS_DIR/\"val_sanitized.csv\")\n",
    "prod_s  = pd.read_csv(ARTIFACTS_DIR/\"production_sanitized.csv\")\n",
    "\n",
    "# Create OFFLINE FGs with unique names\n",
    "FG_TRAIN = create_and_ingest(\"mhr-train-fg\", train_s)\n",
    "FG_VAL   = create_and_ingest(\"mhr-val-fg\",   val_s)\n",
    "FG_BATCH = create_and_ingest(\"mhr-batch-fg\", prod_s)\n",
    "\n",
    "print(\"Feature Store complete:\", FG_TRAIN, FG_VAL, FG_BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91005583-4b6c-4ecc-8829-abe82eb153cd",
   "metadata": {},
   "source": [
    "#### Tracker update (JSON + Markdown) and upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cba8c17-f8e7-43f6-98c9-526e5b7e5fae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:48:18.226883Z",
     "iopub.status.busy": "2025-10-04T12:48:18.226510Z",
     "iopub.status.idle": "2025-10-04T12:48:18.339877Z",
     "shell.execute_reply": "2025-10-04T12:48:18.339151Z",
     "shell.execute_reply.started": "2025-10-04T12:48:18.226845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracker written & uploaded.\n"
     ]
    }
   ],
   "source": [
    "tracker = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"s3_prefix\": f\"s3://{bucket}/{S3_PREFIX}\",\n",
    "    \"dataset\": {\n",
    "        \"rows\": eda_summary[\"rows\"], \"cols\": eda_summary[\"cols\"],\n",
    "        \"class_counts\": eda_summary[\"class_counts\"],\n",
    "        \"dtypes\": eda_summary[\"dtypes\"],\n",
    "        \"missing\": eda_summary[\"missing_counts\"],\n",
    "    },\n",
    "    \"splits\": {\n",
    "        \"train_rows\": len(train_df), \"val_rows\": len(val_df),\n",
    "        \"test_rows\": len(test_df), \"prod_rows\": len(prod_df),\n",
    "    },\n",
    "    \"feature_store_groups\": [FG_TRAIN, FG_VAL, FG_BATCH],\n",
    "}\n",
    "with open(ARTIFACTS_DIR / \"team_tracker_update_week3.json\", \"w\") as f:\n",
    "    json.dump(tracker, f, indent=2)\n",
    "\n",
    "md = f\"\"\"# Week 3 Tracker — Maternal Health Risk (RUN: {RUN_ID})\n",
    "\n",
    "**S3 prefix:** s3://{bucket}/{S3_PREFIX}\n",
    "\n",
    "## Dataset\n",
    "- Rows: {eda_summary['rows']} | Cols: {eda_summary['cols']}\n",
    "- Classes: {eda_summary['class_counts']}\n",
    "\n",
    "## Splits\n",
    "- Train: {len(train_df)} (~40%)\n",
    "- Val:   {len(val_df)} (~10%)\n",
    "- Test:  {len(test_df)} (~10%)\n",
    "- Prod:  {len(prod_df)} (~40%)\n",
    "\n",
    "## Feature Store (offline)\n",
    "- {FG_TRAIN}\n",
    "- {FG_VAL}\n",
    "- {FG_BATCH}\n",
    "\"\"\"\n",
    "with open(ARTIFACTS_DIR / \"team_tracker_update_week3.md\", \"w\") as f:\n",
    "    f.write(md)\n",
    "\n",
    "# Upload tracker docs\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(str(ARTIFACTS_DIR/\"team_tracker_update_week3.json\"), bucket, f\"{S3_PREFIX}/team_tracker_update_week3.json\")\n",
    "s3.upload_file(str(ARTIFACTS_DIR/\"team_tracker_update_week3.md\"),   bucket, f\"{S3_PREFIX}/team_tracker_update_week3.md\")\n",
    "\n",
    "print(\"Tracker written & uploaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c762b5-98d8-4628-85c1-c35c454f3fd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:48:25.677635Z",
     "iopub.status.busy": "2025-10-04T12:48:25.677331Z",
     "iopub.status.idle": "2025-10-04T12:48:25.759221Z",
     "shell.execute_reply": "2025-10-04T12:48:25.758542Z",
     "shell.execute_reply.started": "2025-10-04T12:48:25.677607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_id': '20251004-124606',\n",
       " 's3_prefix': 's3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606',\n",
       " 'dataset': {'rows': 808,\n",
       "  'cols': 7,\n",
       "  'class_counts': {'low risk': 478, 'high risk': 330},\n",
       "  'dtypes': {'Age': 'int64',\n",
       "   'SystolicBP': 'int64',\n",
       "   'DiastolicBP': 'int64',\n",
       "   'BS': 'float64',\n",
       "   'BodyTemp': 'float64',\n",
       "   'HeartRate': 'int64',\n",
       "   'RiskLevel': 'object'},\n",
       "  'missing': {'Age': 0,\n",
       "   'SystolicBP': 0,\n",
       "   'DiastolicBP': 0,\n",
       "   'BS': 0,\n",
       "   'BodyTemp': 0,\n",
       "   'HeartRate': 0,\n",
       "   'RiskLevel': 0}},\n",
       " 'splits': {'train_rows': 322,\n",
       "  'val_rows': 81,\n",
       "  'test_rows': 81,\n",
       "  'prod_rows': 324},\n",
       " 'feature_store_groups': ['mhr-train-fg-20251004-124606',\n",
       "  'mhr-val-fg-20251004-124606',\n",
       "  'mhr-batch-fg-20251004-124606']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View\n",
    "\n",
    "import boto3, json\n",
    "s3 = boto3.client(\"s3\")\n",
    "obj = s3.get_object(Bucket=bucket, Key=f\"{S3_PREFIX}/team_tracker_update_week3.json\")\n",
    "tracker = json.load(obj[\"Body\"])\n",
    "tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dcc032-351e-4674-a70b-409d5bf9c472",
   "metadata": {},
   "source": [
    "## Week 4, Model Development and Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26772f-4388-4d0e-bd69-797a9f5d678b",
   "metadata": {},
   "source": [
    "#### Auto settings; continues from Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d69f10eb-4545-4fa0-9684-1230a160bee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:48:35.567146Z",
     "iopub.status.busy": "2025-10-04T12:48:35.566846Z",
     "iopub.status.idle": "2025-10-04T12:48:35.579801Z",
     "shell.execute_reply": "2025-10-04T12:48:35.579008Z",
     "shell.execute_reply.started": "2025-10-04T12:48:35.567120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Week-3: s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606\n",
      "Writing Week-4: s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20251004-124835\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AAI-540 — Week 4: Benchmark (LogReg), Main Model (XGBoost), Evaluation, Batch Deploy\n",
    "This script:\n",
    "  1) Reuses Week-3 splits from the latest run (or pass WEEK3_PREFIX env var)\n",
    "  2) Trains a *benchmark* Logistic Regression (Age + SystolicBP) via SKLearn Estimator\n",
    "  3) Trains a full-feature **XGBoost** model (built-in container)\n",
    "  4) Compares metrics on the test set and writes confusion matrices\n",
    "  5) Runs **Batch Transform** on Week-3 production data\n",
    "  6) Writes a Week-4 tracker + design-doc snippet with S3 artifact links\n",
    "\"\"\"\n",
    "\n",
    "import os, io, json, time, tarfile\n",
    "from pathlib import Path\n",
    "import boto3, sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Reuse Week-3 objects if they exist; otherwise, auto-init (no manual config)\n",
    "try:\n",
    "    bucket\n",
    "    sm_session\n",
    "    role\n",
    "except NameError:\n",
    "    boto_sess  = boto3.session.Session()\n",
    "    sm_session = Session(boto_sess)\n",
    "    role       = get_execution_role()\n",
    "    bucket     = sm_session.default_bucket()\n",
    "\n",
    "# Use the Week-3 S3 prefix if it’s still in memory; otherwise pick the latest run\n",
    "s3 = boto3.client(\"s3\")\n",
    "try:\n",
    "    WEEK3_PREFIX = S3_PREFIX  # from Week 3 cells\n",
    "except NameError:\n",
    "    base = \"aai540/maternal-risk/week3/\"\n",
    "    resp = s3.list_objects_v2(Bucket=bucket, Prefix=base, Delimiter=\"/\")\n",
    "    runs = [cp[\"Prefix\"].rstrip(\"/\") for cp in resp.get(\"CommonPrefixes\", [])]\n",
    "    assert runs, f\"No Week-3 artifacts found under s3://{bucket}/{base}\"\n",
    "    WEEK3_PREFIX = sorted(runs)[-1]\n",
    "\n",
    "# Create a unique Week-4 prefix\n",
    "RUN_ID = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "W4_PREFIX = f\"aai540/maternal-risk/week4/{RUN_ID}\"\n",
    "\n",
    "print(\"Using Week-3:\", f\"s3://{bucket}/{WEEK3_PREFIX}\")\n",
    "print(\"Writing Week-4:\", f\"s3://{bucket}/{W4_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5bb9a5-d6f2-4a3a-8089-bd8cd2d6bec2",
   "metadata": {},
   "source": [
    "#### Load Week-3 splits (train/val/test) from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a57735c-7195-4ac6-bb29-0220e395e7a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:48:38.761505Z",
     "iopub.status.busy": "2025-10-04T12:48:38.761209Z",
     "iopub.status.idle": "2025-10-04T12:48:38.920206Z",
     "shell.execute_reply": "2025-10-04T12:48:38.919533Z",
     "shell.execute_reply.started": "2025-10-04T12:48:38.761482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (322, 19) (81, 19) (81, 19)\n",
      "Train label balance: {0: 190, 1: 132}\n"
     ]
    }
   ],
   "source": [
    "# LOAD SPLITS\n",
    "\n",
    "def read_csv_from_s3(key: str) -> pd.DataFrame:\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "train = read_csv_from_s3(f\"{WEEK3_PREFIX}/train.csv\")\n",
    "val   = read_csv_from_s3(f\"{WEEK3_PREFIX}/val.csv\")\n",
    "test  = read_csv_from_s3(f\"{WEEK3_PREFIX}/test.csv\")\n",
    "\n",
    "label_col = \"label\"\n",
    "X_train, y_train = train.drop(columns=[label_col]), train[label_col]\n",
    "X_val,   y_val   = val.drop(columns=[label_col]),   val[label_col]\n",
    "X_test,  y_test  = test.drop(columns=[label_col]),  test[label_col]\n",
    "\n",
    "print(\"Loaded:\", train.shape, val.shape, test.shape)\n",
    "print(\"Train label balance:\", y_train.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da29fc7-8a77-4ae7-93ab-214810086a8f",
   "metadata": {},
   "source": [
    "#### Benchmark model in SageMaker (very simple: Logistic Regression on 2 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92889a8b-9ca5-418b-bd45-217acbc0e9f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:48:44.276575Z",
     "iopub.status.busy": "2025-10-04T12:48:44.276288Z",
     "iopub.status.idle": "2025-10-04T12:52:02.148627Z",
     "shell.execute_reply": "2025-10-04T12:52:02.142273Z",
     "shell.execute_reply.started": "2025-10-04T12:48:44.276550Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-scikit-learn-2025-10-04-12-48-44-555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 12:48:44 Starting - Starting the training job...\n",
      "2025-10-04 12:49:16 Starting - Preparing the instances for training...\n",
      "2025-10-04 12:49:38 Downloading - Downloading input data...\n",
      "2025-10-04 12:50:03 Downloading - Downloading the training image......\n",
      "2025-10-04 12:51:19 Training - Training image download completed. Training in progress.\n",
      "2025-10-04 12:51:19 Uploading - Uploading generated training model.\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m2025-10-04 12:51:13,837 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2025-10-04 12:51:13,841 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-10-04 12:51:13,844 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-10-04 12:51:13,862 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-10-04 12:51:14,180 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-10-04 12:51:14,183 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-10-04 12:51:14,202 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-10-04 12:51:14,204 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-10-04 12:51:14,223 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-10-04 12:51:14,226 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-10-04 12:51:14,243 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.large\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.large\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2025-10-04-12-48-44-555\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-533267301342/sagemaker-scikit-learn-2025-10-04-12-48-44-555/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"baseline_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"baseline_train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=baseline_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m5.large\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=baseline_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-533267301342/sagemaker-scikit-learn-2025-10-04-12-48-44-555/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.large\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"sagemaker-scikit-learn-2025-10-04-12-48-44-555\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-533267301342/sagemaker-scikit-learn-2025-10-04-12-48-44-555/source/sourcedir.tar.gz\",\"module_name\":\"baseline_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"baseline_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python39.zip:/miniconda3/lib/python3.9:/miniconda3/lib/python3.9/lib-dynload:/miniconda3/lib/python3.9/site-packages:/miniconda3/lib/python3.9/site-packages/setuptools/_vendor\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python baseline_train.py\u001b[0m\n",
      "\u001b[34m2025-10-04 12:51:14,245 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2025-10-04 12:51:14,246 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m2025-10-04 12:51:15,158 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-10-04 12:51:32 Completed - Training job completed\n",
      "Training seconds: 114\n",
      "Billable seconds: 114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7654320987654321,\n",
       " 'precision': 0.71875,\n",
       " 'recall': 0.696969696969697,\n",
       " 'f1': 0.7076923076923077,\n",
       " 'roc_auc': 0.790719696969697}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why: have a simple, interpretable baseline for comparison (MVP).\n",
    "from sagemaker.sklearn import SKLearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "bm_feats = [\"Age\", \"SystolicBP\"]\n",
    "assert all(f in X_train.columns for f in bm_feats), \"Expected baseline features missing.\"\n",
    "\n",
    "# Stage small CSVs (filenames don't matter inside channels)\n",
    "w4_local = Path(\"w4_benchmark\"); w4_local.mkdir(exist_ok=True)\n",
    "pd.concat([X_train[bm_feats], y_train], axis=1).to_csv(w4_local/\"train_benchmark.csv\", index=False)\n",
    "pd.concat([X_val[bm_feats],   y_val],   axis=1).to_csv(w4_local/\"val_benchmark.csv\",   index=False)\n",
    "\n",
    "bm_train_s3 = sm_session.upload_data(str(w4_local/\"train_benchmark.csv\"), key_prefix=f\"{W4_PREFIX}/benchmark\")\n",
    "bm_val_s3   = sm_session.upload_data(str(w4_local/\"val_benchmark.csv\"),   key_prefix=f\"{W4_PREFIX}/benchmark\")\n",
    "\n",
    "# Entry script: read first *.csv in each channel, fit LR, write metrics + model\n",
    "with open(\"baseline_train.py\",\"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "import os, glob, json, pathlib\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "def first_csv_in(d):\n",
    "    files = sorted(glob.glob(os.path.join(d, '*.csv')))\n",
    "    assert files, f'No CSV found in {d}'\n",
    "    return files[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_dir = os.environ.get('SM_CHANNEL_TRAIN', '/opt/ml/input/data/train')\n",
    "    val_dir   = os.environ.get('SM_CHANNEL_VAL',   '/opt/ml/input/data/val')\n",
    "    model_dir = os.environ.get('SM_MODEL_DIR',     '/opt/ml/model')\n",
    "\n",
    "    df_tr = pd.read_csv(first_csv_in(train_dir))\n",
    "    df_va = pd.read_csv(first_csv_in(val_dir))\n",
    "\n",
    "    Xtr, ytr = df_tr[['Age','SystolicBP']], df_tr['label']\n",
    "    Xva, yva = df_va[['Age','SystolicBP']], df_va['label']\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000).fit(Xtr, ytr)\n",
    "\n",
    "    pred  = clf.predict(Xva)\n",
    "    proba = clf.predict_proba(Xva)[:,1]\n",
    "    acc = accuracy_score(yva, pred)\n",
    "    p,r,f1,_ = precision_recall_fscore_support(yva, pred, average='binary', zero_division=0)\n",
    "    try: auc = roc_auc_score(yva, proba)\n",
    "    except: auc = float('nan')\n",
    "\n",
    "    pathlib.Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "    import joblib\n",
    "    joblib.dump(clf, os.path.join(model_dir, 'model.joblib'))\n",
    "    with open(os.path.join(model_dir, 'metrics.json'), 'w') as f:\n",
    "        json.dump({'accuracy':acc,'precision':p,'recall':r,'f1':f1,'roc_auc':auc}, f)\n",
    "\"\"\")\n",
    "\n",
    "bm_est = SKLearn(\n",
    "    entry_point=\"baseline_train.py\",\n",
    "    framework_version=\"1.2-1\",     # use a tag compatible with your Studio image\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    sagemaker_session=sm_session,\n",
    ")\n",
    "bm_est.fit({\"train\": bm_train_s3, \"val\": bm_val_s3})\n",
    "\n",
    "# Also compute baseline metrics on our held-out TEST for a clean comparison\n",
    "bm_clf   = LogisticRegression(max_iter=1000).fit(train[bm_feats], y_train)\n",
    "bm_proba = bm_clf.predict_proba(test[bm_feats])[:,1]\n",
    "bm_pred  = (bm_proba >= 0.5).astype(int)\n",
    "\n",
    "bm_acc = accuracy_score(y_test, bm_pred)\n",
    "bm_p, bm_r, bm_f1, _ = precision_recall_fscore_support(y_test, bm_pred, average='binary', zero_division=0)\n",
    "try:    bm_auc = roc_auc_score(y_test, bm_proba)\n",
    "except: bm_auc = float(\"nan\")\n",
    "\n",
    "baseline_metrics = {\"accuracy\":bm_acc,\"precision\":bm_p,\"recall\":bm_r,\"f1\":bm_f1,\"roc_auc\":bm_auc}\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07094141-33e0-4b5b-b255-92e41e2e46ec",
   "metadata": {},
   "source": [
    "#### MAIN MODEL in SageMaker (Built-in XGBoost, CSV mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bda5ca46-6455-4069-846a-0ff0dcbb7cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:52:08.796800Z",
     "iopub.status.busy": "2025-10-04T12:52:08.796520Z",
     "iopub.status.idle": "2025-10-04T12:55:26.461889Z",
     "shell.execute_reply": "2025-10-04T12:55:26.461264Z",
     "shell.execute_reply.started": "2025-10-04T12:52:08.796778Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-10-04-12-52-09-009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-04 12:52:10 Starting - Starting the training job...\n",
      "2025-10-04 12:52:25 Starting - Preparing the instances for training...\n",
      "2025-10-04 12:52:46 Downloading - Downloading input data...\n",
      "2025-10-04 12:53:32 Downloading - Downloading the training image......\n",
      "2025-10-04 12:54:37 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-04 12:54:40.296 ip-10-0-173-6.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-10-04 12:54:40.373 ip-10-0-173-6.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] File path /opt/ml/input/data/train of input files\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Making smlinks from folder /opt/ml/input/data/train to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] creating symlink between Path /opt/ml/input/data/train/w4_xgb_train.csv and destination /tmp/sagemaker_xgboost_input_data/w4_xgb_train.csv7404214163048145043\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] File path /opt/ml/input/data/validation of input files\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Making smlinks from folder /opt/ml/input/data/validation to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] creating symlink between Path /opt/ml/input/data/validation/w4_xgb_val.csv and destination /tmp/sagemaker_xgboost_input_data/w4_xgb_val.csv-8506159187154822902\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Train matrix has 322 rows and 18 columns\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Validation matrix has 81 rows\u001b[0m\n",
      "\u001b[34m[2025-10-04 12:54:40.775 ip-10-0-173-6.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-10-04 12:54:40.775 ip-10-0-173-6.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-10-04 12:54:40.776 ip-10-0-173-6.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-10-04 12:54:40.776 ip-10-0-173-6.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2025-10-04:12:54:40:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[2025-10-04 12:54:40.779 ip-10-0-173-6.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2025-10-04 12:54:40.783 ip-10-0-173-6.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.96216#011validation-auc:0.92771\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.96427#011validation-auc:0.92803\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.97984#011validation-auc:0.94760\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.98591#011validation-auc:0.95896\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.98706#011validation-auc:0.95896\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.99187#011validation-auc:0.98295\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.99155#011validation-auc:0.98169\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.99193#011validation-auc:0.98169\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.99288#011validation-auc:0.98232\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.99330#011validation-auc:0.97822\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.99400#011validation-auc:0.97727\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.99448#011validation-auc:0.97917\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.99474#011validation-auc:0.97980\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.99466#011validation-auc:0.97980\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.99498#011validation-auc:0.98106\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.99520#011validation-auc:0.98043\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.99524#011validation-auc:0.98106\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.99607#011validation-auc:0.98295\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.99635#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.99659#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.99703#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.99743#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.99735#011validation-auc:0.98359\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.99715#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.99743#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.99755#011validation-auc:0.98485\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.99767#011validation-auc:0.98359\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.99767#011validation-auc:0.98359\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.99807#011validation-auc:0.98485\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.99807#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.99803#011validation-auc:0.98359\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.99858#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.99862#011validation-auc:0.98485\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.99819#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.99815#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.99823#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.99850#011validation-auc:0.98927\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.99850#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.99866#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.99866#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.99862#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.99870#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:0.99870#011validation-auc:0.98864\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:0.99874#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:0.99878#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:0.99878#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:0.99890#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:0.99882#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:0.99886#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:0.99886#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:0.99886#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:0.99874#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:0.99894#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:0.99890#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:0.99894#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:0.99894#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:0.99894#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:0.99914#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[83]#011train-auc:0.99914#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[84]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:0.99894#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[93]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:0.99906#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:0.99906#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[100]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[101]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[102]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[103]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[104]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[105]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[106]#011train-auc:0.99906#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[107]#011train-auc:0.99906#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[108]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[109]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[110]#011train-auc:0.99906#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[111]#011train-auc:0.99906#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[112]#011train-auc:0.99906#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[113]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[114]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[115]#011train-auc:0.99902#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[116]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[117]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[118]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[119]#011train-auc:0.99894#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[120]#011train-auc:0.99894#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[121]#011train-auc:0.99894#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[122]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[123]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[124]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[125]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[126]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[127]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[128]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[129]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[130]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[131]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[132]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[133]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[134]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[135]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[136]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[137]#011train-auc:0.99930#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[138]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[139]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[140]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[141]#011train-auc:0.99930#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[142]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[143]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[144]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[145]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[146]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[147]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[148]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[149]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[150]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[151]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[152]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[153]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[154]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[155]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[156]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[157]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[158]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[159]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[160]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[161]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[162]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[163]#011train-auc:0.99942#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[164]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[165]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[166]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[167]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[168]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[169]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[170]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[171]#011train-auc:0.99942#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[172]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[173]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[174]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[175]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[176]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[177]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[178]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[179]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[180]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[181]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[182]#011train-auc:0.99942#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[183]#011train-auc:0.99942#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[184]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[185]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[186]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[187]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[188]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[189]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[190]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[191]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[192]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[193]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[194]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[195]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[196]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[197]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[198]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[199]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\n",
      "2025-10-04 12:55:01 Uploading - Uploading generated training model\n",
      "2025-10-04 12:55:01 Completed - Training job completed\n",
      "Training seconds: 135\n",
      "Billable seconds: 135\n",
      "XGBoost model artifact: s3://sagemaker-us-east-1-533267301342/sagemaker-xgboost-2025-10-04-12-52-09-009/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Why built-in? No entry_point needed; just CSV with label first (no header).\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import sagemaker\n",
    "\n",
    "# Helper to reorder columns to [label, features...] for CSV training\n",
    "def reorder_for_xgb(df):\n",
    "    cols = [label_col] + [c for c in df.columns if c != label_col]\n",
    "    return df[cols]\n",
    "\n",
    "xgb_train_local = reorder_for_xgb(train)\n",
    "xgb_val_local   = reorder_for_xgb(val)\n",
    "\n",
    "xgb_train_path = Path(\"w4_xgb_train.csv\"); xgb_val_path = Path(\"w4_xgb_val.csv\")\n",
    "xgb_train_local.to_csv(xgb_train_path, index=False, header=False)\n",
    "xgb_val_local.to_csv(xgb_val_path,   index=False, header=False)\n",
    "\n",
    "s3_xgb_train = sm_session.upload_data(str(xgb_train_path), key_prefix=f\"{W4_PREFIX}/xgb\")\n",
    "s3_xgb_val   = sm_session.upload_data(str(xgb_val_path),   key_prefix=f\"{W4_PREFIX}/xgb\")\n",
    "\n",
    "def get_xgb_image():\n",
    "    for ver in [\"1.7-1\", \"1.5-1\", \"1.3-1\"]:\n",
    "        try:\n",
    "            return sagemaker.image_uris.retrieve(\"xgboost\", sm_session.boto_region_name, version=ver)\n",
    "        except Exception as e:\n",
    "            print(f\"xgboost {ver} not available → trying next … ({e})\")\n",
    "    raise RuntimeError(\"No compatible built-in XGBoost image found.\")\n",
    "\n",
    "xgb_image_uri = get_xgb_image()\n",
    "\n",
    "xgb_est = Estimator(\n",
    "    image_uri=xgb_image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    sagemaker_session=sm_session,\n",
    "    hyperparameters={\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"eval_metric\":\"auc\",\n",
    "        \"max_depth\":5,\n",
    "        \"eta\":0.2,\n",
    "        \"min_child_weight\":1,\n",
    "        \"subsample\":0.8,\n",
    "        \"colsample_bytree\":0.8,\n",
    "        \"num_round\":200,\n",
    "        \"verbosity\":1,\n",
    "    },\n",
    ")\n",
    "\n",
    "# tell container the training data is CSV (otherwise it expects libsvm)\n",
    "train_input = TrainingInput(s3_data=s3_xgb_train, content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(s3_data=s3_xgb_val,   content_type=\"text/csv\")\n",
    "\n",
    "xgb_est.fit({\"train\": train_input, \"validation\": val_input}, wait=True)\n",
    "xgb_model_artifact = xgb_est.model_data\n",
    "print(\"XGBoost model artifact:\", xgb_model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b71be-2aac-4015-ab09-9a350ab6f71f",
   "metadata": {},
   "source": [
    "#### EVALUATE (compare main vs baseline on TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d01b9ed-7a61-488a-85b9-5514ac69af86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:55:36.899130Z",
     "iopub.status.busy": "2025-10-04T12:55:36.898814Z",
     "iopub.status.idle": "2025-10-04T12:55:37.457163Z",
     "shell.execute_reply": "2025-10-04T12:55:37.456526Z",
     "shell.execute_reply.started": "2025-10-04T12:55:36.899105Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_263/132528516.py:12: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  t.extractall(tmp_dir)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'baseline': {'accuracy': 0.7654320987654321,\n",
       "  'precision': 0.71875,\n",
       "  'recall': 0.696969696969697,\n",
       "  'f1': 0.7076923076923077,\n",
       "  'roc_auc': 0.790719696969697},\n",
       " 'xgboost': {'accuracy': 0.9876543209876543,\n",
       "  'precision': 1.0,\n",
       "  'recall': 0.9696969696969697,\n",
       "  'f1': 0.9846153846153847,\n",
       "  'roc_auc': 0.999368686868687}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def parse_s3_uri(uri: str):\n",
    "    assert uri.startswith(\"s3://\")\n",
    "    p = uri[5:]; b, k = p.split(\"/\", 1)\n",
    "    return b, k\n",
    "\n",
    "tmp_dir = Path(\"w4_tmp\"); tmp_dir.mkdir(exist_ok=True)\n",
    "bkt, key = parse_s3_uri(xgb_model_artifact)\n",
    "boto3.client(\"s3\").download_file(bkt, key, str(tmp_dir/\"model.tar.gz\"))\n",
    "with tarfile.open(tmp_dir/\"model.tar.gz\") as t:\n",
    "    t.extractall(tmp_dir)\n",
    "\n",
    "# Score test set with the trained booster\n",
    "dtest   = xgb.DMatrix(test.drop(columns=[label_col]), label=test[label_col])\n",
    "booster = xgb.Booster(); booster.load_model(str(tmp_dir/\"xgboost-model\"))\n",
    "xgb_proba = booster.predict(dtest)\n",
    "xgb_pred  = (xgb_proba >= 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "xgb_p, xgb_r, xgb_f1, _ = precision_recall_fscore_support(y_test, xgb_pred, average='binary', zero_division=0)\n",
    "xgb_auc = roc_auc_score(y_test, xgb_proba)\n",
    "\n",
    "metrics_compare = {\n",
    "    \"baseline\": baseline_metrics,\n",
    "    \"xgboost\":  {\"accuracy\":xgb_acc,\"precision\":xgb_p,\"recall\":xgb_r,\"f1\":xgb_f1,\"roc_auc\":xgb_auc},\n",
    "}\n",
    "metrics_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f1d73-a865-487d-8792-4f3cf2b7fd8f",
   "metadata": {},
   "source": [
    "#### Deploy via Batch Transform (score Week-3 production.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce2d23ae-613f-4512-ba37-4dcc5420d131",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T12:55:41.769179Z",
     "iopub.status.busy": "2025-10-04T12:55:41.768884Z",
     "iopub.status.idle": "2025-10-04T13:01:46.720962Z",
     "shell.execute_reply": "2025-10-04T13:01:46.720197Z",
     "shell.execute_reply.started": "2025-10-04T12:55:41.769156Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-10-04-12-55-41-922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature cols count: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2025-10-04-12-55-42-604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:51:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:51:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:51:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-10-04 13:00:51 +0000] [14] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2025-10-04 13:00:51 +0000] [14] [INFO] Listening at: unix:/tmp/gunicorn.sock (14)\u001b[0m\n",
      "\u001b[34m[2025-10-04 13:00:51 +0000] [14] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2025-10-04 13:00:51 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m[2025-10-04 13:00:51 +0000] [18] [INFO] Booting worker with pid: 18\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:53:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:53:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:53:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:53:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:53:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:53:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:01:00:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [04/Oct/2025:13:01:00 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:01:00:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [04/Oct/2025:13:01:00 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:01:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:01:00:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [04/Oct/2025:13:01:00 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:01:00:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [04/Oct/2025:13:01:00 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:01:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [04/Oct/2025:13:01:00 +0000] \"POST /invocations HTTP/1.1\" 200 6571 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [04/Oct/2025:13:01:00 +0000] \"POST /invocations HTTP/1.1\" 200 6571 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2025-10-04T13:01:00.393:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:51:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:51:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:51:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-10-04 13:00:51 +0000] [14] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2025-10-04 13:00:51 +0000] [14] [INFO] Listening at: unix:/tmp/gunicorn.sock (14)\u001b[0m\n",
      "\u001b[34m[2025-10-04 13:00:51 +0000] [14] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2025-10-04 13:00:51 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:00:51:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:00:51:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:00:51:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2025-10-04 13:00:51 +0000] [14] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[35m[2025-10-04 13:00:51 +0000] [14] [INFO] Listening at: unix:/tmp/gunicorn.sock (14)\u001b[0m\n",
      "\u001b[35m[2025-10-04 13:00:51 +0000] [14] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2025-10-04 13:00:51 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m[2025-10-04 13:00:51 +0000] [18] [INFO] Booting worker with pid: 18\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m[2025-10-04 13:00:51 +0000] [18] [INFO] Booting worker with pid: 18\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:53:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:53:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:53:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:53:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:53:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:00:53:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:00:53:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:00:53:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:00:53:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:00:53:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:00:53:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:00:53:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:01:00:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [04/Oct/2025:13:01:00 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:01:00:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [04/Oct/2025:13:01:00 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-10-04:13:01:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:01:00:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [04/Oct/2025:13:01:00 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:01:00:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [04/Oct/2025:13:01:00 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-04:13:01:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [04/Oct/2025:13:01:00 +0000] \"POST /invocations HTTP/1.1\" 200 6571 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [04/Oct/2025:13:01:00 +0000] \"POST /invocations HTTP/1.1\" 200 6571 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2025-10-04T13:01:00.393:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "Batch output: s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20251004-124835/batch/outputs\n"
     ]
    }
   ],
   "source": [
    "# Inference expects FEATURES ONLY (no label) in the SAME order as training.\n",
    "\n",
    "from sagemaker.inputs import TransformInput\n",
    "\n",
    "prod_df = read_csv_from_s3(f\"{WEEK3_PREFIX}/production.csv\")\n",
    "\n",
    "# Same feature order as used to create training CSVs\n",
    "FEATURE_COLS = [c for c in train.columns if c != label_col]\n",
    "print(\"Feature cols count:\", len(FEATURE_COLS))\n",
    "\n",
    "prod_features = prod_df[FEATURE_COLS].copy()\n",
    "bt_local = Path(\"w4_production_features_only.csv\")\n",
    "prod_features.to_csv(bt_local, index=False, header=False)\n",
    "\n",
    "s3_bt_input = sm_session.upload_data(str(bt_local), key_prefix=f\"{W4_PREFIX}/batch\")\n",
    "\n",
    "transformer = xgb_est.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/{W4_PREFIX}/batch/outputs\",\n",
    "    accept=\"text/csv\",\n",
    "    assemble_with=\"Line\",\n",
    ")\n",
    "\n",
    "transformer.transform(data=s3_bt_input, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()\n",
    "\n",
    "batch_output_s3 = transformer.output_path\n",
    "print(\"Batch output:\", batch_output_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dcfda8-a55b-44db-a67c-c98009fbf264",
   "metadata": {},
   "source": [
    "#### ARTIFACTS + DESIGN-DOC SNIPPET + TRACKER (upload to S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94aa204a-d7f5-4808-8c49-be0b851cfe97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T13:02:15.029358Z",
     "iopub.status.busy": "2025-10-04T13:02:15.029025Z",
     "iopub.status.idle": "2025-10-04T13:02:15.995393Z",
     "shell.execute_reply": "2025-10-04T13:02:15.994608Z",
     "shell.execute_reply.started": "2025-10-04T13:02:15.029327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Week 4 Findings — Model Development & Deployment\n",
      "\n",
      "**Benchmark (LogReg on Age + SystolicBP)**  \n",
      "Acc: 0.765 | Prec: 0.719 | Rec: 0.697 | F1: 0.708 | AUC: 0.791\n",
      "\n",
      "**XGBoost (full features)**  \n",
      "Acc: 0.988 | Prec: 1.000 | Rec: 0.970 | F1: 0.985 | AUC: 0.999\n",
      "\n",
      "**Artifacts**  \n",
      "- Metrics JSON: s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20251004-124835/metrics_compare.json  \n",
      "- Baseline CM:  s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20251004-124835/baseline_cm.png  \n",
      "- XGBoost CM:   s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20251004-124835/xgb_cm.png  \n",
      "- XGBoost Model Artifact: s3://sagemaker-us-east-1-533267301342/sagemaker-xgboost-2025-10-04-12-52-09-009/output/model.tar.gz  \n",
      "- Batch Transform Output: s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20251004-124835/batch/outputs\n",
      "\n",
      "Week-4 tracker written & uploaded → s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20251004-124835/team_tracker_update_week4.*\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_cm(cm, title, path):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest'); plt.title(title); plt.colorbar()\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\"); plt.tight_layout(); plt.savefig(path); plt.close()\n",
    "\n",
    "# Confusion matrices (test set)\n",
    "bm_pred = (LogisticRegression(max_iter=1000).fit(train[bm_feats], y_train)\n",
    "           .predict_proba(test[bm_feats])[:,1] >= 0.5).astype(int)\n",
    "bm_cm   = confusion_matrix(y_test, bm_pred)\n",
    "xgb_cm  = confusion_matrix(y_test, xgb_pred)\n",
    "\n",
    "art_dir = Path(\"w4_artifacts\"); art_dir.mkdir(exist_ok=True)\n",
    "plot_cm(bm_cm,  \"Baseline CM\", art_dir/\"baseline_cm.png\")\n",
    "plot_cm(xgb_cm, \"XGBoost CM\",  art_dir/\"xgb_cm.png\")\n",
    "with open(art_dir/\"metrics_compare.json\",\"w\") as f:\n",
    "    json.dump(metrics_compare, f, indent=2)\n",
    "\n",
    "def up(local, key):\n",
    "    boto3.client(\"s3\").upload_file(str(local), bucket, f\"{W4_PREFIX}/{key}\")\n",
    "    return f\"s3://{bucket}/{W4_PREFIX}/{key}\"\n",
    "\n",
    "metrics_s3 = up(art_dir/\"metrics_compare.json\", \"metrics_compare.json\")\n",
    "bm_cm_s3   = up(art_dir/\"baseline_cm.png\",      \"baseline_cm.png\")\n",
    "xgb_cm_s3  = up(art_dir/\"xgb_cm.png\",           \"xgb_cm.png\")\n",
    "\n",
    "design_doc_snippet = f\"\"\"\n",
    "### Week 4 Findings — Model Development & Deployment\n",
    "\n",
    "**Benchmark (LogReg on Age + SystolicBP)**  \n",
    "Acc: {baseline_metrics['accuracy']:.3f} | Prec: {baseline_metrics['precision']:.3f} | Rec: {baseline_metrics['recall']:.3f} | F1: {baseline_metrics['f1']:.3f} | AUC: {baseline_metrics['roc_auc']:.3f}\n",
    "\n",
    "**XGBoost (full features)**  \n",
    "Acc: {metrics_compare['xgboost']['accuracy']:.3f} | Prec: {metrics_compare['xgboost']['precision']:.3f} | Rec: {metrics_compare['xgboost']['recall']:.3f} | F1: {metrics_compare['xgboost']['f1']:.3f} | AUC: {metrics_compare['xgboost']['roc_auc']:.3f}\n",
    "\n",
    "**Artifacts**  \n",
    "- Metrics JSON: {metrics_s3}  \n",
    "- Baseline CM:  {bm_cm_s3}  \n",
    "- XGBoost CM:   {xgb_cm_s3}  \n",
    "- XGBoost Model Artifact: {xgb_model_artifact}  \n",
    "- Batch Transform Output: {batch_output_s3}\n",
    "\"\"\"\n",
    "print(design_doc_snippet)\n",
    "\n",
    "# Tracker (JSON + Markdown)\n",
    "w4_tracker_dir = Path(\"w4_tracker\"); w4_tracker_dir.mkdir(exist_ok=True)\n",
    "tracker_w4 = {\n",
    "    \"week\": \"4\",\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"week3_prefix\": f\"s3://{bucket}/{WEEK3_PREFIX}\",\n",
    "    \"week4_prefix\": f\"s3://{bucket}/{W4_PREFIX}\",\n",
    "    \"benchmark\": baseline_metrics,\n",
    "    \"xgboost\": metrics_compare[\"xgboost\"],\n",
    "    \"artifacts\": {\n",
    "        \"metrics_json\": metrics_s3,\n",
    "        \"baseline_cm\": bm_cm_s3,\n",
    "        \"xgb_cm\": xgb_cm_s3,\n",
    "        \"model_artifact\": xgb_model_artifact,\n",
    "        \"batch_output\": batch_output_s3\n",
    "    }\n",
    "}\n",
    "with open(w4_tracker_dir/\"team_tracker_update_week4.json\",\"w\") as f:\n",
    "    json.dump(tracker_w4, f, indent=2)\n",
    "\n",
    "md = f\"\"\"# Week 4 Tracker – Maternal Health Risk (RUN: {RUN_ID})\n",
    "\n",
    "**Week-3 prefix:** s3://{bucket}/{WEEK3_PREFIX}  \n",
    "**Week-4 prefix:** s3://{bucket}/{W4_PREFIX}\n",
    "\n",
    "## Benchmark (LogReg on Age + SystolicBP)\n",
    "Acc: {baseline_metrics['accuracy']:.3f} | Prec: {baseline_metrics['precision']:.3f} | Rec: {baseline_metrics['recall']:.3f} | F1: {baseline_metrics['f1']:.3f} | AUC: {baseline_metrics['roc_auc']:.3f}\n",
    "\n",
    "# XGBoost (full features)\n",
    "Acc: {metrics_compare['xgboost']['accuracy']:.3f} | Prec: {metrics_compare['xgboost']['precision']:.3f} | Rec: {metrics_compare['xgboost']['recall']:.3f} | F1: {metrics_compare['xgboost']['f1']:.3f} | AUC: {metrics_compare['xgboost']['roc_auc']:.3f}\n",
    "\n",
    "# Artifacts\n",
    "- Metrics JSON: {metrics_s3}\n",
    "- Baseline CM:  {bm_cm_s3}\n",
    "- XGBoost CM:   {xgb_cm_s3}\n",
    "- Model:        {xgb_model_artifact}\n",
    "- Batch Output: {batch_output_s3}\n",
    "\"\"\"\n",
    "with open(w4_tracker_dir/\"team_tracker_update_week4.md\",\"w\") as f:\n",
    "    f.write(md)\n",
    "\n",
    "boto3.client(\"s3\").upload_file(str(w4_tracker_dir/\"team_tracker_update_week4.json\"), bucket, f\"{W4_PREFIX}/team_tracker_update_week4.json\")\n",
    "boto3.client(\"s3\").upload_file(str(w4_tracker_dir/\"team_tracker_update_week4.md\"),   bucket, f\"{W4_PREFIX}/team_tracker_update_week4.md\")\n",
    "\n",
    "print(\"Week-4 tracker written & uploaded →\", f\"s3://{bucket}/{W4_PREFIX}/team_tracker_update_week4.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865223b-9aa9-41a0-9792-ed43e3e75df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "604ca106-1b79-4ace-beaa-b9609d5ee49c",
   "metadata": {},
   "source": [
    "## AAI-540 - Week 5: Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43fbd319-e179-4789-93a0-8de7cbd91d45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T13:03:29.305935Z",
     "iopub.status.busy": "2025-10-04T13:03:29.305668Z",
     "iopub.status.idle": "2025-10-04T13:03:29.832490Z",
     "shell.execute_reply": "2025-10-04T13:03:29.831739Z",
     "shell.execute_reply.started": "2025-10-04T13:03:29.305916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Region=us-east-1  Bucket=sagemaker-us-east-1-533267301342  Run=20251004-130329\n",
      "Using Week-4: s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week4/20251004-124835\n",
      "Week-3:      s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week3/20251004-124606\n",
      "Model:       s3://sagemaker-us-east-1-533267301342/sagemaker-xgboost-2025-10-04-12-52-09-009/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AAI-540 — Week 5 Monitoring (Maternal Health Risk Prediction)\n",
    "This notebook cell is a complete, reproducible, commented script that delivers:\n",
    "  1) Model monitors (Data Quality + Model Quality)\n",
    "  2) Data monitors (data capture + baseline)\n",
    "  3) Infrastructure monitors (CloudWatch alarms)\n",
    "  4) CloudWatch dashboard\n",
    "  5) S3-hosted Week-5 tracker (MD + JSON) for the Team Project Update\n",
    "\n",
    "\"\"\"\n",
    "import os, io, json, time\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.model_monitor import (\n",
    "    DataCaptureConfig,\n",
    "    DefaultModelMonitor,\n",
    "    ModelQualityMonitor,\n",
    "    CronExpressionGenerator,\n",
    "    DatasetFormat,\n",
    "    EndpointInput,      # older/newer SDK-friendly way to pass MQ attributes\n",
    ")\n",
    "\n",
    "# AWS context & run IDs\n",
    "boto_sess  = boto3.session.Session()\n",
    "region     = boto_sess.region_name\n",
    "sm_sess    = Session(boto_sess)\n",
    "role       = get_execution_role()\n",
    "s3c        = boto3.client(\"s3\")\n",
    "cw         = boto3.client(\"cloudwatch\")\n",
    "bucket     = sm_sess.default_bucket()\n",
    "\n",
    "RUN_ID    = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "W5_PREFIX = f\"aai540/maternal-risk/week5/{RUN_ID}\"\n",
    "\n",
    "print(f\"[INFO] Region={region}  Bucket={bucket}  Run={RUN_ID}\")\n",
    "\n",
    "# Load Week-4 tracker -> model artifact + Week-3 splits\n",
    "WEEK4_PREFIX = os.environ.get(\"WEEK4_PREFIX\")\n",
    "if not WEEK4_PREFIX:\n",
    "    base = \"aai540/maternal-risk/week4/\"\n",
    "    r = s3c.list_objects_v2(Bucket=bucket, Prefix=base, Delimiter=\"/\")\n",
    "    candidates = [cp[\"Prefix\"].rstrip(\"/\") for cp in r.get(\"CommonPrefixes\", [])]\n",
    "    assert candidates, f\"No Week-4 runs found in s3://{bucket}/{base}\"\n",
    "    WEEK4_PREFIX = sorted(candidates)[-1]\n",
    "\n",
    "w4_tracker_key = f\"{WEEK4_PREFIX}/team_tracker_update_week4.json\"\n",
    "w4 = json.loads(s3c.get_object(Bucket=bucket, Key=w4_tracker_key)[\"Body\"].read().decode(\"utf-8\"))\n",
    "MODEL_ARTIFACT = w4[\"artifacts\"][\"model_artifact\"]\n",
    "WEEK3_PREFIX   = w4[\"week3_prefix\"].replace(f\"s3://{bucket}/\", \"\")\n",
    "\n",
    "print(\"Using Week-4:\", f\"s3://{bucket}/{WEEK4_PREFIX}\")\n",
    "print(\"Week-3:     \", f\"s3://{bucket}/{WEEK3_PREFIX}\")\n",
    "print(\"Model:      \", MODEL_ARTIFACT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25626cca-b3fa-4d69-8387-8fbdc49545ca",
   "metadata": {},
   "source": [
    "#### Deploy endpoint with data capture (we'll send a few warm-up inferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6903e08c-1574-4275-944e-283c92abda31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T13:03:31.662542Z",
     "iopub.status.busy": "2025-10-04T13:03:31.662244Z",
     "iopub.status.idle": "2025-10-04T13:07:03.956333Z",
     "shell.execute_reply": "2025-10-04T13:07:03.955653Z",
     "shell.execute_reply.started": "2025-10-04T13:03:31.662519Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating model with name: mhr-xgb-model-20251004-130329\n",
      "INFO:sagemaker:Creating endpoint-config with name mhr-xgb-w5-20251004-130329\n",
      "INFO:sagemaker:Creating endpoint with name mhr-xgb-w5-20251004-130329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!Endpoint ready: mhr-xgb-w5-20251004-130329\n"
     ]
    }
   ],
   "source": [
    "xgb_image = retrieve(\"xgboost\", region, version=\"1.7-1\")\n",
    "ENDPOINT_NAME = f\"mhr-xgb-w5-{RUN_ID}\"\n",
    "\n",
    "model = Model(\n",
    "    image_uri=xgb_image,\n",
    "    model_data=MODEL_ARTIFACT,\n",
    "    role=role,\n",
    "    sagemaker_session=sm_sess,\n",
    "    name=f\"mhr-xgb-model-{RUN_ID}\",\n",
    ")\n",
    "\n",
    "data_capture_prefix = f\"{W5_PREFIX}/datacapture\"\n",
    "data_capture = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,                             # capture all early traffic\n",
    "    destination_s3_uri=f\"s3://{bucket}/{data_capture_prefix}\",\n",
    "    capture_options=[\"Input\", \"Output\"],\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    data_capture_config=data_capture,\n",
    ")\n",
    "\n",
    "# Re-attach safeguard (in case some Studio images return None)\n",
    "if predictor is None:\n",
    "    predictor = Predictor(endpoint_name=ENDPOINT_NAME, sagemaker_session=sm_sess)\n",
    "\n",
    "print(\"Endpoint ready:\", ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba9a78-551f-4c13-9254-3a70945c4abb",
   "metadata": {},
   "source": [
    "#### Warm up endpoint & upload matching ground-truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "505d306a-2aa6-42d4-a6c4-24c98e27deba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T13:14:03.740768Z",
     "iopub.status.busy": "2025-10-04T13:14:03.740487Z",
     "iopub.status.idle": "2025-10-04T13:14:03.944330Z",
     "shell.execute_reply": "2025-10-04T13:14:03.943486Z",
     "shell.execute_reply.started": "2025-10-04T13:14:03.740747Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_s3(key: str) -> pd.DataFrame:\n",
    "    obj = s3c.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "val = read_csv_s3(f\"{WEEK3_PREFIX}/val.csv\")\n",
    "feature_cols = [c for c in val.columns if c != \"label\"]\n",
    "\n",
    "# Send 25 rows to create captured traffic\n",
    "payload = \"\\n\".join(\",\".join(map(str, row)) for row in val[feature_cols].iloc[:25].values)\n",
    "_ = predictor.predict(payload, initial_args={\"ContentType\":\"text/csv\"})\n",
    "\n",
    "# Upload the corresponding labels (one per line, no header)\n",
    "gt_dir = Path(\"gt\"); gt_dir.mkdir(exist_ok=True)\n",
    "gt_file = gt_dir / \"val_labels.csv\"\n",
    "val[\"label\"].iloc[:25].to_csv(gt_file, index=False, header=False)\n",
    "s3c.upload_file(str(gt_file), bucket, f\"{W5_PREFIX}/ground-truth/val_labels.csv\")\n",
    "GROUND_TRUTH_S3 = f\"s3://{bucket}/{W5_PREFIX}/ground-truth/val_labels.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b21dc-af49-4218-af35-c1d7f24197da",
   "metadata": {},
   "source": [
    "#### Data Quality monitor - build baseline & schedule hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e2cec84-ee67-4dc9-a87a-879219bd0f16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T13:14:27.110223Z",
     "iopub.status.busy": "2025-10-04T13:14:27.109940Z",
     "iopub.status.idle": "2025-10-04T13:19:54.304561Z",
     "shell.execute_reply": "2025-10-04T13:19:54.303354Z",
     "shell.execute_reply.started": "2025-10-04T13:14:27.110199Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2025-10-04-13-14-27-285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................\u001b[34m2025-10-04 13:17:07.719521: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:07.719562: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:09.539127: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:09.539161: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:09.539187: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-137-209.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:09.539498: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:11,548 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:533267301342:processing-job/baseline-suggestion-job-2025-10-04-13-14-27-285', 'ProcessingJobName': 'baseline-suggestion-job-2025-10-04-13-14-27-285', 'Environment': {'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week5/20251004-130329/baselines/xgb_train_inputs.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week5/20251004-130329/monitoring/data-quality', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.large', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::533267301342:role/LabRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 1800}}\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:11,548 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:11,548 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:11,548 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:11,548 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:11,549 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:11,861 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:11,862 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:11,863 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.large', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.large', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:11,875 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:11,875 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:11,875 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:13,596 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.137.209\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoo\u001b[0m\n",
      "\u001b[34mp/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:13,629 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:13,636 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-9e85be4c-94ef-4c24-a250-c6bf889864cb\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,752 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,775 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,777 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,782 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,801 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,801 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,801 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,802 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,870 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,896 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,897 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,901 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,905 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Oct 04 13:17:14\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,907 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,907 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,911 INFO util.GSet: 2.0% max memory 1.4 GB = 28.4 MB\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,911 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,975 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,979 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,979 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,979 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,979 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,979 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,979 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,979 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,979 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,979 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,979 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:14,979 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,019 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,019 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,019 INFO util.GSet: 1.0% max memory 1.4 GB = 14.2 MB\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,019 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,021 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,021 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,021 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,021 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,026 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,031 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,031 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,031 INFO util.GSet: 0.25% max memory 1.4 GB = 3.6 MB\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,031 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,040 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,040 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,040 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,044 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,044 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,049 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,049 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,049 INFO util.GSet: 0.029999999329447746% max memory 1.4 GB = 436.4 KB\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,049 INFO util.GSet: capacity      = 2^16 = 65536 entries\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,085 INFO namenode.FSImage: Allocated new BlockPoolId: BP-467049054-10.0.137.209-1759583835076\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,103 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,117 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,245 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,266 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,273 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.137.209\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:15,283 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:17,361 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:17,362 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:19,589 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:19,590 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:21,911 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:21,911 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:24,317 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:24,318 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:26,886 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:26,887 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:36,897 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:39,967 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:40,920 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:41,014 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:41,055 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,041 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,074 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,074 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,075 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,077 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,122 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 5664, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,140 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,142 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,251 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,252 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,252 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,253 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,253 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,866 INFO util.Utils: Successfully started service 'sparkDriver' on port 34623.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:42,935 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:43,001 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:43,038 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:43,039 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:43,087 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:43,144 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-844bd182-de69-435a-ba97-9db6f72f0522\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:43,169 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:43,233 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:43,283 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.137.209:34623/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1759583862034\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:44,137 INFO client.RMProxy: Connecting to ResourceManager at /10.0.137.209:8032\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:45,015 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:45,015 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:45,023 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (7724 MB per container)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:45,024 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:45,024 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:45,024 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:45,032 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:45,144 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:47,704 INFO yarn.Client: Uploading resource file:/tmp/spark-c13f98eb-c920-4745-82f8-c40c5ce3f71f/__spark_libs__4699812012355119371.zip -> hdfs://10.0.137.209/user/root/.sparkStaging/application_1759583843826_0001/__spark_libs__4699812012355119371.zip\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:49,845 INFO yarn.Client: Uploading resource file:/tmp/spark-c13f98eb-c920-4745-82f8-c40c5ce3f71f/__spark_conf__1834439724173775761.zip -> hdfs://10.0.137.209/user/root/.sparkStaging/application_1759583843826_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:49,916 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:49,918 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:49,919 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:49,919 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:49,922 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:49,961 INFO yarn.Client: Submitting application application_1759583843826_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:50,211 INFO impl.YarnClientImpl: Submitted application application_1759583843826_0001\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:51,219 INFO yarn.Client: Application report for application_1759583843826_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:51,225 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Sat Oct 04 13:17:50 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1759583870074\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1759583843826_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:52,229 INFO yarn.Client: Application report for application_1759583843826_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:53,234 INFO yarn.Client: Application report for application_1759583843826_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:54,239 INFO yarn.Client: Application report for application_1759583843826_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:55,246 INFO yarn.Client: Application report for application_1759583843826_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:56,250 INFO yarn.Client: Application report for application_1759583843826_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:57,260 INFO yarn.Client: Application report for application_1759583843826_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:58,264 INFO yarn.Client: Application report for application_1759583843826_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:58,265 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.137.209\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1759583870074\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1759583843826_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:58,267 INFO cluster.YarnClientSchedulerBackend: Application application_1759583843826_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:58,301 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43475.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:58,301 INFO netty.NettyBlockTransferService: Server created on 10.0.137.209:43475\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:58,303 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:58,315 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.137.209, 43475, None)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:58,321 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.137.209:43475 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.137.209, 43475, None)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:58,329 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.137.209, 43475, None)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:58,331 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.137.209, 43475, None)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:58,541 INFO util.log: Logging initialized @21092ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-10-04 13:17:58,903 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1759583843826_0001), /proxy/application_1759583843826_0001\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:01,084 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:06,503 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.137.209:49950) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:06,730 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:40157 with 2.8 GiB RAM, BlockManagerId(1, algo-1, 40157, None)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:13,979 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:14,384 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:14,503 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:14,513 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:16,322 INFO datasources.InMemoryFileIndex: It took 72 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:16,601 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,094 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,102 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.137.209:43475 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,109 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,702 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,710 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,717 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 57479\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,813 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,834 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,835 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,837 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,839 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,855 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,980 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,990 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,991 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.137.209:43475 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:17,992 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:18,020 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:18,022 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:18,082 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4629 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:18,511 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:40157 (size: 4.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:19,829 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:40157 (size: 39.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:20,471 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2407 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:20,481 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:20,500 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 2.572 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:20,518 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:20,521 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:20,527 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 2.713428 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:20,926 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.137.209:43475 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:20,951 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:40157 in memory (size: 4.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:20,990 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.137.209:43475 in memory (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:20,996 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:40157 in memory (size: 39.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:24,819 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:24,821 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:24,831 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 16 more fields>\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,301 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,321 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,322 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.137.209:43475 (size: 39.1 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,324 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,350 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,441 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,445 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,446 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,447 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,451 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,456 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,603 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,618 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,620 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.137.209:43475 (size: 8.4 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,622 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,623 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,623 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,628 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:25,763 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:40157 (size: 8.4 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:27,183 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:40157 (size: 39.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:27,363 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:40157 (size: 19.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:27,601 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1976 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:27,608 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:27,610 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 2.144 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:27,611 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:27,612 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:27,613 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 2.171980 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:28,161 INFO codegen.CodeGenerator: Code generated in 450.096223 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,252 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,561 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,566 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,567 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,567 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,571 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,573 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,613 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 115.1 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,618 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,618 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.137.209:43475 (size: 35.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,620 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,622 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,624 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,633 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:29,662 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:40157 (size: 35.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,281 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1650 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,281 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,284 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.706 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,285 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,285 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,286 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,286 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,491 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,495 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,496 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,497 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,498 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,504 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,574 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 167.6 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,585 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.1 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,587 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.137.209:43475 (size: 46.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,591 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,592 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,592 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,603 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,707 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:40157 (size: 46.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:31,824 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.137.209:49950\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:32,541 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 939 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:32,541 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:32,543 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 1.012 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:32,544 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:32,544 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:32,545 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 1.052923 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:32,661 INFO codegen.CodeGenerator: Code generated in 79.124123 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,336 INFO codegen.CodeGenerator: Code generated in 87.976593 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,511 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.137.209:43475 in memory (size: 8.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,515 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:40157 in memory (size: 8.4 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,550 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,552 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,553 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,553 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,554 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,560 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,577 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:40157 in memory (size: 35.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,587 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.137.209:43475 in memory (size: 35.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,648 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.137.209:43475 in memory (size: 46.1 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,657 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:40157 in memory (size: 46.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,667 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 38.7 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,670 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,671 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.137.209:43475 (size: 16.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,672 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,673 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,673 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,675 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:33,711 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:40157 (size: 16.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,213 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 539 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,215 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.653 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,215 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,216 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,216 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,217 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.665570 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,884 INFO codegen.CodeGenerator: Code generated in 115.980578 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,897 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,898 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,898 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,898 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,899 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,900 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,911 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 75.8 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,913 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.5 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,915 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.137.209:43475 (size: 24.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,924 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,925 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,925 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,929 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:34,959 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:40157 (size: 24.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,094 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 165 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,095 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.192 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,096 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,096 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,096 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,097 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,096 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,405 INFO codegen.CodeGenerator: Code generated in 170.612597 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,431 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,433 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,434 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,434 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,434 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,435 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,444 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 67.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,449 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,451 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.137.209:43475 (size: 19.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,452 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,454 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,455 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,460 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,494 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:40157 (size: 19.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,507 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.137.209:49950\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,695 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 236 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,695 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,704 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.264 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,706 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,707 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,707 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.275065 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:35,850 INFO codegen.CodeGenerator: Code generated in 99.786462 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,061 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,085 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,085 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,085 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,085 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,086 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,089 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,104 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 31.3 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,107 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,118 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.137.209:43475 (size: 14.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,119 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,120 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,120 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,122 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:36,140 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:40157 (size: 14.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,257 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 2135 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,257 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,259 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 2.168 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,260 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,260 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,260 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,260 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,261 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,271 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,273 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,276 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.137.209:43475 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,277 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,277 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,277 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,281 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,297 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:40157 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,306 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.137.209:49950\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,357 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 77 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,358 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,359 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.094 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,359 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,363 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,363 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 2.295025 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,624 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,624 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,624 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,625 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,626 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,627 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,637 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 84.1 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,639 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,641 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.137.209:43475 (size: 27.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,643 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,645 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,645 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,647 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,661 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:40157 (size: 27.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,894 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 247 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,895 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.268 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,895 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,896 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,896 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,895 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,896 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,972 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,973 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,975 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,976 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,976 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,977 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,990 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 168.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,994 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 46.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,996 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.137.209:43475 (size: 46.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,997 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,998 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:38,999 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,001 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,013 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:40157 (size: 46.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,031 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.137.209:49950\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,165 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 164 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,165 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,166 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.188 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,168 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,168 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,172 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.199649 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,339 INFO codegen.CodeGenerator: Code generated in 16.935031 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,382 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,383 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,384 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,384 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,385 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,387 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,397 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 38.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,462 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,470 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.137.209:43475 (size: 16.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,475 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,476 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,478 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,480 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,494 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:40157 in memory (size: 27.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,496 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.137.209:43475 in memory (size: 27.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,553 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:40157 (size: 16.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,585 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.137.209:43475 in memory (size: 19.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,593 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:40157 in memory (size: 19.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,657 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.137.209:43475 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,668 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:40157 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,714 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.137.209:43475 in memory (size: 16.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,718 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:40157 in memory (size: 16.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,787 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 307 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,788 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.398 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,789 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,789 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,790 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,790 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.407815 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,877 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.137.209:43475 in memory (size: 24.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,891 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:40157 in memory (size: 24.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,980 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.137.209:43475 in memory (size: 14.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:39,989 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:40157 in memory (size: 14.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,054 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:40157 in memory (size: 46.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,064 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.137.209:43475 in memory (size: 46.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,519 INFO codegen.CodeGenerator: Code generated in 122.703543 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,538 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,538 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,539 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,539 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,540 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,540 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,546 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 76.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,548 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 24.6 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,550 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.137.209:43475 (size: 24.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,550 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,551 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,551 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,553 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,566 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:40157 (size: 24.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,756 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 203 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,756 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,757 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.214 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,761 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,762 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,762 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,762 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,894 INFO codegen.CodeGenerator: Code generated in 59.1868 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,919 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,921 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,921 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,921 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,921 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,922 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,925 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 67.4 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,929 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 20.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,931 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.137.209:43475 (size: 20.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,931 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,932 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,933 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,935 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,952 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:40157 (size: 20.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:40,957 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.137.209:49950\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,137 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 202 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,138 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,141 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.217 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,142 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,143 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,143 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.223867 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,258 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,259 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,260 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,260 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,260 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,260 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,263 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,272 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 31.3 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,274 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 14.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,276 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.137.209:43475 (size: 14.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,277 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,277 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,278 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,283 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,299 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:40157 (size: 14.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,345 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 62 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,345 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,346 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.082 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,348 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,348 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,348 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,348 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,352 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,356 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,358 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,360 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.137.209:43475 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,363 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,364 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,365 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,367 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,383 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:40157 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,392 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.137.209:49950\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,416 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 50 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,416 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,418 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.065 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,419 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,419 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,419 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.161016 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,744 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,744 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,745 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,745 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,746 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,746 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,752 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 84.1 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,755 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,755 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.137.209:43475 (size: 27.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,756 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,757 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,757 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,759 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,775 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:40157 (size: 27.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,934 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 176 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,934 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,935 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.187 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,935 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,935 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,935 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,935 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,982 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,984 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,986 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,986 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,986 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,986 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,995 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 168.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,998 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 46.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,998 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.137.209:43475 (size: 46.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:41,999 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,000 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,000 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,002 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,021 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:40157 (size: 46.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,062 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.137.209:49950\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,230 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 228 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,230 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,233 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.245 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,233 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,233 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,234 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.251505 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,413 INFO codegen.CodeGenerator: Code generated in 20.413175 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,449 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,450 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,450 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,450 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,450 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,451 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,459 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 38.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,460 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,461 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.137.209:43475 (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,461 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,462 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,462 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,464 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,476 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:40157 (size: 16.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,557 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 94 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,560 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,561 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.110 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,562 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,562 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,563 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.113875 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,886 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.137.209:43475 in memory (size: 14.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,895 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:40157 in memory (size: 14.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,933 INFO codegen.CodeGenerator: Code generated in 115.081014 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,936 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.137.209:43475 in memory (size: 16.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,947 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:40157 in memory (size: 16.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,952 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,953 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,953 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,953 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,954 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,954 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,958 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 74.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,960 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,961 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.137.209:43475 (size: 24.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,961 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,965 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,966 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,967 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,981 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:40157 (size: 24.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,982 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.137.209:43475 in memory (size: 27.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:42,986 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:40157 in memory (size: 27.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,046 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.137.209:43475 in memory (size: 20.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,055 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:40157 in memory (size: 20.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,101 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.137.209:43475 in memory (size: 16.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,126 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:40157 in memory (size: 16.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,183 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:40157 in memory (size: 46.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,184 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.137.209:43475 in memory (size: 46.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,190 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 223 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,190 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,191 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.236 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,191 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,191 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,191 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,192 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,245 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:40157 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,257 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.137.209:43475 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,330 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:40157 in memory (size: 24.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,347 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.137.209:43475 in memory (size: 24.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,429 INFO codegen.CodeGenerator: Code generated in 77.652701 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,451 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,452 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,453 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,453 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,453 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,454 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,456 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 66.5 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,458 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,458 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.137.209:43475 (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,459 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,459 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,460 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,461 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,477 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:40157 (size: 19.4 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,486 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.137.209:49950\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,618 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 157 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,620 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.165 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,622 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,622 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,623 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,623 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.171468 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,829 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,837 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,838 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,839 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,839 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,839 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,844 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,853 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 31.3 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,858 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 14.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,861 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.137.209:43475 (size: 14.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,861 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,862 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,862 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,873 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:43,891 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:40157 (size: 14.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,016 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 143 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,018 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,018 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.172 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,019 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,020 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,020 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,020 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,020 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,022 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,024 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,024 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.137.209:43475 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,026 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,027 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,027 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,028 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,043 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:40157 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,050 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.137.209:49950\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,081 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 53 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,081 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,087 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.061 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,088 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,088 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,088 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.252387 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,464 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,465 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,465 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,465 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,465 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,465 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,469 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 63.6 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,484 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 22.7 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,487 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.137.209:43475 (size: 22.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,488 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,489 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,489 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,491 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,515 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:40157 (size: 22.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,874 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 384 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,874 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,875 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.409 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,878 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,882 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,883 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,883 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,955 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,961 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,962 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,962 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,962 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,963 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,976 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 115.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,983 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,986 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.137.209:43475 (size: 35.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,987 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,987 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,987 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:44,989 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,000 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:40157 (size: 35.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,020 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.137.209:49950\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,187 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 198 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,187 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,188 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.223 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,191 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,191 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,192 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.232344 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,228 INFO codegen.CodeGenerator: Code generated in 33.048403 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,400 INFO codegen.CodeGenerator: Code generated in 34.897108 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,465 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,475 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,475 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,475 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,477 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,479 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,488 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 36.8 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,490 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,491 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.137.209:43475 (size: 16.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,493 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,493 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,494 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,495 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,579 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:40157 (size: 16.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,679 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 184 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,679 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,680 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 0.199 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,681 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,681 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,681 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 0.208156 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,832 INFO codegen.CodeGenerator: Code generated in 48.878317 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,838 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,839 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,839 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,839 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,840 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,840 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,845 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 53.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,848 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 18.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,849 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.137.209:43475 (size: 18.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,850 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,850 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,851 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,852 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,862 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:40157 (size: 18.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,940 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 88 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,940 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,941 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.099 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,942 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,943 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,943 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:45,943 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,126 INFO codegen.CodeGenerator: Code generated in 83.601219 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,164 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,165 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,165 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,165 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,165 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,166 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,172 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 43.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,174 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,175 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.137.209:43475 (size: 14.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,175 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,176 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,177 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,178 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,193 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:40157 (size: 14.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,217 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.137.209:49950\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,282 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 104 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,282 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,283 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.113 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,284 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,285 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,286 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.121806 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,348 INFO codegen.CodeGenerator: Code generated in 46.215158 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,409 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,410 INFO scheduler.DAGScheduler: Registering RDD 150 (countByKey at ColumnProfiler.scala:592) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,411 INFO scheduler.DAGScheduler: Got job 25 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,411 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,411 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,411 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,413 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,418 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 31.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,420 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 14.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,421 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.137.209:43475 (size: 14.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,422 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,423 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,423 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,425 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,443 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:40157 (size: 14.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,518 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 94 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,518 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,519 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (countByKey at ColumnProfiler.scala:592) finished in 0.105 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,521 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,522 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,522 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 37)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,522 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,523 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,524 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,527 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,527 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.137.209:43475 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,528 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,529 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,529 INFO cluster.YarnScheduler: Adding task set 37.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,530 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,542 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:40157 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,546 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.137.209:49950\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,566 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 29) in 36 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,566 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,567 INFO scheduler.DAGScheduler: ResultStage 37 (countByKey at ColumnProfiler.scala:592) finished in 0.044 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,568 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,568 INFO cluster.YarnScheduler: Killing all running tasks in stage 37: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,569 INFO scheduler.DAGScheduler: Job 25 finished: countByKey at ColumnProfiler.scala:592, took 0.160327 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:46,929 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,006 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:40157 in memory (size: 14.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,032 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.137.209:43475 in memory (size: 14.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,037 INFO codegen.CodeGenerator: Code generated in 32.225927 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,057 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:40157 in memory (size: 35.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,060 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.137.209:43475 in memory (size: 35.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,063 INFO scheduler.DAGScheduler: Registering RDD 156 (count at StatsGenerator.scala:66) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,063 INFO scheduler.DAGScheduler: Got map stage job 26 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,064 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 38 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,064 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,065 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,065 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[156] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,081 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:40157 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,086 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 23.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,089 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 10.6 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,086 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.0.137.209:43475 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,090 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.137.209:43475 (size: 10.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,090 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,091 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[156] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,091 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,093 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.137.209:43475 in memory (size: 24.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,095 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:40157 in memory (size: 24.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,097 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,108 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.137.209:43475 in memory (size: 18.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,119 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:40157 in memory (size: 18.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,121 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:40157 (size: 10.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,136 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.137.209:43475 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,149 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:40157 in memory (size: 19.4 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,154 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.137.209:43475 in memory (size: 14.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,161 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:40157 in memory (size: 14.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,169 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.0.137.209:43475 in memory (size: 14.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,180 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:40157 in memory (size: 14.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,184 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.137.209:43475 in memory (size: 22.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,192 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:40157 in memory (size: 22.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,196 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.137.209:43475 in memory (size: 16.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,199 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:40157 in memory (size: 16.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,214 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.137.209:43475 in memory (size: 3.0 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,221 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:40157 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,258 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 30) in 162 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,258 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,259 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (count at StatsGenerator.scala:66) finished in 0.192 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,259 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,259 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,260 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,260 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,290 INFO codegen.CodeGenerator: Code generated in 22.618462 ms\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,316 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,318 INFO scheduler.DAGScheduler: Got job 27 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,318 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,318 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,318 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,319 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[159] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,321 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 11.1 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,323 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,324 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.137.209:43475 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,324 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,324 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[159] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,325 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,326 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,339 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:40157 (size: 5.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,343 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.137.209:49950\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,365 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 39 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,365 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,366 INFO scheduler.DAGScheduler: ResultStage 40 (count at StatsGenerator.scala:66) finished in 0.046 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,368 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,368 INFO cluster.YarnScheduler: Killing all running tasks in stage 40: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,369 INFO scheduler.DAGScheduler: Job 27 finished: count at StatsGenerator.scala:66, took 0.052306 s\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,752 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,771 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,833 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,835 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,846 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,900 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,951 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,954 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,963 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:47,976 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:48,045 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:48,045 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:48,046 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:48,073 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:48,078 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-725dc08f-7dd2-4459-82f2-b6dde3a5ff4d\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:48,087 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-c13f98eb-c920-4745-82f8-c40c5ce3f71f\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:48,198 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2025-10-04 13:18:48,199 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: mhr-dq-sched-20251004-130329\n",
      "ERROR:sagemaker.model_monitor.model_monitoring:Failed to create monitoring schedule.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/sagemaker/model_monitor/model_monitoring.py\", line 2050, in create_monitoring_schedule\n",
      "    self._create_monitoring_schedule_from_job_definition(\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/sagemaker/model_monitor/model_monitoring.py\", line 1594, in _create_monitoring_schedule_from_job_definition\n",
      "    self.sagemaker_session.sagemaker_client.create_monitoring_schedule(\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/botocore/client.py\", line 569, in _api_call\n",
      "    return self._make_api_call(operation_name, kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/botocore/client.py\", line 1023, in _make_api_call\n",
      "    raise error_class(parsed_response, operation_name)\n",
      "botocore.errorfactory.ResourceInUse: An error occurred (ResourceInUse) when calling the CreateMonitoringSchedule operation: Monitoring Schedule arn:aws:sagemaker:us-east-1:533267301342:monitoring-schedule/mhr-dq-sched-20251004-130329 already exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">30</span>                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 </span>)                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span>DQ_SCHEDULE = <span style=\"color: #808000; text-decoration-color: #808000\">f\"mhr-dq-sched-{</span>RUN_ID<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>30 dqm.create_monitoring_schedule(                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>monitor_schedule_name=DQ_SCHEDULE,                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">32 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>endpoint_input=ENDPOINT_NAME,                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>output_s3_uri=dq_output,                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.12/site-packages/sagemaker/model_monitor/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model_monitoring.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2050</span> in      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">create_monitoring_schedule</span>                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2047 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2048 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># create schedule</span>                                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2049 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2050 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._create_monitoring_schedule_from_job_definition(                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2051 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>monitor_schedule_name=monitor_schedule_name,                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2052 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>job_definition_name=new_job_definition_name,                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2053 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>schedule_cron_expression=schedule_cron_expression,                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.12/site-packages/sagemaker/model_monitor/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model_monitoring.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1594</span> in      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_create_monitoring_schedule_from_job_definition</span>                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1591 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># config key MONITORING_SCHEDULE_INTER_CONTAINER_ENCRYPTION_PATH here</span>             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1592 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># because no MonitoringJobDefinition is set for this call</span>                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1593 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1594 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.sagemaker_session.sagemaker_client.create_monitoring_schedule(</span>               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1595 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">MonitoringScheduleName=monitor_schedule_name,</span>                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1596 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">MonitoringScheduleConfig=monitoring_schedule_config,</span>                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1597 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">Tags=all_tags </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold; text-decoration: underline\">or</span><span style=\"font-weight: bold; text-decoration: underline\"> [],</span>                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.12/site-packages/botocore/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">client.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">569</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_api_call</span>                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 566 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>py_operation_name<span style=\"color: #808000; text-decoration-color: #808000\">}() only accepts keyword arguments.\"</span>              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 567 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 568 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The \"self\" in this scope is referring to the BaseClient.</span>                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 569 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">._make_api_call(operation_name, kwargs)</span>                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 570 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 571 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>_api_call.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(py_operation_name)                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 572 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.12/site-packages/botocore/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">client.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1023</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_make_api_call</span>                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1020 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Code\"</span>                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1021 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1022 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>error_class = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.exceptions.from_code(error_code)                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1023 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">raise</span><span style=\"font-weight: bold; text-decoration: underline\"> error_class(parsed_response, operation_name)</span>                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> parsed_response                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1026 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ResourceInUse: </span>An error occurred <span style=\"font-weight: bold\">(</span>ResourceInUse<span style=\"font-weight: bold\">)</span> when calling the CreateMonitoringSchedule operation: Monitoring \n",
       "Schedule arn:aws:sagemaker:us-east-1:533267301342:monitoring-schedule/mhr-dq-sched-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20251004</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">130329</span> already exists\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0m╭─\u001b[0m\u001b[38;2;255;0;0m──────────────────────────────\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0m───────────────────────────────\u001b[0m\u001b[38;2;255;0;0m─╮\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m30\u001b[0m                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m27 \u001b[0m)                                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m28 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m29 \u001b[0mDQ_SCHEDULE = \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mmhr-dq-sched-\u001b[0m\u001b[33m{\u001b[0mRUN_ID\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m                                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m30 dqm.create_monitoring_schedule(                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m31 \u001b[0m\u001b[2m│   \u001b[0mmonitor_schedule_name=DQ_SCHEDULE,                                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m32 \u001b[0m\u001b[2m│   \u001b[0mendpoint_input=ENDPOINT_NAME,                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m33 \u001b[0m\u001b[2m│   \u001b[0moutput_s3_uri=dq_output,                                                                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.12/site-packages/sagemaker/model_monitor/\u001b[0m\u001b[1;33mmodel_monitoring.py\u001b[0m:\u001b[94m2050\u001b[0m in      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[92mcreate_monitoring_schedule\u001b[0m                                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m2047 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m2048 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# create schedule\u001b[0m                                                                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m2049 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m2050 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._create_monitoring_schedule_from_job_definition(                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m2051 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmonitor_schedule_name=monitor_schedule_name,                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m2052 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mjob_definition_name=new_job_definition_name,                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m2053 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mschedule_cron_expression=schedule_cron_expression,                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.12/site-packages/sagemaker/model_monitor/\u001b[0m\u001b[1;33mmodel_monitoring.py\u001b[0m:\u001b[94m1594\u001b[0m in      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[92m_create_monitoring_schedule_from_job_definition\u001b[0m                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1591 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# config key MONITORING_SCHEDULE_INTER_CONTAINER_ENCRYPTION_PATH here\u001b[0m             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1592 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# because no MonitoringJobDefinition is set for this call\u001b[0m                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1593 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m1594 \u001b[2m│   │   \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.sagemaker_session.sagemaker_client.create_monitoring_schedule(\u001b[0m               \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1595 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4mMonitoringScheduleName=monitor_schedule_name,\u001b[0m                                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1596 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4mMonitoringScheduleConfig=monitoring_schedule_config,\u001b[0m                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1597 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4mTags=all_tags \u001b[0m\u001b[1;4;95mor\u001b[0m\u001b[1;4m [],\u001b[0m                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1;33mclient.py\u001b[0m:\u001b[94m569\u001b[0m in \u001b[92m_api_call\u001b[0m                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 566 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mpy_operation_name\u001b[33m}\u001b[0m\u001b[33m() only accepts keyword arguments.\u001b[0m\u001b[33m\"\u001b[0m              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 567 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 568 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m 569 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m._make_api_call(operation_name, kwargs)\u001b[0m                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 570 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 571 \u001b[0m\u001b[2m│   │   \u001b[0m_api_call.\u001b[91m__name__\u001b[0m = \u001b[96mstr\u001b[0m(py_operation_name)                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 572 \u001b[0m                                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1;33mclient.py\u001b[0m:\u001b[94m1023\u001b[0m in \u001b[92m_make_api_call\u001b[0m                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1020 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCode\u001b[0m\u001b[33m\"\u001b[0m                                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1021 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1022 \u001b[0m\u001b[2m│   │   │   \u001b[0merror_class = \u001b[96mself\u001b[0m.exceptions.from_code(error_code)                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m1023 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m error_class(parsed_response, operation_name)\u001b[0m                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1024 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1025 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m parsed_response                                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1026 \u001b[0m                                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mResourceInUse: \u001b[0mAn error occurred \u001b[1m(\u001b[0mResourceInUse\u001b[1m)\u001b[0m when calling the CreateMonitoringSchedule operation: Monitoring \n",
       "Schedule arn:aws:sagemaker:us-east-1:533267301342:monitoring-schedule/mhr-dq-sched-\u001b[1;36m20251004\u001b[0m-\u001b[1;36m130329\u001b[0m already exists\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Baseline from Week-4 train CSV (drop label column 0 to reflect inference inputs)\n",
    "resp = s3c.list_objects_v2(Bucket=bucket, Prefix=f\"{WEEK4_PREFIX}/xgb/\")\n",
    "train_keys = [o[\"Key\"] for o in resp.get(\"Contents\", []) if o[\"Key\"].endswith(\"w4_xgb_train.csv\")]\n",
    "assert train_keys, \"Week-4 XGB train CSV not found.\"\n",
    "xgb_train_key = train_keys[0]\n",
    "\n",
    "raw = pd.read_csv(io.BytesIO(s3c.get_object(Bucket=bucket, Key=xgb_train_key)[\"Body\"].read()), header=None)\n",
    "raw.drop(columns=[0]).to_csv(\"xgb_train_inputs.csv\", index=False, header=False)\n",
    "baseline_inputs_key = f\"{W5_PREFIX}/baselines/xgb_train_inputs.csv\"\n",
    "s3c.upload_file(\"xgb_train_inputs.csv\", bucket, baseline_inputs_key)\n",
    "BASELINE_INPUTS_S3 = f\"s3://{bucket}/{baseline_inputs_key}\"\n",
    "\n",
    "dq_output = f\"s3://{bucket}/{W5_PREFIX}/monitoring/data-quality\"\n",
    "dqm = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,                 # < 1 hour cadence\n",
    "    sagemaker_session=sm_sess,\n",
    ")\n",
    "dqm.suggest_baseline(\n",
    "    baseline_dataset=BASELINE_INPUTS_S3,\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=dq_output,\n",
    "    wait=True,\n",
    ")\n",
    "\n",
    "DQ_SCHEDULE = f\"mhr-dq-sched-{RUN_ID}\"\n",
    "dqm.create_monitoring_schedule(\n",
    "    monitor_schedule_name=DQ_SCHEDULE,\n",
    "    endpoint_input=ENDPOINT_NAME,\n",
    "    output_s3_uri=dq_output,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")\n",
    "print(\"Data Quality schedule:\", DQ_SCHEDULE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9818e3f2-4808-40a8-ac8b-cc31d5078b71",
   "metadata": {},
   "source": [
    "#### Model Quality monitor - schedule hourly on previous full hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98d10283-b224-4577-a185-e81333f8399c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T13:20:57.488125Z",
     "iopub.status.busy": "2025-10-04T13:20:57.487849Z",
     "iopub.status.idle": "2025-10-04T13:20:58.442498Z",
     "shell.execute_reply": "2025-10-04T13:20:58.441760Z",
     "shell.execute_reply.started": "2025-10-04T13:20:57.488103Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: mhr-mq-sched-20251004-130329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Quality schedule: mhr-mq-sched-20251004-130329\n"
     ]
    }
   ],
   "source": [
    "mq_output = f\"s3://{bucket}/{W5_PREFIX}/monitoring/model-quality\"\n",
    "mqm = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,                 # < 1 hour cadence\n",
    "    sagemaker_session=sm_sess,\n",
    ")\n",
    "\n",
    "ENDPOINT_DEST = \"/opt/ml/processing/input/endpoint\"\n",
    "\n",
    "endpoint_input = EndpointInput(\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    destination=ENDPOINT_DEST,\n",
    "    # previous full hour window (works reliably in us-east-1)\n",
    "    start_time_offset=\"-PT2H\",\n",
    "    end_time_offset=\"-PT1H\",\n",
    "    # model output is a single probability in CSV column 0\n",
    "    probability_attribute=\"0\",\n",
    "    probability_threshold_attribute=0.5,\n",
    ")\n",
    "\n",
    "MQ_SCHEDULE = f\"mhr-mq-sched-{RUN_ID}\"\n",
    "mqm.create_monitoring_schedule(\n",
    "    monitor_schedule_name=MQ_SCHEDULE,\n",
    "    endpoint_input=endpoint_input,\n",
    "    ground_truth_input=GROUND_TRUTH_S3,\n",
    "    problem_type=\"BinaryClassification\",\n",
    "    output_s3_uri=mq_output,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")\n",
    "print(\"Model Quality schedule:\", MQ_SCHEDULE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eed816-ac7a-4d86-a194-b3d4bffd1726",
   "metadata": {},
   "source": [
    "#### Infrastructure monitors  CloudWatch alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05c08725-9290-4c6d-85fc-5d9d21f934f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T13:21:03.520648Z",
     "iopub.status.busy": "2025-10-04T13:21:03.520238Z",
     "iopub.status.idle": "2025-10-04T13:21:04.131018Z",
     "shell.execute_reply": "2025-10-04T13:21:04.130243Z",
     "shell.execute_reply.started": "2025-10-04T13:21:03.520615Z"
    }
   },
   "outputs": [],
   "source": [
    "ALARM_PREFIX = f\"MHR-W5-{RUN_ID}\"\n",
    "\n",
    "def put_alarm(metric, stat, comp, thresh, period=60, evals=1, unit=None):\n",
    "    dims=[{\"Name\":\"EndpointName\",\"Value\":ENDPOINT_NAME},\n",
    "          {\"Name\":\"VariantName\",\"Value\":\"AllTraffic\"}]\n",
    "    params=dict(\n",
    "        AlarmName=f\"{ALARM_PREFIX}-{metric}\",\n",
    "        Namespace=\"AWS/SageMaker\",\n",
    "        MetricName=metric,\n",
    "        Dimensions=dims,\n",
    "        Statistic=stat,\n",
    "        Period=period,\n",
    "        EvaluationPeriods=evals,\n",
    "        ComparisonOperator=comp,\n",
    "        Threshold=thresh,\n",
    "        ActionsEnabled=False,                # flip True + SNS if we want emails\n",
    "        TreatMissingData=\"notBreaching\",\n",
    "    )\n",
    "    if unit: params[\"Unit\"]=unit\n",
    "    cw.put_metric_alarm(**params)\n",
    "\n",
    "put_alarm(\"ModelLatency\",    \"Average\", \"GreaterThanThreshold\", 60000)\n",
    "put_alarm(\"OverheadLatency\", \"Average\", \"GreaterThanThreshold\", 60000)\n",
    "put_alarm(\"5XXErrors\",       \"Sum\",     \"GreaterThanThreshold\", 0)\n",
    "put_alarm(\"Invocations\",     \"Sum\",     \"GreaterThanThreshold\", 500)\n",
    "put_alarm(\"CPUUtilization\",  \"Average\", \"GreaterThanThreshold\", 95, unit=\"Percent\")\n",
    "put_alarm(\"MemoryUtilization\",\"Average\",\"GreaterThanThreshold\", 95, unit=\"Percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67671689-4bed-420f-987e-fa85c8a2a2d6",
   "metadata": {},
   "source": [
    "#### CloudWatch dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65fc69d3-c918-4cd9-9372-32c636ba0874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T13:21:09.875915Z",
     "iopub.status.busy": "2025-10-04T13:21:09.875642Z",
     "iopub.status.idle": "2025-10-04T13:21:09.977635Z",
     "shell.execute_reply": "2025-10-04T13:21:09.976802Z",
     "shell.execute_reply.started": "2025-10-04T13:21:09.875895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CloudWatch dashboard: MHR-W5-Dashboard-20251004-130329\n"
     ]
    }
   ],
   "source": [
    "DASHBOARD = f\"MHR-W5-Dashboard-{RUN_ID}\"\n",
    "widgets = [\n",
    "  {\"type\":\"metric\",\"width\":12,\"height\":6,\"properties\":{\n",
    "      \"region\":region,\"title\":\"Latency (ms)\",\n",
    "      \"metrics\":[[\"AWS/SageMaker\",\"ModelLatency\",\"EndpointName\",ENDPOINT_NAME,\"VariantName\",\"AllTraffic\"],\n",
    "                 [\".\",\"OverheadLatency\",\".\",\".\",\".\",\".\"]],\n",
    "      \"period\":60,\"stat\":\"Average\",\"view\":\"timeSeries\"}},\n",
    "  {\"type\":\"metric\",\"width\":12,\"height\":6,\"properties\":{\n",
    "      \"region\":region,\"title\":\"Invocations & 5XX\",\n",
    "      \"metrics\":[[\"AWS/SageMaker\",\"Invocations\",\"EndpointName\",ENDPOINT_NAME,\"VariantName\",\"AllTraffic\"],\n",
    "                 [\".\",\"5XXErrors\",\".\",\".\",\".\",\".\"]],\n",
    "      \"period\":60,\"stat\":\"Sum\",\"view\":\"timeSeries\"}}\n",
    "]\n",
    "cw.put_dashboard(DashboardName=DASHBOARD, DashboardBody=json.dumps({\"widgets\":widgets}))\n",
    "print(\"CloudWatch dashboard:\", DASHBOARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c56d5-282d-4ee2-9865-164a53ebf20f",
   "metadata": {},
   "source": [
    "#### Week-5 tracker (JSON + MD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "459e8ed4-fb2c-45c9-806e-25a7353107b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T13:21:16.829641Z",
     "iopub.status.busy": "2025-10-04T13:21:16.829373Z",
     "iopub.status.idle": "2025-10-04T13:21:16.935911Z",
     "shell.execute_reply": "2025-10-04T13:21:16.935036Z",
     "shell.execute_reply.started": "2025-10-04T13:21:16.829621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week-5 monitoring setup complete.\n"
     ]
    }
   ],
   "source": [
    "tracker = {\n",
    "    \"week\": 5,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"week4_prefix\": f\"s3://{bucket}/{WEEK4_PREFIX}\",\n",
    "    \"week5_prefix\": f\"s3://{bucket}/{W5_PREFIX}\",\n",
    "    \"endpoint\": ENDPOINT_NAME,\n",
    "    \"datacapture_s3\": f\"s3://{bucket}/{data_capture_prefix}\",\n",
    "    \"schedules\": {\"data_quality\": DQ_SCHEDULE, \"model_quality\": MQ_SCHEDULE},\n",
    "    \"outputs\": {\"data_quality\": dq_output, \"model_quality\": mq_output, \"ground_truth\": GROUND_TRUTH_S3},\n",
    "    \"cloudwatch\": {\"alarms_prefix\": ALARM_PREFIX, \"dashboard\": DASHBOARD}\n",
    "}\n",
    "Path(\"w5_tracker\").mkdir(exist_ok=True)\n",
    "json.dump(tracker, open(\"w5_tracker/team_tracker_update_week5.json\",\"w\"), indent=2)\n",
    "open(\"w5_tracker/team_tracker_update_week5.md\",\"w\").write(\n",
    "    f\"# Week 5 Tracker – Maternal Health Risk (RUN: {RUN_ID})\\n\\n\"\n",
    "    f\"**Week-4:** s3://{bucket}/{WEEK4_PREFIX}\\n\"\n",
    "    f\"**Week-5:** s3://{bucket}/{W5_PREFIX}\\n\\n\"\n",
    "    f\"## Endpoint\\n- {ENDPOINT_NAME}\\n- Data Capture: s3://{bucket}/{data_capture_prefix}\\n\\n\"\n",
    "    f\"## Monitoring Schedules\\n- Data Quality: {DQ_SCHEDULE}\\n- Model Quality: {MQ_SCHEDULE}\\n\\n\"\n",
    "    f\"## Outputs\\n- DQ: {dq_output}\\n- MQ: {mq_output}\\n- GT: {GROUND_TRUTH_S3}\\n\\n\"\n",
    "    f\"## CloudWatch\\n- Alarms prefix: {ALARM_PREFIX}\\n- Dashboard: {DASHBOARD}\\n\"\n",
    ")\n",
    "s3c.upload_file(\"w5_tracker/team_tracker_update_week5.json\", bucket, f\"{W5_PREFIX}/team_tracker_update_week5.json\")\n",
    "s3c.upload_file(\"w5_tracker/team_tracker_update_week5.md\",   bucket, f\"{W5_PREFIX}/team_tracker_update_week5.md\")\n",
    "\n",
    "print(\"Week-5 monitoring setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd0818-60e0-4b84-87f8-0366d3dbf1bd",
   "metadata": {},
   "source": [
    "### week5_generate_reports.py <-- We'll run this after at least one schedule execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "606f0377-71b7-4350-81fc-bc473ed8baea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T13:21:27.908092Z",
     "iopub.status.busy": "2025-10-04T13:21:27.907806Z",
     "iopub.status.idle": "2025-10-04T13:21:28.346438Z",
     "shell.execute_reply": "2025-10-04T13:21:28.345773Z",
     "shell.execute_reply.started": "2025-10-04T13:21:27.908060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Report written → s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week5/20251004-130329/monitoring_report.md\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AAI-540 — Week 5 Reporting (student version)\n",
    "- Finds the latest Week-5 run\n",
    "- Lists Data/Model Quality outputs in S3\n",
    "- Writes compact MD + JSON report for Team Update\n",
    "\"\"\"\n",
    "\n",
    "import json, io\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "\n",
    "s3  = boto3.client(\"s3\")\n",
    "bkt = Session().default_bucket()\n",
    "\n",
    "# Find latest Week-5 folder\n",
    "base=\"aai540/maternal-risk/week5/\"\n",
    "resp=s3.list_objects_v2(Bucket=bkt, Prefix=base, Delimiter=\"/\")\n",
    "runs=[cp[\"Prefix\"].rstrip(\"/\") for cp in resp.get(\"CommonPrefixes\",[])]\n",
    "assert runs, f\"No Week-5 runs under s3://{bkt}/{base}\"\n",
    "w5=sorted(runs)[-1]\n",
    "\n",
    "# Load Week-5 tracker\n",
    "tracker=json.loads(s3.get_object(Bucket=bkt, Key=f\"{w5}/team_tracker_update_week5.json\")[\"Body\"].read())\n",
    "dq_out=tracker[\"outputs\"][\"data_quality\"]; mq_out=tracker[\"outputs\"][\"model_quality\"]\n",
    "\n",
    "def list_all(prefix_uri):\n",
    "    \"\"\"List all keys under a given s3://bucket/prefix and return as full s3 URIs.\"\"\"\n",
    "    prefix=prefix_uri.replace(f\"s3://{bkt}/\",\"\")\n",
    "    out=[]; token=None\n",
    "    while True:\n",
    "        kw=dict(Bucket=bkt, Prefix=prefix)\n",
    "        if token: kw[\"ContinuationToken\"]=token\n",
    "        r=s3.list_objects_v2(**kw)\n",
    "        out += [o[\"Key\"] for o in r.get(\"Contents\",[])]\n",
    "        token=r.get(\"NextContinuationToken\")\n",
    "        if not token: break\n",
    "    return [f\"s3://{bkt}/{k}\" for k in out]\n",
    "\n",
    "dq_files=list_all(dq_out)\n",
    "mq_files=list_all(mq_out)\n",
    "\n",
    "report={\n",
    "  \"week\":5,\n",
    "  \"run\":w5,\n",
    "  \"data_quality\":{\n",
    "    \"statistics\": next((f for f in dq_files if f.endswith(\"statistics.json\")), \"N/A\"),\n",
    "    \"constraints\": next((f for f in dq_files if f.endswith(\"constraints.json\")), \"N/A\"),\n",
    "    \"violations\": next((f for f in dq_files if \"constraint_violations.json\" in f), \"N/A\"),\n",
    "  },\n",
    "  \"model_quality\":{\n",
    "    \"files\": [f for f in mq_files if f.endswith((\".json\",\".csv\"))][-10:]\n",
    "  }\n",
    "}\n",
    "\n",
    "Path(\"w5_reports\").mkdir(exist_ok=True)\n",
    "open(\"w5_reports/monitoring_report.md\",\"w\").write(\n",
    "  \"# Week 5 Monitoring Report\\n\\n\"\n",
    "  f\"**Run:** {w5}\\n\\n\"\n",
    "  \"## Data Quality\\n\"\n",
    "  f\"- Statistics: {report['data_quality']['statistics']}\\n\"\n",
    "  f\"- Constraints: {report['data_quality']['constraints']}\\n\"\n",
    "  f\"- Violations: {report['data_quality']['violations']}\\n\\n\"\n",
    "  \"## Model Quality (latest files)\\n\" + \"\\n\".join(f\"- {p}\" for p in report[\"model_quality\"][\"files\"])\n",
    ")\n",
    "open(\"w5_reports/monitoring_report.json\",\"w\").write(json.dumps(report, indent=2))\n",
    "\n",
    "s3.upload_file(\"w5_reports/monitoring_report.md\",   bkt, f\"{w5}/monitoring_report.md\")\n",
    "s3.upload_file(\"w5_reports/monitoring_report.json\", bkt, f\"{w5}/monitoring_report.json\")\n",
    "\n",
    "print(\"📄 Report written →\", f\"s3://{bkt}/{w5}/monitoring_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047650ec-8836-4caa-afd2-85434a053941",
   "metadata": {},
   "source": [
    "### Week 5 cleanup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "60b51900-0587-4bef-a922-5b5daffec9c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T21:15:24.085727Z",
     "iopub.status.busy": "2025-10-04T21:15:24.085438Z",
     "iopub.status.idle": "2025-10-04T21:15:26.248601Z",
     "shell.execute_reply": "2025-10-04T21:15:26.247985Z",
     "shell.execute_reply.started": "2025-10-04T21:15:24.085705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Cleanup complete for run: aai540/maternal-risk/week5/20251004-130329\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AAI-540 — Week 5 Cleanup\n",
    "Deletes: monitoring schedules, endpoint (+config), CW dashboard & alarms.\n",
    "Artifacts remain in S3 for grading.\n",
    "\"\"\"\n",
    "\n",
    "import json, boto3\n",
    "from sagemaker.session import Session\n",
    "\n",
    "s3 = boto3.client(\"s3\"); sm = boto3.client(\"sagemaker\"); cw=boto3.client(\"cloudwatch\")\n",
    "bkt=Session().default_bucket()\n",
    "\n",
    "# latest Week-5 run\n",
    "base=\"aai540/maternal-risk/week5/\"\n",
    "resp=s3.list_objects_v2(Bucket=bkt, Prefix=base, Delimiter=\"/\")\n",
    "w5=sorted([cp[\"Prefix\"].rstrip(\"/\") for cp in resp.get(\"CommonPrefixes\",[])])[-1]\n",
    "\n",
    "trk=json.loads(s3.get_object(Bucket=bkt, Key=f\"{w5}/team_tracker_update_week5.json\")[\"Body\"].read())\n",
    "ep=trk[\"endpoint\"]; dq=trk[\"schedules\"][\"data_quality\"]; mq=trk[\"schedules\"][\"model_quality\"]\n",
    "dash=trk[\"cloudwatch\"][\"dashboard\"]; prefix=trk[\"cloudwatch\"][\"alarms_prefix\"]\n",
    "\n",
    "# stop & delete schedules\n",
    "for name in [dq, mq]:\n",
    "    try: sm.stop_monitoring_schedule(MonitoringScheduleName=name)\n",
    "    except: pass\n",
    "    try: sm.delete_monitoring_schedule(MonitoringScheduleName=name)\n",
    "    except: pass\n",
    "\n",
    "# delete endpoint (+ config)\n",
    "try: sm.delete_endpoint(EndpointName=ep)\n",
    "except: pass\n",
    "try: sm.delete_endpoint_config(EndpointConfigName=ep)\n",
    "except: pass\n",
    "\n",
    "# delete dashboard\n",
    "try: cw.delete_dashboards(DashboardNames=[dash])\n",
    "except: pass\n",
    "\n",
    "# delete alarms with our prefix\n",
    "alarms=cw.describe_alarms(AlarmNamePrefix=prefix).get(\"MetricAlarms\",[])\n",
    "if alarms:\n",
    "    cw.delete_alarms(AlarmNames=[a[\"AlarmName\"] for a in alarms])\n",
    "\n",
    "print(\"🧹 Cleanup complete for run:\", w5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1d5575-bb4a-4c90-8e4f-1202bc504bc8",
   "metadata": {},
   "source": [
    "## Week 6: Implement CI/CD Pipeline to automate training, evaluation, and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc13734-1e9f-43d2-9ea4-f17f00b48271",
   "metadata": {},
   "source": [
    "#### Setup + dataset resolver (auto-detect the latest CSV on S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "fa15bcb5-f1f3-495b-894a-6ee175eff267",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T21:15:46.788136Z",
     "iopub.status.busy": "2025-10-04T21:15:46.787840Z",
     "iopub.status.idle": "2025-10-04T21:15:47.305270Z",
     "shell.execute_reply": "2025-10-04T21:15:47.304623Z",
     "shell.execute_reply.started": "2025-10-04T21:15:46.788109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region: us-east-1\n",
      "role: arn:aws:iam::533267301342:role/LabRole\n",
      "bucket: sagemaker-us-east-1-533267301342\n",
      "base prefix: aai540/maternal-risk/week6/20251004-211547\n",
      "[resolver] Picked latest CSV: s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week5/20251004-130329/baselines/xgb_train_inputs.csv\n",
      "Resolved DatasetS3Uri: s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week5/20251004-130329/baselines/xgb_train_inputs.csv\n"
     ]
    }
   ],
   "source": [
    "# Setup + Dataset Resolver\n",
    "\n",
    "import os, time, json, pathlib, boto3, sagemaker\n",
    "from botocore.exceptions import ClientError\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession  # required for Pipelines\n",
    "\n",
    "# sessions, role, S3 \n",
    "pipeline_session = PipelineSession()\n",
    "region = boto3.session.Session().region_name\n",
    "role   = sagemaker.get_execution_role()\n",
    "bucket = pipeline_session.default_bucket()\n",
    "s3     = boto3.client(\"s3\")\n",
    "\n",
    "RUN_ID = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "PIPELINE_BASE_NAME = \"MaternalHealthRisk-CICD\"   # letters/numbers/hyphens only\n",
    "BASE_PREFIX = f\"aai540/maternal-risk/week6/{RUN_ID}\"\n",
    "\n",
    "print(\"region:\", region)\n",
    "print(\"role:\", role)\n",
    "print(\"bucket:\", bucket)\n",
    "print(\"base prefix:\", BASE_PREFIX)\n",
    "\n",
    "# helper: list all keys under a prefix\n",
    "def list_keys(bucket, prefix, suffix=None, max_keys=1000):\n",
    "    keys = []\n",
    "    token = None\n",
    "    while True:\n",
    "        kwargs = dict(Bucket=bucket, Prefix=prefix, MaxKeys=1000)\n",
    "        if token: kwargs[\"ContinuationToken\"] = token\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp.get(\"Contents\", []):\n",
    "            k = obj[\"Key\"]\n",
    "            if not suffix or k.endswith(suffix):\n",
    "                keys.append(k)\n",
    "        token = resp.get(\"NextContinuationToken\")\n",
    "        if not token or len(keys) >= max_keys:\n",
    "            break\n",
    "    return keys\n",
    "\n",
    "# RESOLVE the dataset automatically\n",
    "def resolve_dataset_s3_uri():\n",
    "    \"\"\"\n",
    "    Strategy:\n",
    "    1) Preferred canonical path (if present):\n",
    "       aai540/datasets/maternal/Maternal_Health_Risk_Data.csv\n",
    "    2) Otherwise, scan Week-3/4/5 prefixes for any CSV that looks like the dataset,\n",
    "       and choose the newest (last modified).\n",
    "    3) If nothing is found, raise a clear error with advice.\n",
    "    \"\"\"\n",
    "    canonical_key = \"aai540/datasets/maternal/Maternal_Health_Risk_Data.csv\"\n",
    "    # canonical\n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket, Key=canonical_key)\n",
    "        return f\"s3://{bucket}/{canonical_key}\"\n",
    "    except ClientError:\n",
    "        pass\n",
    "\n",
    "    # scan likely week folders (to be adjust/add if stored it elsewhere)\n",
    "    candidate_prefixes = [\n",
    "        \"aai540/maternal-risk/week5/\",\n",
    "        \"aai540/maternal-risk/week4/\",\n",
    "        \"aai540/maternal-risk/week3/\",\n",
    "        \"aai540/datasets/maternal/\",\n",
    "        \"aai540/\",\n",
    "    ]\n",
    "    candidates = []\n",
    "    for p in candidate_prefixes:\n",
    "        resp = s3.list_objects_v2(Bucket=bucket, Prefix=p)\n",
    "        for obj in resp.get(\"Contents\", []):\n",
    "            k = obj[\"Key\"]\n",
    "            if k.lower().endswith(\".csv\") and \"maternal\" in k.lower():\n",
    "                candidates.append((k, obj[\"LastModified\"]))\n",
    "    if candidates:\n",
    "        # pick the newest by LastModified\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        chosen_key = candidates[0][0]\n",
    "        print(f\"[resolver] Picked latest CSV: s3://{bucket}/{chosen_key}\")\n",
    "        return f\"s3://{bucket}/{chosen_key}\"\n",
    "\n",
    "    # nothing found --> clear guidance\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find the dataset on S3. Upload the CSV to either:\\n\"\n",
    "        f\"  s3://{bucket}/aai540/datasets/maternal/Maternal_Health_Risk_Data.csv\\n\"\n",
    "        \"or any aai540/week3-5 path (file name should include 'maternal'), then re-run.\"\n",
    "    )\n",
    "\n",
    "DATASET_S3_URI = resolve_dataset_s3_uri()\n",
    "print(\"Resolved DatasetS3Uri:\", DATASET_S3_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c676d6-ddb8-409c-87c0-83b8c831db0d",
   "metadata": {},
   "source": [
    "#### Write the two pipeline scripts used by Processing/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "861ef18f-d72f-4027-856f-1c163ee6785c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T21:16:35.286173Z",
     "iopub.status.busy": "2025-10-04T21:16:35.285886Z",
     "iopub.status.idle": "2025-10-04T21:16:35.292248Z",
     "shell.execute_reply": "2025-10-04T21:16:35.291584Z",
     "shell.execute_reply.started": "2025-10-04T21:16:35.286150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing.py updated.\n"
     ]
    }
   ],
   "source": [
    "# Write helper scripts used inside pipeline steps\n",
    "\n",
    "from pathlib import Path\n",
    "Path(\"week6_code\").mkdir(exist_ok=True)\n",
    "\n",
    "(Path(\"week6_code\")/\"processing.py\").write_text(r\"\"\"\n",
    "import os, glob, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "OUT = \"/opt/ml/processing/output\"\n",
    "LOG = os.path.join(OUT, \"_preprocess_log.json\")\n",
    "\n",
    "def log(msg, **kv):\n",
    "    os.makedirs(OUT, exist_ok=True)\n",
    "    rec = {\"msg\": msg, **kv}\n",
    "    print(\"[preprocess]\", json.dumps(rec))\n",
    "    with open(LOG, \"a\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "def engineer(df: pd.DataFrame):\n",
    "    X = df.copy()\n",
    "\n",
    "    # --- create simple derived features ---\n",
    "    X[\"PulsePressure\"]    = X[\"SystolicBP\"] - X[\"DiastolicBP\"]\n",
    "    X[\"SBPtoDBP\"]         = X[\"SystolicBP\"] / X[\"DiastolicBP\"].replace(0, np.nan)\n",
    "    X[\"Fever\"]            = (X[\"BodyTemp\"] > 99.5).astype(int)\n",
    "    X[\"Tachycardia\"]      = (X[\"HeartRate\"] >= 100).astype(int)\n",
    "    X[\"HypertensionFlag\"] = ((X[\"SystolicBP\"] >= 140) | (X[\"DiastolicBP\"] >= 90)).astype(int)\n",
    "\n",
    "    # --- scale continuous cols (store as z*) ---\n",
    "    cont = [\"Age\",\"SystolicBP\",\"DiastolicBP\",\"BS\",\"BodyTemp\",\"HeartRate\",\"PulsePressure\",\"SBPtoDBP\"]\n",
    "    X[[\"z\"+c for c in cont]] = StandardScaler().fit_transform(X[cont])\n",
    "\n",
    "    # --- binarize label: low -> 0, (mid|high) -> 1 ---\n",
    "    label_map = {\"low risk\":0, \"mid risk\":1, \"high risk\":1}\n",
    "    if \"RiskLevel\" not in X.columns:\n",
    "        raise ValueError(\"Column 'RiskLevel' not found. Available cols: %s\" % list(X.columns))\n",
    "    y = X[\"RiskLevel\"].str.lower().map(label_map)\n",
    "\n",
    "    # guard: drop rows that still ended up NaN (unexpected labels)\n",
    "    bad = y.isna().sum()\n",
    "    if bad > 0:\n",
    "        log(\"dropping rows with unknown RiskLevel\", bad_rows=int(bad))\n",
    "        keep = ~y.isna()\n",
    "        X, y = X.loc[keep].copy(), y.loc[keep].copy()\n",
    "\n",
    "    X = X.drop(columns=[\"RiskLevel\"])\n",
    "    return X, y.astype(int)\n",
    "\n",
    "def to_xgb(df):\n",
    "    # xgboost algorithm container expects: label first column, no header\n",
    "    cols = [\"label\"] + [c for c in df.columns if c != \"label\"]\n",
    "    return df[cols]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        in_dir  = \"/opt/ml/processing/input\"\n",
    "        out_dir = OUT\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        csvs = sorted(glob.glob(os.path.join(in_dir, \"*.csv\")))\n",
    "        assert csvs, f\"No CSV in {in_dir}\"\n",
    "        log(\"found_input_csv\", files=csvs)\n",
    "\n",
    "        df = pd.read_csv(csvs[0])\n",
    "        needed = [\"Age\",\"SystolicBP\",\"DiastolicBP\",\"BS\",\"BodyTemp\",\"HeartRate\",\"RiskLevel\"]\n",
    "        missing = [c for c in needed if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "        log(\"df_shape_before\", rows=int(df.shape[0]), cols=int(df.shape[1]))\n",
    "        X, y = engineer(df)\n",
    "        log(\"label_counts\", **{str(k): int(v) for k, v in pd.Series(y).value_counts().to_dict().items()})\n",
    "\n",
    "        # --- split: 40% production holdout; remaining 60% -> 40/10/10 ---\n",
    "        X_tmp, X_prod, y_tmp, y_prod = train_test_split(X, y, test_size=0.40, random_state=42, stratify=y)\n",
    "        X_train, X_rem, y_train, y_rem = train_test_split(X_tmp, y_tmp, test_size=(1/3), random_state=42, stratify=y_tmp)\n",
    "        X_val, X_test, y_val, y_test   = train_test_split(X_rem, y_rem, test_size=0.5,  random_state=42, stratify=y_rem)\n",
    "\n",
    "        def dump(name, Xd, yd, header=True):\n",
    "            out = Xd.copy(); out[\"label\"] = yd.values\n",
    "            out.to_csv(os.path.join(out_dir, f\"{name}.csv\"), index=False, header=header)\n",
    "            log(\"wrote_split\", name=name, rows=int(out.shape[0]), cols=int(out.shape[1]))\n",
    "\n",
    "        # human-readable CSVs (header)\n",
    "        dump(\"train\", X_train, y_train, header=True)\n",
    "        dump(\"val\",   X_val,   y_val,   header=True)\n",
    "        dump(\"test\",  X_test,  y_test,  header=True)\n",
    "        dump(\"production\", X_prod, y_prod, header=True)\n",
    "\n",
    "        # xgboost files (no header, label first)\n",
    "        t = pd.read_csv(os.path.join(out_dir, \"train.csv\"))\n",
    "        v = pd.read_csv(os.path.join(out_dir, \"val.csv\"))\n",
    "        to_xgb(t).to_csv(os.path.join(out_dir, \"xgb_train.csv\"), index=False, header=False)\n",
    "        to_xgb(v).to_csv(os.path.join(out_dir, \"xgb_val.csv\"),   index=False, header=False)\n",
    "        log(\"wrote_xgb_files\", files=[\"xgb_train.csv\", \"xgb_val.csv\"])\n",
    "\n",
    "        # sanity: ensure xgb files exist\n",
    "        assert os.path.exists(os.path.join(out_dir, \"xgb_train.csv\"))\n",
    "        assert os.path.exists(os.path.join(out_dir, \"xgb_val.csv\"))\n",
    "        log(\"success\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # write message so you can read it directly from S3 if the step fails\n",
    "        log(\"fatal_error\", error=str(e))\n",
    "        raise\n",
    "\"\"\")\n",
    "print(\"processing.py updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41595c-198e-48a1-bae7-c073410c466e",
   "metadata": {},
   "source": [
    "#### Build + run the pipeline (Preprocess --> Train --> Evaluate --> Gate --> Register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "16288bd4-e906-4896-abb0-71c656b9a213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T21:16:36.687680Z",
     "iopub.status.busy": "2025-10-04T21:16:36.687265Z",
     "iopub.status.idle": "2025-10-04T21:16:36.693100Z",
     "shell.execute_reply": "2025-10-04T21:16:36.692359Z",
     "shell.execute_reply.started": "2025-10-04T21:16:36.687656Z"
    }
   },
   "outputs": [],
   "source": [
    "# CI/CD: build pipeline\n",
    "\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat, ParameterInteger\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CacheConfig\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.functions import Join, JsonGet\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "# exposed parameters (easy to tweak)\n",
    "p_base_prefix       = ParameterString(\"BaseS3Prefix\", default_value=BASE_PREFIX)\n",
    "p_dataset_s3_uri    = ParameterString(\"DatasetS3Uri\", default_value=DATASET_S3_URI)  # <- auto-resolved\n",
    "p_model_pkg_group   = ParameterString(\"ModelPackageGroupName\", default_value=\"MaternalHealthRisk\")\n",
    "p_train_instance    = ParameterString(\"TrainInstanceType\", default_value=\"ml.m5.large\")\n",
    "p_train_count       = ParameterInteger(\"TrainInstanceCount\", default_value=1)\n",
    "p_auc_threshold     = ParameterFloat(\"AUCThreshold\", default_value=0.90)\n",
    "p_max_depth         = ParameterInteger(\"MaxDepth\", default_value=5)\n",
    "p_eta               = ParameterFloat(\"Eta\", default_value=0.2)\n",
    "p_rounds            = ParameterInteger(\"NumRounds\", default_value=200)\n",
    "\n",
    "cache = CacheConfig(enable_caching=True, expire_after=\"30d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6a980-e930-4ee0-8389-d45037c4e77f",
   "metadata": {},
   "source": [
    "#### Preprocess & split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "429d28d8-95d0-4323-a756-a67e5873ba39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T21:16:39.010319Z",
     "iopub.status.busy": "2025-10-04T21:16:39.010024Z",
     "iopub.status.idle": "2025-10-04T21:16:39.040429Z",
     "shell.execute_reply": "2025-10-04T21:16:39.039629Z",
     "shell.execute_reply.started": "2025-10-04T21:16:39.010296Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "# Preprocess \n",
    "preproc = SKLearnProcessor(\n",
    "    framework_version=\"1.2-1\",\n",
    "    role=role, instance_type=\"ml.m5.large\", instance_count=1,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "step_preprocess = ProcessingStep(\n",
    "    name=\"PreprocessAndSplit\",\n",
    "    processor=preproc,\n",
    "    code=str(CODE_DIR / \"processing.py\"),\n",
    "    inputs=[ProcessingInput(source=p_dataset_s3_uri, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[ProcessingOutput(\n",
    "        output_name=\"splits\",\n",
    "        source=\"/opt/ml/processing/output\",\n",
    "        destination=Join(on=\"/\", values=[\"s3:/\", bucket, p_base_prefix, \"data\"])\n",
    "    )],\n",
    "    cache_config=cache\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0150abb-abff-4c5e-8559-8142d774ba71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T13:33:59.351730Z",
     "iopub.status.busy": "2025-10-04T13:33:59.351428Z",
     "iopub.status.idle": "2025-10-04T13:33:59.382632Z",
     "shell.execute_reply": "2025-10-04T13:33:59.381967Z",
     "shell.execute_reply.started": "2025-10-04T13:33:59.351707Z"
    }
   },
   "source": [
    "#### Train (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ceecf8cd-2d6e-4ccd-9b4b-2d8cf97d3dae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T21:16:40.433579Z",
     "iopub.status.busy": "2025-10-04T21:16:40.433298Z",
     "iopub.status.idle": "2025-10-04T21:16:40.472420Z",
     "shell.execute_reply": "2025-10-04T21:16:40.471755Z",
     "shell.execute_reply.started": "2025-10-04T21:16:40.433557Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "/opt/conda/lib/python3.12/site-packages/sagemaker/workflow/steps.py:485: UserWarning: Profiling is enabled on the provided estimator. The default profiler rule includes a timestamp which will change each time the pipeline is upserted, causing cache misses. If profiling is not needed, set disable_profiler to True on the estimator.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "xgb_image = image_uris.retrieve(\"xgboost\", region=region, version=\"1.7-1\")  # OK to fallback to 1.5-1\n",
    "xgb = Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=p_train_count, instance_type=p_train_instance,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    hyperparameters={\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"max_depth\": p_max_depth,\n",
    "        \"eta\": p_eta,\n",
    "        \"num_round\": p_rounds,\n",
    "        \"subsample\": 0.8, \"colsample_bytree\": 0.8, \"verbosity\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "train_s3 = Join(on=\"/\", values=[\"s3:/\", bucket, p_base_prefix, \"data\", \"xgb_train.csv\"])\n",
    "val_s3   = Join(on=\"/\", values=[\"s3:/\", bucket, p_base_prefix, \"data\", \"xgb_val.csv\"])\n",
    "\n",
    "# IMPORTANT: pass content_type by name (older SDKs interpret positional arg as distribution!)\n",
    "step_train = TrainingStep(\n",
    "    name=\"TrainXGBoost\",\n",
    "    estimator=xgb,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(s3_data=train_s3, content_type=\"text/csv\"),\n",
    "        \"validation\": TrainingInput(s3_data=val_s3, content_type=\"text/csv\"),\n",
    "    },\n",
    "    cache_config=cache\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718f2a1-20dc-43bf-92f8-b10b2d8b4068",
   "metadata": {},
   "source": [
    "#### Evaluate on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "75284ea7-861c-4fc0-835e-61c19436da7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T21:16:41.755140Z",
     "iopub.status.busy": "2025-10-04T21:16:41.754864Z",
     "iopub.status.idle": "2025-10-04T21:16:41.783497Z",
     "shell.execute_reply": "2025-10-04T21:16:41.782651Z",
     "shell.execute_reply.started": "2025-10-04T21:16:41.755119Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n"
     ]
    }
   ],
   "source": [
    "eval_proc = ScriptProcessor(\n",
    "    image_uri=image_uris.retrieve(\"sklearn\", region=region, version=\"1.2-1\"),\n",
    "    command=[\"python3\"],\n",
    "    role=role, instance_type=\"ml.m5.large\", instance_count=1,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "eval_report = PropertyFile(name=\"EvalReport\", output_name=\"evaluation\", path=\"evaluation.json\")\n",
    "\n",
    "step_evaluate = ProcessingStep(\n",
    "    name=\"EvaluateOnTest\",\n",
    "    processor=eval_proc,\n",
    "    code=str(CODE_DIR / \"evaluate.py\"),\n",
    "    inputs=[\n",
    "        ProcessingInput(source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                        destination=\"/opt/ml/processing/model\"),\n",
    "        ProcessingInput(source=Join(on=\"/\", values=[\"s3:/\", bucket, p_base_prefix, \"data\", \"test.csv\"]),\n",
    "                        destination=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(\n",
    "        output_name=\"evaluation\",\n",
    "        source=\"/opt/ml/processing/evaluation\",\n",
    "        destination=Join(on=\"/\", values=[\"s3:/\", bucket, p_base_prefix, \"evaluation\"])\n",
    "    )],\n",
    "    property_files=[eval_report],\n",
    "    cache_config=cache\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e159d-ecd8-43bc-a56c-45a85fe2d15c",
   "metadata": {},
   "source": [
    "#### Quality gate (AUC >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0629357b-5998-482b-8b62-9ab27e7865c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T21:16:43.005074Z",
     "iopub.status.busy": "2025-10-04T21:16:43.004794Z",
     "iopub.status.idle": "2025-10-04T21:16:43.010007Z",
     "shell.execute_reply": "2025-10-04T21:16:43.008931Z",
     "shell.execute_reply.started": "2025-10-04T21:16:43.005052Z"
    }
   },
   "outputs": [],
   "source": [
    "cond_auc_ok = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(step_name=step_evaluate.name,\n",
    "                 property_file=eval_report,\n",
    "                 json_path=\"binary_classification_metrics.auc\"),\n",
    "    right=p_auc_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8db667-a1bc-4776-8834-694909fd97e4",
   "metadata": {},
   "source": [
    "#### Register model in Model Registry (deployment target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5e8d5d6a-a318-4682-a55b-d405bcebafab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T21:16:43.906152Z",
     "iopub.status.busy": "2025-10-04T21:16:43.905883Z",
     "iopub.status.idle": "2025-10-04T21:16:44.602980Z",
     "shell.execute_reply": "2025-10-04T21:16:44.602197Z",
     "shell.execute_reply.started": "2025-10-04T21:16:43.906131Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline JSON length: 7484\n",
      "{\"Version\": \"2020-12-01\", \"Metadata\": {}, \"Parameters\": [{\"Name\": \"BaseS3Prefix\", \"Type\": \"String\", \"DefaultValue\": \"aai540/maternal-risk/week6/20251004-211547\"}, {\"Name\": \"DatasetS3Uri\", \"Type\": \"String\", \"DefaultValue\": \"s3://sagemaker-us-east-1-533267301342/aai540/maternal-risk/week5/20251004-130329/baselines/xgb_train_inputs.csv\"}, {\"Name\": \"ModelPackageGroupName\", \"Type\": \"String\", \"DefaultValue\": \"MaternalHealthRisk\"}, {\"Name\": \"TrainInstanceType\", \"Type\": \"String\", \"DefaultValue\": \"ml.m5.large\"}, {\"Name\": \"TrainInstanceCount\", \"Type\": \"Integer\", \"DefaultValue\": 1}, {\"Name\": \"AUCThreshold\", \"Type\": \"Float\", \"DefaultValue\": 0.9}, {\"Name\": \"MaxDepth\", \"Type\": \"Integer\", \"DefaultValue\": 5}, {\"Name\": \"Eta\", \"Type\": \"Float\", \"DefaultValue\": 0.2}, {\"Name\": \"NumRounds\", \"Type\": \"Integer\", \"DefaultValue\": 200}], \"PipelineExperimentConfig\": {\"ExperimentName\": {\"Get\": \"Execution.PipelineName\"}, \"TrialName\": {\"Get\": \"Execution.PipelineExecutionId\"}}, \"Steps\": [{\"Name\": \"PreprocessAndSplit\",\n",
      "Started pipeline: MaternalHealthRisk-CICD-20251004-211547\n",
      "Execution ARN: arn:aws:sagemaker:us-east-1:533267301342:pipeline/MaternalHealthRisk-CICD-20251004-211547/execution/n7hmqyjwr8d0\n"
     ]
    }
   ],
   "source": [
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=Join(on=\"/\", values=[\"s3:/\", bucket, p_base_prefix, \"evaluation\", \"evaluation.json\"]),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "register = RegisterModel(\n",
    "    name=\"RegisterPassedModel\",\n",
    "    estimator=xgb,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"], response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.m5.large\"], transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=p_model_pkg_group,\n",
    "    model_metrics=model_metrics,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "quality_gate = ConditionStep(\n",
    "    name=\"QualityGate\",\n",
    "    conditions=[cond_auc_ok],\n",
    "    if_steps=[register],\n",
    "    else_steps=[]\n",
    ")\n",
    "\n",
    "# assemble & launch\n",
    "pipeline_name = f\"{PIPELINE_BASE_NAME}-{RUN_ID}\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[p_base_prefix, p_dataset_s3_uri, p_model_pkg_group, p_train_instance,\n",
    "                p_train_count, p_auc_threshold, p_max_depth, p_eta, p_rounds],\n",
    "    steps=[step_preprocess, step_train, step_evaluate, quality_gate],\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "pipe_json = pipeline.definition()\n",
    "print(\"Pipeline JSON length:\", len(pipe_json))\n",
    "print(pipe_json[:1000])\n",
    "\n",
    "role_arn_str = role if isinstance(role, str) else str(role)\n",
    "pipeline.upsert(role_arn=role_arn_str)\n",
    "\n",
    "execution = pipeline.start(parameters=dict(\n",
    "    BaseS3Prefix=BASE_PREFIX,\n",
    "    DatasetS3Uri=DATASET_S3_URI,                     \n",
    "    ModelPackageGroupName=\"MaternalHealthRisk\",\n",
    "    TrainInstanceType=\"ml.m5.large\",\n",
    "    TrainInstanceCount=1,\n",
    "    AUCThreshold=0.90,\n",
    "    MaxDepth=5, Eta=0.2, NumRounds=200\n",
    "))\n",
    "print(\"Started pipeline:\", pipeline_name)\n",
    "print(\"Execution ARN:\", execution.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821c11b-c732-4223-bbf8-b3aa623f8a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

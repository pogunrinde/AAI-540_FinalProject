{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fbb218c-3c95-479d-87eb-b38ac4101ba9",
   "metadata": {},
   "source": [
    "# Group 1 Final Project Work "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b79343-85ed-4a30-b48f-4475c77a98e1",
   "metadata": {},
   "source": [
    "#### Specific data on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3387b813-213e-4b8d-b13c-01d8e44ecc7e",
   "metadata": {},
   "source": [
    "### Maternal Health Risk Dataset Summary\n",
    "\n",
    "**Shape:** 808 records × 7 columns  \n",
    "\n",
    "**Columns:**\n",
    "- `Age`\n",
    "- `SystolicBP` (Systolic Blood Pressure)\n",
    "- `DiastolicBP` (Diastolic Blood Pressure)\n",
    "- `BS` (Blood Sugar level)\n",
    "- `BodyTemp` (Body Temperature, °F)\n",
    "- `HeartRate` (Heart Rate, bpm)\n",
    "- `RiskLevel` (Target: maternal health risk category)\n",
    "\n",
    "---\n",
    "\n",
    "#### First 5 Records\n",
    "| Age | SystolicBP | DiastolicBP | BS   | BodyTemp | HeartRate | RiskLevel  |\n",
    "|-----|------------|--------------|------|----------|-----------|------------|\n",
    "| 25  | 130        | 80           | 15.0 | 98.0     | 86        | high risk  |\n",
    "| 35  | 140        | 90           | 13.0 | 98.0     | 70        | high risk  |\n",
    "| 29  | 90         | 70           | 8.0  | 100.0    | 80        | high risk  |\n",
    "| 30  | 140        | 85           | 7.0  | 98.0     | 70        | high risk  |\n",
    "| 35  | 120        | 60           | 6.1  | 98.0     | 76        | low risk   |\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary Statistics\n",
    "- **Age:** 10–70 years (mean = 30.6, std = 13.9)  \n",
    "- **SystolicBP:** 70–160 mmHg (mean = 113, std = 19.9)  \n",
    "- **DiastolicBP:** 49–100 mmHg (mean = 77.5, std = 14.8)  \n",
    "- **BS:** 6–19 mmol/L (mean = 9.26, std = 3.62)  \n",
    "- **BodyTemp:** 98–103 °F (mean = 98.6, std = 1.39)  \n",
    "- **HeartRate:** 7–90 bpm (mean = 74.3, std = 8.82)  \n",
    "\n",
    "---\n",
    "\n",
    "#### Target Variable: RiskLevel\n",
    "- **Low risk:** 478 records (~59.2%)  \n",
    "- **High risk:** 330 records (~40.8%)  \n",
    "- **Medium risk:** Not present in this dataset version  \n",
    "\n",
    " Note: The dataset is binary-labeled (low vs. high risk), so if a 3-class model (low/mid/high) is needed, additional data preprocessing or augmentation may be required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ac998-df87-4fa8-a23c-c32dd6250ae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T00:29:58.488110Z",
     "iopub.status.busy": "2025-09-20T00:29:58.487852Z",
     "iopub.status.idle": "2025-09-20T00:29:58.491576Z",
     "shell.execute_reply": "2025-09-20T00:29:58.490728Z",
     "shell.execute_reply.started": "2025-09-20T00:29:58.488088Z"
    }
   },
   "source": [
    "### Week 3 - Training and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d938a2-5f7a-4a04-b9a7-c4dd14f5aa77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T00:33:39.970066Z",
     "iopub.status.busy": "2025-09-20T00:33:39.969690Z",
     "iopub.status.idle": "2025-09-20T00:33:39.973857Z",
     "shell.execute_reply": "2025-09-20T00:33:39.972733Z",
     "shell.execute_reply.started": "2025-09-20T00:33:39.970039Z"
    }
   },
   "source": [
    "#### Environment (auto role + auto bucket) and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204e533a-2bf3-413f-b03d-2bd34a912eea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:17:06.524380Z",
     "iopub.status.busy": "2025-10-07T22:17:06.524117Z",
     "iopub.status.idle": "2025-10-07T22:17:11.490951Z",
     "shell.execute_reply": "2025-10-07T22:17:11.490354Z",
     "shell.execute_reply.started": "2025-10-07T22:17:06.524356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Region: us-east-1\n",
      "Role:   arn:aws:iam::849121223812:role/LabRole\n",
      "S3:     s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711\n"
     ]
    }
   ],
   "source": [
    "# NO MANUAL SETTINGS: bucket/role are auto-detected from your Studio kernel.\n",
    "\n",
    "import os, io, json, time\n",
    "from pathlib import Path\n",
    "import boto3, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "# AWS context (auto from Studio kernel)\n",
    "boto_sess  = boto3.session.Session()\n",
    "region     = boto_sess.region_name\n",
    "sm_session = Session(boto_sess)\n",
    "role       = get_execution_role()\n",
    "bucket     = sm_session.default_bucket()\n",
    "\n",
    "RUN_ID    = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "S3_PREFIX = f\"aai540/maternal-risk/week3/{RUN_ID}\"\n",
    "\n",
    "# Paths\n",
    "DATA_CSV      = Path(\"Maternal_Risk.csv\")    # Using our Kaggle CSV here\n",
    "ARTIFACTS_DIR = Path(\"week3_outputs\"); ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:  \", role)\n",
    "print(\"S3:    \", f\"s3://{bucket}/{S3_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d4adc-f878-43ba-8088-6b1ae27302c2",
   "metadata": {},
   "source": [
    "#### Load data + lightweight EDA (plots + JSON summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8831eee7-cacf-4d1d-9ab8-722e439b677d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:17:15.931122Z",
     "iopub.status.busy": "2025-10-07T22:17:15.930595Z",
     "iopub.status.idle": "2025-10-07T22:17:16.642029Z",
     "shell.execute_reply": "2025-10-07T22:17:16.641366Z",
     "shell.execute_reply.started": "2025-10-07T22:17:15.931091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA done --> week3_outputs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 768x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert DATA_CSV.exists(), f\"Missing dataset at {DATA_CSV.resolve()}\"\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "\n",
    "# Keep a small EDA summary to feed the team tracker\n",
    "eda_summary = {\n",
    "    \"rows\": int(df.shape[0]),\n",
    "    \"cols\": int(df.shape[1]),\n",
    "    \"columns\": df.columns.tolist(),\n",
    "    \"dtypes\": {c: str(t) for c, t in df.dtypes.items()},\n",
    "    \"missing_counts\": df.isna().sum().to_dict(),\n",
    "    \"class_counts\": df[\"RiskLevel\"].value_counts().to_dict(),\n",
    "}\n",
    "json.dump(eda_summary, open(ARTIFACTS_DIR/\"eda_summary.json\",\"w\"), indent=2)\n",
    "\n",
    "# A few simple plots for the design doc\n",
    "(df[\"RiskLevel\"].value_counts()\n",
    "   .plot(kind=\"bar\", title=\"Class Distribution\")\n",
    "   .get_figure().savefig(ARTIFACTS_DIR/\"chart_class_distribution.png\")); plt.clf()\n",
    "\n",
    "df[\"Age\"].plot(kind=\"hist\", bins=20, title=\"Age Distribution\").get_figure().savefig(\n",
    "    ARTIFACTS_DIR/\"chart_age_hist.png\"); plt.clf()\n",
    "\n",
    "plt.boxplot([df[\"SystolicBP\"], df[\"DiastolicBP\"]], tick_labels=[\"SystolicBP\",\"DiastolicBP\"])\n",
    "plt.title(\"Blood Pressure Boxplots\"); plt.tight_layout()\n",
    "plt.savefig(ARTIFACTS_DIR/\"chart_bp_box.png\"); plt.clf()\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "corr = df[num_cols].corr()\n",
    "plt.imshow(corr, interpolation=\"nearest\"); plt.colorbar()\n",
    "plt.xticks(range(len(num_cols)), num_cols, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(len(num_cols)), num_cols)\n",
    "plt.title(\"Correlation Heatmap\"); plt.tight_layout()\n",
    "plt.savefig(ARTIFACTS_DIR/\"chart_corr_heatmap.png\"); plt.clf()\n",
    "\n",
    "print(\"EDA done -->\", ARTIFACTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d720b-3cbf-4645-81e5-cda60c4bf8c6",
   "metadata": {},
   "source": [
    "#### Feature engineering (clinically-motivated features + z-scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b953f3-132d-4faf-9042-56c570bb7df2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:17:20.099794Z",
     "iopub.status.busy": "2025-10-07T22:17:20.099498Z",
     "iopub.status.idle": "2025-10-07T22:17:20.160136Z",
     "shell.execute_reply": "2025-10-07T22:17:20.159267Z",
     "shell.execute_reply.started": "2025-10-07T22:17:20.099772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering done.\n"
     ]
    }
   ],
   "source": [
    "# We derive simple vitals-based features and also add z-scaled versions for linear models.\n",
    "X = df.copy()\n",
    "# simple clinically meaningful features\n",
    "X[\"PulsePressure\"]    = X[\"SystolicBP\"] - X[\"DiastolicBP\"]\n",
    "X[\"SBP_to_DBP\"]       = X[\"SystolicBP\"] / (X[\"DiastolicBP\"].replace(0, np.nan))\n",
    "X[\"Fever\"]            = (X[\"BodyTemp\"] > 99.5).astype(int)\n",
    "X[\"Tachycardia\"]      = (X[\"HeartRate\"] >= 100).astype(int)\n",
    "X[\"HypertensionFlag\"] = ((X[\"SystolicBP\"] >= 140) | (X[\"DiastolicBP\"] >= 90)).astype(int)\n",
    "\n",
    "# z-scaling for linear models\n",
    "cont = [\"Age\",\"SystolicBP\",\"DiastolicBP\",\"BS\",\"BodyTemp\",\"HeartRate\",\"PulsePressure\"]\n",
    "X[[f\"z_{c}\" for c in cont]] = StandardScaler().fit_transform(X[cont])\n",
    "\n",
    "# binary labels (in this dataset: \"low risk\" / \"high risk\")\n",
    "label_map = {\"low risk\": 0, \"high risk\": 1}\n",
    "y = X[\"RiskLevel\"].map(label_map)\n",
    "engineered = pd.concat([X.drop(columns=[\"RiskLevel\"]), y.rename(\"label\")], axis=1)\n",
    "engineered.to_csv(ARTIFACTS_DIR/\"maternal_features_full.csv\", index=False)\n",
    "json.dump(label_map, open(ARTIFACTS_DIR/\"label_map.json\",\"w\"), indent=2)\n",
    "\n",
    "# Stratified splits (train/val/test/prod = 40/10/10/40)\n",
    "X_no_target = engineered.drop(columns=[\"label\"])\n",
    "y_only      = engineered[\"label\"]\n",
    "\n",
    "X_tmp, X_prod, y_tmp, y_prod = train_test_split(\n",
    "    X_no_target, y_only, test_size=0.40, random_state=42, stratify=y_only\n",
    ")\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=(1/3), random_state=42, stratify=y_tmp\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_rem, y_rem, test_size=0.5, random_state=42, stratify=y_rem\n",
    ")\n",
    "\n",
    "def save_split(name, Xd, yd):\n",
    "    out = Xd.copy(); out[\"label\"] = yd.values\n",
    "    out.to_csv(ARTIFACTS_DIR/f\"{name}.csv\", index=False)\n",
    "    return out\n",
    "\n",
    "train_df = save_split(\"train\", X_train, y_train)\n",
    "val_df   = save_split(\"val\",   X_val,   y_val)\n",
    "test_df  = save_split(\"test\",  X_test,  y_test)\n",
    "prod_df  = save_split(\"production\", X_prod, y_prod)\n",
    "\n",
    "print(\"Feature engineering done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d0b66-2582-4e76-9aba-50b1c9193047",
   "metadata": {},
   "source": [
    "#### Stratified splits: 40% prod, 40% train, 10% val, 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882288ee-155b-42ff-a6a4-9583a26635cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:17:23.127834Z",
     "iopub.status.busy": "2025-10-07T22:17:23.127532Z",
     "iopub.status.idle": "2025-10-07T22:17:23.159933Z",
     "shell.execute_reply": "2025-10-07T22:17:23.159042Z",
     "shell.execute_reply.started": "2025-10-07T22:17:23.127810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 322, 'val': 81, 'test': 81, 'production': 324}\n"
     ]
    }
   ],
   "source": [
    "# We first carve out 40% as \"production\" holdout for future batch inference/monitoring.\n",
    "# The remaining 60% --> train (40%), val (10%), test (10%) of the original dataset.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 40% set aside for future batch inference/monitoring\n",
    "X_tmp, X_prod, y_tmp, y_prod = train_test_split(\n",
    "    X_no_target, y, test_size=0.40, random_state=42, stratify=y\n",
    ")\n",
    "# remaining 60% -> 40/10/10\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=(1/3), random_state=42, stratify=y_tmp\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_rem, y_rem, test_size=0.5, random_state=42, stratify=y_rem\n",
    ")\n",
    "\n",
    "def _save(name, Xd, yd):\n",
    "    out = Xd.copy(); out[\"label\"] = yd.values\n",
    "    out.to_csv(ARTIFACTS_DIR / f\"{name}.csv\", index=False)\n",
    "    return out\n",
    "\n",
    "train_df = _save(\"train\",      X_train, y_train)\n",
    "val_df   = _save(\"val\",        X_val,   y_val)\n",
    "test_df  = _save(\"test\",       X_test,  y_test)\n",
    "prod_df  = _save(\"production\", X_prod,  y_prod)\n",
    "\n",
    "print({\"train\":len(train_df), \"val\":len(val_df), \"test\":len(test_df), \"production\":len(prod_df)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818f682-6948-49c9-89ab-29fda6b786fa",
   "metadata": {},
   "source": [
    "#### Upload artifacts to S3 (no manual bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9c2544-d4f1-4f72-948b-2b27a0defd9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:17:24.360070Z",
     "iopub.status.busy": "2025-10-07T22:17:24.359739Z",
     "iopub.status.idle": "2025-10-07T22:17:24.919516Z",
     "shell.execute_reply": "2025-10-07T22:17:24.918482Z",
     "shell.execute_reply.started": "2025-10-07T22:17:24.360035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711/train.csv\n",
      "Uploaded s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711/val.csv\n",
      "Uploaded s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711/test.csv\n",
      "Uploaded s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711/production.csv\n",
      "Uploaded s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711/maternal_features_full.csv\n",
      "Uploaded s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711/label_map.json\n",
      "Uploaded s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711/eda_summary.json\n",
      "Uploaded s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711/figures/chart_class_distribution.png\n",
      "Uploaded s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711/figures/chart_age_hist.png\n",
      "Uploaded s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711/figures/chart_bp_box.png\n",
      "Uploaded s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711/figures/chart_corr_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "# Upload the CSVs, label map, EDA summary, and figures to your default bucket/prefix.\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def s3_upload(local: Path, key: str):\n",
    "    s3.upload_file(str(local), bucket, f\"{S3_PREFIX}/{key}\")\n",
    "    print(\"Uploaded\", f\"s3://{bucket}/{S3_PREFIX}/{key}\")\n",
    "\n",
    "# CSVs + summaries\n",
    "for fname in [\"train.csv\",\"val.csv\",\"test.csv\",\"production.csv\",\n",
    "              \"maternal_features_full.csv\",\"label_map.json\",\"eda_summary.json\"]:\n",
    "    s3_upload(ARTIFACTS_DIR / fname, fname)\n",
    "\n",
    "# Plots\n",
    "for fname in [\"chart_class_distribution.png\",\"chart_age_hist.png\",\"chart_bp_box.png\",\"chart_corr_heatmap.png\"]:\n",
    "    s3_upload(ARTIFACTS_DIR / fname, f\"figures/{fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0280c5-9725-46f2-8231-3995920e9b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T15:14:52.533587Z",
     "iopub.status.busy": "2025-09-20T15:14:52.533071Z",
     "iopub.status.idle": "2025-09-20T15:14:52.537175Z",
     "shell.execute_reply": "2025-09-20T15:14:52.536233Z",
     "shell.execute_reply.started": "2025-09-20T15:14:52.533545Z"
    }
   },
   "source": [
    "#### Sanitize column names (Feature Store regex) & write sanitized splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d6fae3-05c2-4908-b3a2-30b588255b4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:17:26.109655Z",
     "iopub.status.busy": "2025-10-07T22:17:26.109377Z",
     "iopub.status.idle": "2025-10-07T22:17:26.139820Z",
     "shell.execute_reply": "2025-10-07T22:17:26.138885Z",
     "shell.execute_reply.started": "2025-10-07T22:17:26.109634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized splits saved.\n"
     ]
    }
   ],
   "source": [
    "# FS rules: names must be letters/numbers/hyphens only; must start with alnum; <=64 chars.\n",
    "\n",
    "def sanitize_col(name: str) -> str:\n",
    "    if name == \"SBP_to_DBP\": name = \"SBPtoDBP\"   # preserve meaning\n",
    "    if name.startswith(\"z_\"): name = \"z\" + name[2:]\n",
    "    name = name.replace(\"_\", \"\")\n",
    "    name = \"\".join(ch for ch in name if ch.isalnum() or ch == \"-\")\n",
    "    if not name or not name[0].isalnum(): name = \"f\" + name\n",
    "    return name[:64]\n",
    "\n",
    "def sanitize_df_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    newcols, seen = [], set()\n",
    "    for c in df.columns:\n",
    "        s = sanitize_col(c)\n",
    "        if s in seen:\n",
    "            i, base = 2, s\n",
    "            while f\"{base}{i}\" in seen: i += 1\n",
    "            s = f\"{base}{i}\"\n",
    "        newcols.append(s); seen.add(s)\n",
    "    out = df.copy(); out.columns = newcols\n",
    "    return out\n",
    "\n",
    "label_col = \"label\"\n",
    "def sanitize_split(df):\n",
    "    feats = df.drop(columns=[label_col])\n",
    "    feats = sanitize_df_cols(feats)\n",
    "    feats[label_col] = df[label_col].values\n",
    "    return feats\n",
    "\n",
    "train_s = sanitize_split(train_df); train_s.to_csv(ARTIFACTS_DIR/\"train_sanitized.csv\", index=False)\n",
    "val_s   = sanitize_split(val_df);   val_s.to_csv(ARTIFACTS_DIR/\"val_sanitized.csv\",   index=False)\n",
    "test_s  = sanitize_split(test_df);  test_s.to_csv(ARTIFACTS_DIR/\"test_sanitized.csv\", index=False)\n",
    "prod_s  = sanitize_split(prod_df);  prod_s.to_csv(ARTIFACTS_DIR/\"production_sanitized.csv\", index=False)\n",
    "\n",
    "print(\"Sanitized splits saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a97e7b-0caa-44c8-bf22-b0937dc35491",
   "metadata": {},
   "source": [
    "#### Create & ingest Feature Store (OFFLINE, unique names per run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41bbe8b7-6ef8-4721-b6be-35b82e2dc8e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:17:28.335981Z",
     "iopub.status.busy": "2025-10-07T22:17:28.335688Z",
     "iopub.status.idle": "2025-10-07T22:19:09.518119Z",
     "shell.execute_reply": "2025-10-07T22:19:09.517270Z",
     "shell.execute_reply.started": "2025-10-07T22:17:28.335960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status mhr-train-fg-20251007-221711: Creating\n",
      "[READY] mhr-train-fg-20251007-221711\n",
      "[OK] Ingested 322 rows → mhr-train-fg-20251007-221711\n",
      "Status mhr-val-fg-20251007-221711: Creating\n",
      "[READY] mhr-val-fg-20251007-221711\n",
      "[OK] Ingested 81 rows → mhr-val-fg-20251007-221711\n",
      "Status mhr-batch-fg-20251007-221711: Creating\n",
      "[READY] mhr-batch-fg-20251007-221711\n",
      "[OK] Ingested 324 rows → mhr-batch-fg-20251007-221711\n",
      "Feature Store complete: mhr-train-fg-20251007-221711 mhr-val-fg-20251007-221711 mhr-batch-fg-20251007-221711\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.feature_definition import FeatureDefinition, FeatureTypeEnum\n",
    "\n",
    "sm      = boto3.client(\"sagemaker\")\n",
    "session = Session(boto3.session.Session(region_name=region))\n",
    "\n",
    "def ensure_id_time(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_in.copy()\n",
    "    if \"recordid\" not in df.columns:\n",
    "        df[\"recordid\"] = range(1, len(df)+1)\n",
    "    if \"eventtime\" not in df.columns:\n",
    "        df[\"eventtime\"] = pd.Timestamp.utcnow().isoformat()\n",
    "    return df\n",
    "\n",
    "def to_boto_feature_defs(df: pd.DataFrame):\n",
    "    out = []\n",
    "    for c, d in df.dtypes.items():\n",
    "        if c == \"eventtime\":\n",
    "            t = \"String\"\n",
    "        elif pd.api.types.is_integer_dtype(d):\n",
    "            t = \"Integral\"\n",
    "        elif pd.api.types.is_float_dtype(d):\n",
    "            t = \"Fractional\"\n",
    "        else:\n",
    "            t = \"String\"\n",
    "        out.append({\"FeatureName\": c, \"FeatureType\": t})\n",
    "    return out\n",
    "\n",
    "def create_fg_boto3(name: str, df_local: pd.DataFrame, s3_uri: str):\n",
    "    fdefs = to_boto_feature_defs(df_local)\n",
    "    try:\n",
    "        resp = sm.create_feature_group(\n",
    "            FeatureGroupName=name,\n",
    "            RecordIdentifierFeatureName=\"recordid\",\n",
    "            EventTimeFeatureName=\"eventtime\",\n",
    "            FeatureDefinitions=fdefs,\n",
    "            OfflineStoreConfig={\"S3StorageConfig\": {\"S3Uri\": s3_uri}},\n",
    "            OnlineStoreConfig={\"EnableOnlineStore\": False},\n",
    "            RoleArn=role,\n",
    "            Description=f\"Maternal Health Risk – {name}\",\n",
    "        )\n",
    "        return resp\n",
    "    except sm.exceptions.ResourceInUse:\n",
    "        # Already exists --> safe to reuse after we confirm it's Created\n",
    "        return {\"FeatureGroupArn\": f\"arn:aws:sagemaker:{region}:{boto3.client('sts').get_caller_identity()['Account']}:feature-group/{name}\"}\n",
    "\n",
    "def wait_fg_created(name: str, timeout_s: int = 900, poll_s: int = 10):\n",
    "    start = time.time()\n",
    "    last = \"\"\n",
    "    while True:\n",
    "        desc = sm.describe_feature_group(FeatureGroupName=name)\n",
    "        status = desc.get(\"FeatureGroupStatus\", \"\")\n",
    "        if status == \"Created\":\n",
    "            print(f\"[READY] {name}\")\n",
    "            return desc\n",
    "        if status == \"CreateFailed\":\n",
    "            raise RuntimeError(f\"{name} failed: {desc.get('FailureReason')}\")\n",
    "        if time.time() - start > timeout_s:\n",
    "            raise TimeoutError(f\"Timeout waiting for {name} (last status={status})\")\n",
    "        if status != last:\n",
    "            print(f\"Status {name}: {status}\")\n",
    "            last = status\n",
    "        time.sleep(poll_s)\n",
    "\n",
    "def create_and_ingest(name_base: str, df_local: pd.DataFrame):\n",
    "    # unique FG names per run to avoid collisions\n",
    "    name = f\"{name_base}-{RUN_ID}\"             # e.g., mhr-train-fg-20250920-154301\n",
    "    assert \"_\" not in name, \"FG name must not contain underscores.\"\n",
    "    df_local = ensure_id_time(df_local)\n",
    "    s3_uri   = f\"s3://{bucket}/{S3_PREFIX}/feature-store/{name}\"\n",
    "\n",
    "    create_fg_boto3(name, df_local, s3_uri)\n",
    "    wait_fg_created(name)\n",
    "\n",
    "    fg = FeatureGroup(name=name, sagemaker_session=session)\n",
    "    fg.load_feature_definitions(data_frame=df_local)    # make sure SDK knows schema\n",
    "    fg.ingest(data_frame=df_local, max_workers=4, wait=True)\n",
    "    print(f\"[OK] Ingested {len(df_local)} rows → {name}\")\n",
    "    return name\n",
    "\n",
    "# Load sanitized splits\n",
    "train_s = pd.read_csv(ARTIFACTS_DIR/\"train_sanitized.csv\")\n",
    "val_s   = pd.read_csv(ARTIFACTS_DIR/\"val_sanitized.csv\")\n",
    "prod_s  = pd.read_csv(ARTIFACTS_DIR/\"production_sanitized.csv\")\n",
    "\n",
    "# Create OFFLINE FGs with unique names\n",
    "FG_TRAIN = create_and_ingest(\"mhr-train-fg\", train_s)\n",
    "FG_VAL   = create_and_ingest(\"mhr-val-fg\",   val_s)\n",
    "FG_BATCH = create_and_ingest(\"mhr-batch-fg\", prod_s)\n",
    "\n",
    "print(\"Feature Store complete:\", FG_TRAIN, FG_VAL, FG_BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91005583-4b6c-4ecc-8829-abe82eb153cd",
   "metadata": {},
   "source": [
    "#### Tracker update (JSON + Markdown) and upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cba8c17-f8e7-43f6-98c9-526e5b7e5fae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:19:41.932527Z",
     "iopub.status.busy": "2025-10-07T22:19:41.932092Z",
     "iopub.status.idle": "2025-10-07T22:19:42.085104Z",
     "shell.execute_reply": "2025-10-07T22:19:42.084311Z",
     "shell.execute_reply.started": "2025-10-07T22:19:41.932493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracker written & uploaded.\n"
     ]
    }
   ],
   "source": [
    "tracker = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"s3_prefix\": f\"s3://{bucket}/{S3_PREFIX}\",\n",
    "    \"dataset\": {\n",
    "        \"rows\": eda_summary[\"rows\"], \"cols\": eda_summary[\"cols\"],\n",
    "        \"class_counts\": eda_summary[\"class_counts\"],\n",
    "        \"dtypes\": eda_summary[\"dtypes\"],\n",
    "        \"missing\": eda_summary[\"missing_counts\"],\n",
    "    },\n",
    "    \"splits\": {\n",
    "        \"train_rows\": len(train_df), \"val_rows\": len(val_df),\n",
    "        \"test_rows\": len(test_df), \"prod_rows\": len(prod_df),\n",
    "    },\n",
    "    \"feature_store_groups\": [FG_TRAIN, FG_VAL, FG_BATCH],\n",
    "}\n",
    "with open(ARTIFACTS_DIR / \"team_tracker_update_week3.json\", \"w\") as f:\n",
    "    json.dump(tracker, f, indent=2)\n",
    "\n",
    "md = f\"\"\"# Week 3 Tracker — Maternal Health Risk (RUN: {RUN_ID})\n",
    "\n",
    "**S3 prefix:** s3://{bucket}/{S3_PREFIX}\n",
    "\n",
    "## Dataset\n",
    "- Rows: {eda_summary['rows']} | Cols: {eda_summary['cols']}\n",
    "- Classes: {eda_summary['class_counts']}\n",
    "\n",
    "## Splits\n",
    "- Train: {len(train_df)} (~40%)\n",
    "- Val:   {len(val_df)} (~10%)\n",
    "- Test:  {len(test_df)} (~10%)\n",
    "- Prod:  {len(prod_df)} (~40%)\n",
    "\n",
    "## Feature Store (offline)\n",
    "- {FG_TRAIN}\n",
    "- {FG_VAL}\n",
    "- {FG_BATCH}\n",
    "\"\"\"\n",
    "with open(ARTIFACTS_DIR / \"team_tracker_update_week3.md\", \"w\") as f:\n",
    "    f.write(md)\n",
    "\n",
    "# Upload tracker docs\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(str(ARTIFACTS_DIR/\"team_tracker_update_week3.json\"), bucket, f\"{S3_PREFIX}/team_tracker_update_week3.json\")\n",
    "s3.upload_file(str(ARTIFACTS_DIR/\"team_tracker_update_week3.md\"),   bucket, f\"{S3_PREFIX}/team_tracker_update_week3.md\")\n",
    "\n",
    "print(\"Tracker written & uploaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c762b5-98d8-4628-85c1-c35c454f3fd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:19:44.577391Z",
     "iopub.status.busy": "2025-10-07T22:19:44.577111Z",
     "iopub.status.idle": "2025-10-07T22:19:44.649157Z",
     "shell.execute_reply": "2025-10-07T22:19:44.648416Z",
     "shell.execute_reply.started": "2025-10-07T22:19:44.577368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_id': '20251007-221711',\n",
       " 's3_prefix': 's3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711',\n",
       " 'dataset': {'rows': 808,\n",
       "  'cols': 7,\n",
       "  'class_counts': {'low risk': 478, 'high risk': 330},\n",
       "  'dtypes': {'Age': 'int64',\n",
       "   'SystolicBP': 'int64',\n",
       "   'DiastolicBP': 'int64',\n",
       "   'BS': 'float64',\n",
       "   'BodyTemp': 'float64',\n",
       "   'HeartRate': 'int64',\n",
       "   'RiskLevel': 'object'},\n",
       "  'missing': {'Age': 0,\n",
       "   'SystolicBP': 0,\n",
       "   'DiastolicBP': 0,\n",
       "   'BS': 0,\n",
       "   'BodyTemp': 0,\n",
       "   'HeartRate': 0,\n",
       "   'RiskLevel': 0}},\n",
       " 'splits': {'train_rows': 322,\n",
       "  'val_rows': 81,\n",
       "  'test_rows': 81,\n",
       "  'prod_rows': 324},\n",
       " 'feature_store_groups': ['mhr-train-fg-20251007-221711',\n",
       "  'mhr-val-fg-20251007-221711',\n",
       "  'mhr-batch-fg-20251007-221711']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View\n",
    "\n",
    "import boto3, json\n",
    "s3 = boto3.client(\"s3\")\n",
    "obj = s3.get_object(Bucket=bucket, Key=f\"{S3_PREFIX}/team_tracker_update_week3.json\")\n",
    "tracker = json.load(obj[\"Body\"])\n",
    "tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dcc032-351e-4674-a70b-409d5bf9c472",
   "metadata": {},
   "source": [
    "## Week 4, Model Development and Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26772f-4388-4d0e-bd69-797a9f5d678b",
   "metadata": {},
   "source": [
    "#### Auto settings; continues from Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d69f10eb-4545-4fa0-9684-1230a160bee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:19:46.990510Z",
     "iopub.status.busy": "2025-10-07T22:19:46.990223Z",
     "iopub.status.idle": "2025-10-07T22:19:47.005352Z",
     "shell.execute_reply": "2025-10-07T22:19:47.004613Z",
     "shell.execute_reply.started": "2025-10-07T22:19:46.990484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Week-3: s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711\n",
      "Writing Week-4: s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week4/20251007-221946\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AAI-540 — Week 4: Benchmark (LogReg), Main Model (XGBoost), Evaluation, Batch Deploy\n",
    "This script:\n",
    "  1) Reuses Week-3 splits from the latest run (or pass WEEK3_PREFIX env var)\n",
    "  2) Trains a *benchmark* Logistic Regression (Age + SystolicBP) via SKLearn Estimator\n",
    "  3) Trains a full-feature **XGBoost** model (built-in container)\n",
    "  4) Compares metrics on the test set and writes confusion matrices\n",
    "  5) Runs **Batch Transform** on Week-3 production data\n",
    "  6) Writes a Week-4 tracker + design-doc snippet with S3 artifact links\n",
    "\"\"\"\n",
    "\n",
    "import os, io, json, time, tarfile\n",
    "from pathlib import Path\n",
    "import boto3, sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Reuse Week-3 objects if they exist; otherwise, auto-init (no manual config)\n",
    "try:\n",
    "    bucket\n",
    "    sm_session\n",
    "    role\n",
    "except NameError:\n",
    "    boto_sess  = boto3.session.Session()\n",
    "    sm_session = Session(boto_sess)\n",
    "    role       = get_execution_role()\n",
    "    bucket     = sm_session.default_bucket()\n",
    "\n",
    "# Use the Week-3 S3 prefix if it’s still in memory; otherwise pick the latest run\n",
    "s3 = boto3.client(\"s3\")\n",
    "try:\n",
    "    WEEK3_PREFIX = S3_PREFIX  # from Week 3 cells\n",
    "except NameError:\n",
    "    base = \"aai540/maternal-risk/week3/\"\n",
    "    resp = s3.list_objects_v2(Bucket=bucket, Prefix=base, Delimiter=\"/\")\n",
    "    runs = [cp[\"Prefix\"].rstrip(\"/\") for cp in resp.get(\"CommonPrefixes\", [])]\n",
    "    assert runs, f\"No Week-3 artifacts found under s3://{bucket}/{base}\"\n",
    "    WEEK3_PREFIX = sorted(runs)[-1]\n",
    "\n",
    "# Create a unique Week-4 prefix\n",
    "RUN_ID = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "W4_PREFIX = f\"aai540/maternal-risk/week4/{RUN_ID}\"\n",
    "\n",
    "print(\"Using Week-3:\", f\"s3://{bucket}/{WEEK3_PREFIX}\")\n",
    "print(\"Writing Week-4:\", f\"s3://{bucket}/{W4_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5bb9a5-d6f2-4a3a-8089-bd8cd2d6bec2",
   "metadata": {},
   "source": [
    "#### Load Week-3 splits (train/val/test) from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a57735c-7195-4ac6-bb29-0220e395e7a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:19:48.673295Z",
     "iopub.status.busy": "2025-10-07T22:19:48.673021Z",
     "iopub.status.idle": "2025-10-07T22:19:48.838546Z",
     "shell.execute_reply": "2025-10-07T22:19:48.837627Z",
     "shell.execute_reply.started": "2025-10-07T22:19:48.673273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (322, 19) (81, 19) (81, 19)\n",
      "Train label balance: {0: 190, 1: 132}\n"
     ]
    }
   ],
   "source": [
    "# LOAD SPLITS\n",
    "\n",
    "def read_csv_from_s3(key: str) -> pd.DataFrame:\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "train = read_csv_from_s3(f\"{WEEK3_PREFIX}/train.csv\")\n",
    "val   = read_csv_from_s3(f\"{WEEK3_PREFIX}/val.csv\")\n",
    "test  = read_csv_from_s3(f\"{WEEK3_PREFIX}/test.csv\")\n",
    "\n",
    "label_col = \"label\"\n",
    "X_train, y_train = train.drop(columns=[label_col]), train[label_col]\n",
    "X_val,   y_val   = val.drop(columns=[label_col]),   val[label_col]\n",
    "X_test,  y_test  = test.drop(columns=[label_col]),  test[label_col]\n",
    "\n",
    "print(\"Loaded:\", train.shape, val.shape, test.shape)\n",
    "print(\"Train label balance:\", y_train.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da29fc7-8a77-4ae7-93ab-214810086a8f",
   "metadata": {},
   "source": [
    "#### Benchmark model in SageMaker (very simple: Logistic Regression on 2 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92889a8b-9ca5-418b-bd45-217acbc0e9f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:19:52.196978Z",
     "iopub.status.busy": "2025-10-07T22:19:52.196669Z",
     "iopub.status.idle": "2025-10-07T22:22:39.423682Z",
     "shell.execute_reply": "2025-10-07T22:22:39.422701Z",
     "shell.execute_reply.started": "2025-10-07T22:19:52.196953Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-scikit-learn-2025-10-07-22-19-52-447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 22:19:53 Starting - Starting the training job...\n",
      "2025-10-07 22:20:07 Starting - Preparing the instances for training...\n",
      "2025-10-07 22:20:54 Downloading - Downloading the training image........\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m2025-10-07 22:22:02,796 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2025-10-07 22:22:02,800 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:22:02,803 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:22:02,819 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:22:03,129 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:22:03,133 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:22:03,151 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:22:03,154 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:22:03,171 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:22:03,174 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:22:03,190 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.large\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.large\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2025-10-07-22-19-52-447\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-849121223812/sagemaker-scikit-learn-2025-10-07-22-19-52-447/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"baseline_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"baseline_train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=baseline_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m5.large\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=baseline_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-849121223812/sagemaker-scikit-learn-2025-10-07-22-19-52-447/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.large\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"sagemaker-scikit-learn-2025-10-07-22-19-52-447\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-849121223812/sagemaker-scikit-learn-2025-10-07-22-19-52-447/source/sourcedir.tar.gz\",\"module_name\":\"baseline_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"baseline_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python39.zip:/miniconda3/lib/python3.9:/miniconda3/lib/python3.9/lib-dynload:/miniconda3/lib/python3.9/site-packages:/miniconda3/lib/python3.9/site-packages/setuptools/_vendor\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python baseline_train.py\u001b[0m\n",
      "\u001b[34m2025-10-07 22:22:03,191 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:22:03,191 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:22:04,046 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-10-07 22:22:23 Training - Training image download completed. Training in progress.\n",
      "2025-10-07 22:22:23 Uploading - Uploading generated training model\n",
      "2025-10-07 22:22:23 Completed - Training job completed\n",
      "Training seconds: 115\n",
      "Billable seconds: 115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7654320987654321,\n",
       " 'precision': 0.71875,\n",
       " 'recall': 0.696969696969697,\n",
       " 'f1': 0.7076923076923077,\n",
       " 'roc_auc': 0.790719696969697}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why: have a simple, interpretable baseline for comparison (MVP).\n",
    "from sagemaker.sklearn import SKLearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "bm_feats = [\"Age\", \"SystolicBP\"]\n",
    "assert all(f in X_train.columns for f in bm_feats), \"Expected baseline features missing.\"\n",
    "\n",
    "# Stage small CSVs (filenames don't matter inside channels)\n",
    "w4_local = Path(\"w4_benchmark\"); w4_local.mkdir(exist_ok=True)\n",
    "pd.concat([X_train[bm_feats], y_train], axis=1).to_csv(w4_local/\"train_benchmark.csv\", index=False)\n",
    "pd.concat([X_val[bm_feats],   y_val],   axis=1).to_csv(w4_local/\"val_benchmark.csv\",   index=False)\n",
    "\n",
    "bm_train_s3 = sm_session.upload_data(str(w4_local/\"train_benchmark.csv\"), key_prefix=f\"{W4_PREFIX}/benchmark\")\n",
    "bm_val_s3   = sm_session.upload_data(str(w4_local/\"val_benchmark.csv\"),   key_prefix=f\"{W4_PREFIX}/benchmark\")\n",
    "\n",
    "# Entry script: read first *.csv in each channel, fit LR, write metrics + model\n",
    "with open(\"baseline_train.py\",\"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "import os, glob, json, pathlib\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "def first_csv_in(d):\n",
    "    files = sorted(glob.glob(os.path.join(d, '*.csv')))\n",
    "    assert files, f'No CSV found in {d}'\n",
    "    return files[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_dir = os.environ.get('SM_CHANNEL_TRAIN', '/opt/ml/input/data/train')\n",
    "    val_dir   = os.environ.get('SM_CHANNEL_VAL',   '/opt/ml/input/data/val')\n",
    "    model_dir = os.environ.get('SM_MODEL_DIR',     '/opt/ml/model')\n",
    "\n",
    "    df_tr = pd.read_csv(first_csv_in(train_dir))\n",
    "    df_va = pd.read_csv(first_csv_in(val_dir))\n",
    "\n",
    "    Xtr, ytr = df_tr[['Age','SystolicBP']], df_tr['label']\n",
    "    Xva, yva = df_va[['Age','SystolicBP']], df_va['label']\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000).fit(Xtr, ytr)\n",
    "\n",
    "    pred  = clf.predict(Xva)\n",
    "    proba = clf.predict_proba(Xva)[:,1]\n",
    "    acc = accuracy_score(yva, pred)\n",
    "    p,r,f1,_ = precision_recall_fscore_support(yva, pred, average='binary', zero_division=0)\n",
    "    try: auc = roc_auc_score(yva, proba)\n",
    "    except: auc = float('nan')\n",
    "\n",
    "    pathlib.Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "    import joblib\n",
    "    joblib.dump(clf, os.path.join(model_dir, 'model.joblib'))\n",
    "    with open(os.path.join(model_dir, 'metrics.json'), 'w') as f:\n",
    "        json.dump({'accuracy':acc,'precision':p,'recall':r,'f1':f1,'roc_auc':auc}, f)\n",
    "\"\"\")\n",
    "\n",
    "bm_est = SKLearn(\n",
    "    entry_point=\"baseline_train.py\",\n",
    "    framework_version=\"1.2-1\",     # use a tag compatible with your Studio image\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    sagemaker_session=sm_session,\n",
    ")\n",
    "bm_est.fit({\"train\": bm_train_s3, \"val\": bm_val_s3})\n",
    "\n",
    "# Also compute baseline metrics on our held-out TEST for a clean comparison\n",
    "bm_clf   = LogisticRegression(max_iter=1000).fit(train[bm_feats], y_train)\n",
    "bm_proba = bm_clf.predict_proba(test[bm_feats])[:,1]\n",
    "bm_pred  = (bm_proba >= 0.5).astype(int)\n",
    "\n",
    "bm_acc = accuracy_score(y_test, bm_pred)\n",
    "bm_p, bm_r, bm_f1, _ = precision_recall_fscore_support(y_test, bm_pred, average='binary', zero_division=0)\n",
    "try:    bm_auc = roc_auc_score(y_test, bm_proba)\n",
    "except: bm_auc = float(\"nan\")\n",
    "\n",
    "baseline_metrics = {\"accuracy\":bm_acc,\"precision\":bm_p,\"recall\":bm_r,\"f1\":bm_f1,\"roc_auc\":bm_auc}\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07094141-33e0-4b5b-b255-92e41e2e46ec",
   "metadata": {},
   "source": [
    "#### MAIN MODEL in SageMaker (Built-in XGBoost, CSV mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bda5ca46-6455-4069-846a-0ff0dcbb7cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:24:35.213275Z",
     "iopub.status.busy": "2025-10-07T22:24:35.212807Z",
     "iopub.status.idle": "2025-10-07T22:27:53.300187Z",
     "shell.execute_reply": "2025-10-07T22:27:53.299243Z",
     "shell.execute_reply.started": "2025-10-07T22:24:35.213248Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-10-07-22-24-35-764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 22:24:38 Starting - Starting the training job...\n",
      "2025-10-07 22:24:53 Starting - Preparing the instances for training...\n",
      "2025-10-07 22:25:12 Downloading - Downloading input data...\n",
      "2025-10-07 22:25:52 Downloading - Downloading the training image......\n",
      "2025-10-07 22:27:09 Training - Training image download completed. Training in progress.\n",
      "2025-10-07 22:27:09 Uploading - Uploading generated training model\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:27:01.074 ip-10-2-254-89.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:27:01.151 ip-10-2-254-89.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] File path /opt/ml/input/data/train of input files\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Making smlinks from folder /opt/ml/input/data/train to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] creating symlink between Path /opt/ml/input/data/train/w4_xgb_train.csv and destination /tmp/sagemaker_xgboost_input_data/w4_xgb_train.csv9081121086446526640\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] File path /opt/ml/input/data/validation of input files\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Making smlinks from folder /opt/ml/input/data/validation to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] creating symlink between Path /opt/ml/input/data/validation/w4_xgb_val.csv and destination /tmp/sagemaker_xgboost_input_data/w4_xgb_val.csv-6775894113991623914\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Train matrix has 322 rows and 18 columns\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Validation matrix has 81 rows\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:27:01:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:27:01.806 ip-10-2-254-89.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:27:01.807 ip-10-2-254-89.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:27:01.808 ip-10-2-254-89.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:27:01.808 ip-10-2-254-89.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:27:01.887 ip-10-2-254-89.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:27:01.895 ip-10-2-254-89.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.96216#011validation-auc:0.92771\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.96427#011validation-auc:0.92803\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.97984#011validation-auc:0.94760\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.98591#011validation-auc:0.95896\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.98706#011validation-auc:0.95896\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.99187#011validation-auc:0.98295\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.99155#011validation-auc:0.98169\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.99193#011validation-auc:0.98169\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.99288#011validation-auc:0.98232\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.99330#011validation-auc:0.97822\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.99400#011validation-auc:0.97727\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.99448#011validation-auc:0.97917\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.99474#011validation-auc:0.97980\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.99466#011validation-auc:0.97980\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.99498#011validation-auc:0.98106\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.99520#011validation-auc:0.98043\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.99524#011validation-auc:0.98106\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.99607#011validation-auc:0.98295\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.99635#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.99659#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.99703#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.99743#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.99735#011validation-auc:0.98359\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.99715#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.99743#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.99755#011validation-auc:0.98485\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.99767#011validation-auc:0.98359\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.99767#011validation-auc:0.98359\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.99807#011validation-auc:0.98485\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.99807#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.99803#011validation-auc:0.98359\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.99858#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.99862#011validation-auc:0.98485\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.99819#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.99815#011validation-auc:0.98422\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.99823#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.99850#011validation-auc:0.98927\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.99850#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.99866#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.99866#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.99862#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.99870#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:0.99870#011validation-auc:0.98864\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:0.99874#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:0.99878#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:0.99878#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:0.99890#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:0.99882#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:0.99886#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:0.99886#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:0.99878#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:0.99886#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:0.99874#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:0.99878#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:0.99894#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:0.99890#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:0.99890#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:0.99894#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:0.99894#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:0.99894#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:0.99914#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[83]#011train-auc:0.99914#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[84]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:0.99894#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[93]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:0.99906#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:0.99906#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[100]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[101]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[102]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[103]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[104]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[105]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[106]#011train-auc:0.99906#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[107]#011train-auc:0.99906#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[108]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[109]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[110]#011train-auc:0.99906#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[111]#011train-auc:0.99906#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[112]#011train-auc:0.99906#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[113]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[114]#011train-auc:0.99918#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[115]#011train-auc:0.99902#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[116]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[117]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[118]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[119]#011train-auc:0.99894#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[120]#011train-auc:0.99894#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[121]#011train-auc:0.99894#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[122]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[123]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[124]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[125]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[126]#011train-auc:0.99918#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[127]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[128]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[129]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[130]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[131]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[132]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[133]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[134]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[135]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[136]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[137]#011train-auc:0.99930#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[138]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[139]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[140]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[141]#011train-auc:0.99930#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[142]#011train-auc:0.99918#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[143]#011train-auc:0.99930#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[144]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[145]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[146]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[147]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[148]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[149]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[150]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[151]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[152]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[153]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[154]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[155]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[156]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[157]#011train-auc:0.99942#011validation-auc:0.98548\u001b[0m\n",
      "\u001b[34m[158]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[159]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[160]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[161]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[162]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[163]#011train-auc:0.99942#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[164]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[165]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[166]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[167]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[168]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[169]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[170]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[171]#011train-auc:0.99942#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[172]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[173]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[174]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[175]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[176]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[177]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[178]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[179]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[180]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[181]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[182]#011train-auc:0.99942#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[183]#011train-auc:0.99942#011validation-auc:0.98737\u001b[0m\n",
      "\u001b[34m[184]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[185]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[186]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[187]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[188]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[189]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[190]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[191]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[192]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[193]#011train-auc:0.99942#011validation-auc:0.98674\u001b[0m\n",
      "\u001b[34m[194]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[195]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[196]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[197]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[198]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\u001b[34m[199]#011train-auc:0.99942#011validation-auc:0.98611\u001b[0m\n",
      "\n",
      "2025-10-07 22:27:22 Completed - Training job completed\n",
      "Training seconds: 130\n",
      "Billable seconds: 130\n",
      "XGBoost model artifact: s3://sagemaker-us-east-1-849121223812/sagemaker-xgboost-2025-10-07-22-24-35-764/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Why built-in? No entry_point needed; just CSV with label first (no header).\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import sagemaker\n",
    "\n",
    "# Helper to reorder columns to [label, features...] for CSV training\n",
    "def reorder_for_xgb(df):\n",
    "    cols = [label_col] + [c for c in df.columns if c != label_col]\n",
    "    return df[cols]\n",
    "\n",
    "xgb_train_local = reorder_for_xgb(train)\n",
    "xgb_val_local   = reorder_for_xgb(val)\n",
    "\n",
    "xgb_train_path = Path(\"w4_xgb_train.csv\"); xgb_val_path = Path(\"w4_xgb_val.csv\")\n",
    "xgb_train_local.to_csv(xgb_train_path, index=False, header=False)\n",
    "xgb_val_local.to_csv(xgb_val_path,   index=False, header=False)\n",
    "\n",
    "s3_xgb_train = sm_session.upload_data(str(xgb_train_path), key_prefix=f\"{W4_PREFIX}/xgb\")\n",
    "s3_xgb_val   = sm_session.upload_data(str(xgb_val_path),   key_prefix=f\"{W4_PREFIX}/xgb\")\n",
    "\n",
    "def get_xgb_image():\n",
    "    for ver in [\"1.7-1\", \"1.5-1\", \"1.3-1\"]:\n",
    "        try:\n",
    "            return sagemaker.image_uris.retrieve(\"xgboost\", sm_session.boto_region_name, version=ver)\n",
    "        except Exception as e:\n",
    "            print(f\"xgboost {ver} not available → trying next … ({e})\")\n",
    "    raise RuntimeError(\"No compatible built-in XGBoost image found.\")\n",
    "\n",
    "xgb_image_uri = get_xgb_image()\n",
    "\n",
    "xgb_est = Estimator(\n",
    "    image_uri=xgb_image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    sagemaker_session=sm_session,\n",
    "    hyperparameters={\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"eval_metric\":\"auc\",\n",
    "        \"max_depth\":5,\n",
    "        \"eta\":0.2,\n",
    "        \"min_child_weight\":1,\n",
    "        \"subsample\":0.8,\n",
    "        \"colsample_bytree\":0.8,\n",
    "        \"num_round\":200,\n",
    "        \"verbosity\":1,\n",
    "    },\n",
    ")\n",
    "\n",
    "# tell container the training data is CSV (otherwise it expects libsvm)\n",
    "train_input = TrainingInput(s3_data=s3_xgb_train, content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(s3_data=s3_xgb_val,   content_type=\"text/csv\")\n",
    "\n",
    "xgb_est.fit({\"train\": train_input, \"validation\": val_input}, wait=True)\n",
    "xgb_model_artifact = xgb_est.model_data\n",
    "print(\"XGBoost model artifact:\", xgb_model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b71be-2aac-4015-ab09-9a350ab6f71f",
   "metadata": {},
   "source": [
    "#### EVALUATE (compare main vs baseline on TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d01b9ed-7a61-488a-85b9-5514ac69af86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:28:20.569706Z",
     "iopub.status.busy": "2025-10-07T22:28:20.569417Z",
     "iopub.status.idle": "2025-10-07T22:28:20.814168Z",
     "shell.execute_reply": "2025-10-07T22:28:20.813371Z",
     "shell.execute_reply.started": "2025-10-07T22:28:20.569684Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_228/132528516.py:12: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  t.extractall(tmp_dir)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'baseline': {'accuracy': 0.7654320987654321,\n",
       "  'precision': 0.71875,\n",
       "  'recall': 0.696969696969697,\n",
       "  'f1': 0.7076923076923077,\n",
       "  'roc_auc': 0.790719696969697},\n",
       " 'xgboost': {'accuracy': 0.9876543209876543,\n",
       "  'precision': 1.0,\n",
       "  'recall': 0.9696969696969697,\n",
       "  'f1': 0.9846153846153847,\n",
       "  'roc_auc': 0.999368686868687}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def parse_s3_uri(uri: str):\n",
    "    assert uri.startswith(\"s3://\")\n",
    "    p = uri[5:]; b, k = p.split(\"/\", 1)\n",
    "    return b, k\n",
    "\n",
    "tmp_dir = Path(\"w4_tmp\"); tmp_dir.mkdir(exist_ok=True)\n",
    "bkt, key = parse_s3_uri(xgb_model_artifact)\n",
    "boto3.client(\"s3\").download_file(bkt, key, str(tmp_dir/\"model.tar.gz\"))\n",
    "with tarfile.open(tmp_dir/\"model.tar.gz\") as t:\n",
    "    t.extractall(tmp_dir)\n",
    "\n",
    "# Score test set with the trained booster\n",
    "dtest   = xgb.DMatrix(test.drop(columns=[label_col]), label=test[label_col])\n",
    "booster = xgb.Booster(); booster.load_model(str(tmp_dir/\"xgboost-model\"))\n",
    "xgb_proba = booster.predict(dtest)\n",
    "xgb_pred  = (xgb_proba >= 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "xgb_p, xgb_r, xgb_f1, _ = precision_recall_fscore_support(y_test, xgb_pred, average='binary', zero_division=0)\n",
    "xgb_auc = roc_auc_score(y_test, xgb_proba)\n",
    "\n",
    "metrics_compare = {\n",
    "    \"baseline\": baseline_metrics,\n",
    "    \"xgboost\":  {\"accuracy\":xgb_acc,\"precision\":xgb_p,\"recall\":xgb_r,\"f1\":xgb_f1,\"roc_auc\":xgb_auc},\n",
    "}\n",
    "metrics_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f1d73-a865-487d-8792-4f3cf2b7fd8f",
   "metadata": {},
   "source": [
    "#### Deploy via Batch Transform (score Week-3 production.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce2d23ae-613f-4512-ba37-4dcc5420d131",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:28:22.684414Z",
     "iopub.status.busy": "2025-10-07T22:28:22.684128Z",
     "iopub.status.idle": "2025-10-07T22:34:57.486652Z",
     "shell.execute_reply": "2025-10-07T22:34:57.485844Z",
     "shell.execute_reply.started": "2025-10-07T22:28:22.684391Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-10-07-22-28-22-853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature cols count: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2025-10-07-22-28-23-498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:55:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:55:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:55:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:55:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:55:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:55:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:33:55 +0000] [14] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:33:55 +0000] [14] [INFO] Listening at: unix:/tmp/gunicorn.sock (14)\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:33:55 +0000] [14] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:33:55 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:33:55 +0000] [18] [INFO] Booting worker with pid: 18\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:57:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:57:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:57:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:57:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2025-10-07 22:33:55 +0000] [14] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[35m[2025-10-07 22:33:55 +0000] [14] [INFO] Listening at: unix:/tmp/gunicorn.sock (14)\u001b[0m\n",
      "\u001b[35m[2025-10-07 22:33:55 +0000] [14] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2025-10-07 22:33:55 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[35m[2025-10-07 22:33:55 +0000] [18] [INFO] Booting worker with pid: 18\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:57:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:57:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:57:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:57:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:57:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:57:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:57:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:57:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:34:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [07/Oct/2025:22:34:04 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:34:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [07/Oct/2025:22:34:04 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:34:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [07/Oct/2025:22:34:04 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:34:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [07/Oct/2025:22:34:04 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2025-10-07T22:34:04.562:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:34:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [07/Oct/2025:22:34:04 +0000] \"POST /invocations HTTP/1.1\" 200 6571 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:34:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [07/Oct/2025:22:34:04 +0000] \"POST /invocations HTTP/1.1\" 200 6571 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:55:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:55:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:55:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:55:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:55:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:55:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:33:55 +0000] [14] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:33:55 +0000] [14] [INFO] Listening at: unix:/tmp/gunicorn.sock (14)\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:33:55 +0000] [14] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:33:55 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[34m[2025-10-07 22:33:55 +0000] [18] [INFO] Booting worker with pid: 18\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:57:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:57:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:57:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:57:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2025-10-07 22:33:55 +0000] [14] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[35m[2025-10-07 22:33:55 +0000] [14] [INFO] Listening at: unix:/tmp/gunicorn.sock (14)\u001b[0m\n",
      "\u001b[35m[2025-10-07 22:33:55 +0000] [14] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2025-10-07 22:33:55 +0000] [17] [INFO] Booting worker with pid: 17\u001b[0m\n",
      "\u001b[35m[2025-10-07 22:33:55 +0000] [18] [INFO] Booting worker with pid: 18\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:57:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:57:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:57:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:57:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:57:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:33:57:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:57:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:33:57:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:34:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [07/Oct/2025:22:34:04 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:34:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [07/Oct/2025:22:34:04 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:34:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [07/Oct/2025:22:34:04 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:34:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [07/Oct/2025:22:34:04 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2025-10-07T22:34:04.562:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2025-10-07:22:34:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [07/Oct/2025:22:34:04 +0000] \"POST /invocations HTTP/1.1\" 200 6571 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-07:22:34:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [07/Oct/2025:22:34:04 +0000] \"POST /invocations HTTP/1.1\" 200 6571 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "Batch output: s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week4/20251007-221946/batch/outputs\n"
     ]
    }
   ],
   "source": [
    "# Inference expects FEATURES ONLY (no label) in the SAME order as training.\n",
    "\n",
    "from sagemaker.inputs import TransformInput\n",
    "\n",
    "prod_df = read_csv_from_s3(f\"{WEEK3_PREFIX}/production.csv\")\n",
    "\n",
    "# Same feature order as used to create training CSVs\n",
    "FEATURE_COLS = [c for c in train.columns if c != label_col]\n",
    "print(\"Feature cols count:\", len(FEATURE_COLS))\n",
    "\n",
    "prod_features = prod_df[FEATURE_COLS].copy()\n",
    "bt_local = Path(\"w4_production_features_only.csv\")\n",
    "prod_features.to_csv(bt_local, index=False, header=False)\n",
    "\n",
    "s3_bt_input = sm_session.upload_data(str(bt_local), key_prefix=f\"{W4_PREFIX}/batch\")\n",
    "\n",
    "transformer = xgb_est.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/{W4_PREFIX}/batch/outputs\",\n",
    "    accept=\"text/csv\",\n",
    "    assemble_with=\"Line\",\n",
    ")\n",
    "\n",
    "transformer.transform(data=s3_bt_input, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()\n",
    "\n",
    "batch_output_s3 = transformer.output_path\n",
    "print(\"Batch output:\", batch_output_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dcfda8-a55b-44db-a67c-c98009fbf264",
   "metadata": {},
   "source": [
    "#### ARTIFACTS + DESIGN-DOC SNIPPET + TRACKER (upload to S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94aa204a-d7f5-4808-8c49-be0b851cfe97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:40:12.668186Z",
     "iopub.status.busy": "2025-10-07T22:40:12.667730Z",
     "iopub.status.idle": "2025-10-07T22:40:13.718270Z",
     "shell.execute_reply": "2025-10-07T22:40:13.717479Z",
     "shell.execute_reply.started": "2025-10-07T22:40:12.668111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Week 4 Findings — Model Development & Deployment\n",
      "\n",
      "**Benchmark (LogReg on Age + SystolicBP)**  \n",
      "Acc: 0.765 | Prec: 0.719 | Rec: 0.697 | F1: 0.708 | AUC: 0.791\n",
      "\n",
      "**XGBoost (full features)**  \n",
      "Acc: 0.988 | Prec: 1.000 | Rec: 0.970 | F1: 0.985 | AUC: 0.999\n",
      "\n",
      "**Artifacts**  \n",
      "- Metrics JSON: s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week4/20251007-221946/metrics_compare.json  \n",
      "- Baseline CM:  s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week4/20251007-221946/baseline_cm.png  \n",
      "- XGBoost CM:   s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week4/20251007-221946/xgb_cm.png  \n",
      "- XGBoost Model Artifact: s3://sagemaker-us-east-1-849121223812/sagemaker-xgboost-2025-10-07-22-24-35-764/output/model.tar.gz  \n",
      "- Batch Transform Output: s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week4/20251007-221946/batch/outputs\n",
      "\n",
      "Week-4 tracker written & uploaded → s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week4/20251007-221946/team_tracker_update_week4.*\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_cm(cm, title, path):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest'); plt.title(title); plt.colorbar()\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\"); plt.tight_layout(); plt.savefig(path); plt.close()\n",
    "\n",
    "# Confusion matrices (test set)\n",
    "bm_pred = (LogisticRegression(max_iter=1000).fit(train[bm_feats], y_train)\n",
    "           .predict_proba(test[bm_feats])[:,1] >= 0.5).astype(int)\n",
    "bm_cm   = confusion_matrix(y_test, bm_pred)\n",
    "xgb_cm  = confusion_matrix(y_test, xgb_pred)\n",
    "\n",
    "art_dir = Path(\"w4_artifacts\"); art_dir.mkdir(exist_ok=True)\n",
    "plot_cm(bm_cm,  \"Baseline CM\", art_dir/\"baseline_cm.png\")\n",
    "plot_cm(xgb_cm, \"XGBoost CM\",  art_dir/\"xgb_cm.png\")\n",
    "with open(art_dir/\"metrics_compare.json\",\"w\") as f:\n",
    "    json.dump(metrics_compare, f, indent=2)\n",
    "\n",
    "def up(local, key):\n",
    "    boto3.client(\"s3\").upload_file(str(local), bucket, f\"{W4_PREFIX}/{key}\")\n",
    "    return f\"s3://{bucket}/{W4_PREFIX}/{key}\"\n",
    "\n",
    "metrics_s3 = up(art_dir/\"metrics_compare.json\", \"metrics_compare.json\")\n",
    "bm_cm_s3   = up(art_dir/\"baseline_cm.png\",      \"baseline_cm.png\")\n",
    "xgb_cm_s3  = up(art_dir/\"xgb_cm.png\",           \"xgb_cm.png\")\n",
    "\n",
    "design_doc_snippet = f\"\"\"\n",
    "### Week 4 Findings — Model Development & Deployment\n",
    "\n",
    "**Benchmark (LogReg on Age + SystolicBP)**  \n",
    "Acc: {baseline_metrics['accuracy']:.3f} | Prec: {baseline_metrics['precision']:.3f} | Rec: {baseline_metrics['recall']:.3f} | F1: {baseline_metrics['f1']:.3f} | AUC: {baseline_metrics['roc_auc']:.3f}\n",
    "\n",
    "**XGBoost (full features)**  \n",
    "Acc: {metrics_compare['xgboost']['accuracy']:.3f} | Prec: {metrics_compare['xgboost']['precision']:.3f} | Rec: {metrics_compare['xgboost']['recall']:.3f} | F1: {metrics_compare['xgboost']['f1']:.3f} | AUC: {metrics_compare['xgboost']['roc_auc']:.3f}\n",
    "\n",
    "**Artifacts**  \n",
    "- Metrics JSON: {metrics_s3}  \n",
    "- Baseline CM:  {bm_cm_s3}  \n",
    "- XGBoost CM:   {xgb_cm_s3}  \n",
    "- XGBoost Model Artifact: {xgb_model_artifact}  \n",
    "- Batch Transform Output: {batch_output_s3}\n",
    "\"\"\"\n",
    "print(design_doc_snippet)\n",
    "\n",
    "# Tracker (JSON + Markdown)\n",
    "w4_tracker_dir = Path(\"w4_tracker\"); w4_tracker_dir.mkdir(exist_ok=True)\n",
    "tracker_w4 = {\n",
    "    \"week\": \"4\",\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"week3_prefix\": f\"s3://{bucket}/{WEEK3_PREFIX}\",\n",
    "    \"week4_prefix\": f\"s3://{bucket}/{W4_PREFIX}\",\n",
    "    \"benchmark\": baseline_metrics,\n",
    "    \"xgboost\": metrics_compare[\"xgboost\"],\n",
    "    \"artifacts\": {\n",
    "        \"metrics_json\": metrics_s3,\n",
    "        \"baseline_cm\": bm_cm_s3,\n",
    "        \"xgb_cm\": xgb_cm_s3,\n",
    "        \"model_artifact\": xgb_model_artifact,\n",
    "        \"batch_output\": batch_output_s3\n",
    "    }\n",
    "}\n",
    "with open(w4_tracker_dir/\"team_tracker_update_week4.json\",\"w\") as f:\n",
    "    json.dump(tracker_w4, f, indent=2)\n",
    "\n",
    "md = f\"\"\"# Week 4 Tracker – Maternal Health Risk (RUN: {RUN_ID})\n",
    "\n",
    "**Week-3 prefix:** s3://{bucket}/{WEEK3_PREFIX}  \n",
    "**Week-4 prefix:** s3://{bucket}/{W4_PREFIX}\n",
    "\n",
    "## Benchmark (LogReg on Age + SystolicBP)\n",
    "Acc: {baseline_metrics['accuracy']:.3f} | Prec: {baseline_metrics['precision']:.3f} | Rec: {baseline_metrics['recall']:.3f} | F1: {baseline_metrics['f1']:.3f} | AUC: {baseline_metrics['roc_auc']:.3f}\n",
    "\n",
    "# XGBoost (full features)\n",
    "Acc: {metrics_compare['xgboost']['accuracy']:.3f} | Prec: {metrics_compare['xgboost']['precision']:.3f} | Rec: {metrics_compare['xgboost']['recall']:.3f} | F1: {metrics_compare['xgboost']['f1']:.3f} | AUC: {metrics_compare['xgboost']['roc_auc']:.3f}\n",
    "\n",
    "# Artifacts\n",
    "- Metrics JSON: {metrics_s3}\n",
    "- Baseline CM:  {bm_cm_s3}\n",
    "- XGBoost CM:   {xgb_cm_s3}\n",
    "- Model:        {xgb_model_artifact}\n",
    "- Batch Output: {batch_output_s3}\n",
    "\"\"\"\n",
    "with open(w4_tracker_dir/\"team_tracker_update_week4.md\",\"w\") as f:\n",
    "    f.write(md)\n",
    "\n",
    "boto3.client(\"s3\").upload_file(str(w4_tracker_dir/\"team_tracker_update_week4.json\"), bucket, f\"{W4_PREFIX}/team_tracker_update_week4.json\")\n",
    "boto3.client(\"s3\").upload_file(str(w4_tracker_dir/\"team_tracker_update_week4.md\"),   bucket, f\"{W4_PREFIX}/team_tracker_update_week4.md\")\n",
    "\n",
    "print(\"Week-4 tracker written & uploaded →\", f\"s3://{bucket}/{W4_PREFIX}/team_tracker_update_week4.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865223b-9aa9-41a0-9792-ed43e3e75df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "604ca106-1b79-4ace-beaa-b9609d5ee49c",
   "metadata": {},
   "source": [
    "## AAI-540 - Week 5: Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43fbd319-e179-4789-93a0-8de7cbd91d45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:40:35.767818Z",
     "iopub.status.busy": "2025-10-07T22:40:35.767518Z",
     "iopub.status.idle": "2025-10-07T22:40:36.316212Z",
     "shell.execute_reply": "2025-10-07T22:40:36.314658Z",
     "shell.execute_reply.started": "2025-10-07T22:40:35.767792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Region=us-east-1  Bucket=sagemaker-us-east-1-849121223812  Run=20251007-224036\n",
      "Using Week-4: s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week4/20251007-221946\n",
      "Week-3:      s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week3/20251007-221711\n",
      "Model:       s3://sagemaker-us-east-1-849121223812/sagemaker-xgboost-2025-10-07-22-24-35-764/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AAI-540 — Week 5 Monitoring (Maternal Health Risk Prediction)\n",
    "This notebook cell is a complete, reproducible, commented script that delivers:\n",
    "  1) Model monitors (Data Quality + Model Quality)\n",
    "  2) Data monitors (data capture + baseline)\n",
    "  3) Infrastructure monitors (CloudWatch alarms)\n",
    "  4) CloudWatch dashboard\n",
    "  5) S3-hosted Week-5 tracker (MD + JSON) for the Team Project Update\n",
    "\n",
    "\"\"\"\n",
    "import os, io, json, time\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.model_monitor import (\n",
    "    DataCaptureConfig,\n",
    "    DefaultModelMonitor,\n",
    "    ModelQualityMonitor,\n",
    "    CronExpressionGenerator,\n",
    "    DatasetFormat,\n",
    "    EndpointInput,      # older/newer SDK-friendly way to pass MQ attributes\n",
    ")\n",
    "\n",
    "# AWS context & run IDs\n",
    "boto_sess  = boto3.session.Session()\n",
    "region     = boto_sess.region_name\n",
    "sm_sess    = Session(boto_sess)\n",
    "role       = get_execution_role()\n",
    "s3c        = boto3.client(\"s3\")\n",
    "cw         = boto3.client(\"cloudwatch\")\n",
    "bucket     = sm_sess.default_bucket()\n",
    "\n",
    "RUN_ID    = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "W5_PREFIX = f\"aai540/maternal-risk/week5/{RUN_ID}\"\n",
    "\n",
    "print(f\"[INFO] Region={region}  Bucket={bucket}  Run={RUN_ID}\")\n",
    "\n",
    "# Load Week-4 tracker -> model artifact + Week-3 splits\n",
    "WEEK4_PREFIX = os.environ.get(\"WEEK4_PREFIX\")\n",
    "if not WEEK4_PREFIX:\n",
    "    base = \"aai540/maternal-risk/week4/\"\n",
    "    r = s3c.list_objects_v2(Bucket=bucket, Prefix=base, Delimiter=\"/\")\n",
    "    candidates = [cp[\"Prefix\"].rstrip(\"/\") for cp in r.get(\"CommonPrefixes\", [])]\n",
    "    assert candidates, f\"No Week-4 runs found in s3://{bucket}/{base}\"\n",
    "    WEEK4_PREFIX = sorted(candidates)[-1]\n",
    "\n",
    "w4_tracker_key = f\"{WEEK4_PREFIX}/team_tracker_update_week4.json\"\n",
    "w4 = json.loads(s3c.get_object(Bucket=bucket, Key=w4_tracker_key)[\"Body\"].read().decode(\"utf-8\"))\n",
    "MODEL_ARTIFACT = w4[\"artifacts\"][\"model_artifact\"]\n",
    "WEEK3_PREFIX   = w4[\"week3_prefix\"].replace(f\"s3://{bucket}/\", \"\")\n",
    "\n",
    "print(\"Using Week-4:\", f\"s3://{bucket}/{WEEK4_PREFIX}\")\n",
    "print(\"Week-3:     \", f\"s3://{bucket}/{WEEK3_PREFIX}\")\n",
    "print(\"Model:      \", MODEL_ARTIFACT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25626cca-b3fa-4d69-8387-8fbdc49545ca",
   "metadata": {},
   "source": [
    "#### Deploy endpoint with data capture (we'll send a few warm-up inferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6903e08c-1574-4275-944e-283c92abda31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:40:40.593920Z",
     "iopub.status.busy": "2025-10-07T22:40:40.593638Z",
     "iopub.status.idle": "2025-10-07T22:44:12.792806Z",
     "shell.execute_reply": "2025-10-07T22:44:12.792084Z",
     "shell.execute_reply.started": "2025-10-07T22:40:40.593899Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating model with name: mhr-xgb-model-20251007-224036\n",
      "INFO:sagemaker:Creating endpoint-config with name mhr-xgb-w5-20251007-224036\n",
      "INFO:sagemaker:Creating endpoint with name mhr-xgb-w5-20251007-224036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!Endpoint ready: mhr-xgb-w5-20251007-224036\n"
     ]
    }
   ],
   "source": [
    "xgb_image = retrieve(\"xgboost\", region, version=\"1.7-1\")\n",
    "ENDPOINT_NAME = f\"mhr-xgb-w5-{RUN_ID}\"\n",
    "\n",
    "model = Model(\n",
    "    image_uri=xgb_image,\n",
    "    model_data=MODEL_ARTIFACT,\n",
    "    role=role,\n",
    "    sagemaker_session=sm_sess,\n",
    "    name=f\"mhr-xgb-model-{RUN_ID}\",\n",
    ")\n",
    "\n",
    "data_capture_prefix = f\"{W5_PREFIX}/datacapture\"\n",
    "data_capture = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,                             # capture all early traffic\n",
    "    destination_s3_uri=f\"s3://{bucket}/{data_capture_prefix}\",\n",
    "    capture_options=[\"Input\", \"Output\"],\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    data_capture_config=data_capture,\n",
    ")\n",
    "\n",
    "# Re-attach safeguard (in case some Studio images return None)\n",
    "if predictor is None:\n",
    "    predictor = Predictor(endpoint_name=ENDPOINT_NAME, sagemaker_session=sm_sess)\n",
    "\n",
    "print(\"Endpoint ready:\", ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba9a78-551f-4c13-9254-3a70945c4abb",
   "metadata": {},
   "source": [
    "#### Warm up endpoint & upload matching ground-truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "505d306a-2aa6-42d4-a6c4-24c98e27deba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:45:09.298130Z",
     "iopub.status.busy": "2025-10-07T22:45:09.297799Z",
     "iopub.status.idle": "2025-10-07T22:45:09.597289Z",
     "shell.execute_reply": "2025-10-07T22:45:09.596381Z",
     "shell.execute_reply.started": "2025-10-07T22:45:09.298106Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_s3(key: str) -> pd.DataFrame:\n",
    "    obj = s3c.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "val = read_csv_s3(f\"{WEEK3_PREFIX}/val.csv\")\n",
    "feature_cols = [c for c in val.columns if c != \"label\"]\n",
    "\n",
    "# Send 25 rows to create captured traffic\n",
    "payload = \"\\n\".join(\",\".join(map(str, row)) for row in val[feature_cols].iloc[:25].values)\n",
    "_ = predictor.predict(payload, initial_args={\"ContentType\":\"text/csv\"})\n",
    "\n",
    "# Upload the corresponding labels (one per line, no header)\n",
    "gt_dir = Path(\"gt\"); gt_dir.mkdir(exist_ok=True)\n",
    "gt_file = gt_dir / \"val_labels.csv\"\n",
    "val[\"label\"].iloc[:25].to_csv(gt_file, index=False, header=False)\n",
    "s3c.upload_file(str(gt_file), bucket, f\"{W5_PREFIX}/ground-truth/val_labels.csv\")\n",
    "GROUND_TRUTH_S3 = f\"s3://{bucket}/{W5_PREFIX}/ground-truth/val_labels.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b21dc-af49-4218-af35-c1d7f24197da",
   "metadata": {},
   "source": [
    "#### Data Quality monitor - build baseline & schedule hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e2cec84-ee67-4dc9-a87a-879219bd0f16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:45:14.461850Z",
     "iopub.status.busy": "2025-10-07T22:45:14.461566Z",
     "iopub.status.idle": "2025-10-07T22:50:39.828488Z",
     "shell.execute_reply": "2025-10-07T22:50:39.827569Z",
     "shell.execute_reply.started": "2025-10-07T22:45:14.461829Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2025-10-07-22-45-14-608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\u001b[34m2025-10-07 22:47:48.206741: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:48.206778: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:49.852620: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:49.852651: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:49.852675: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-100-50.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:49.852961: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:51,711 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:849121223812:processing-job/baseline-suggestion-job-2025-10-07-22-45-14-608', 'ProcessingJobName': 'baseline-suggestion-job-2025-10-07-22-45-14-608', 'Environment': {'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week5/20251007-224036/baselines/xgb_train_inputs.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week5/20251007-224036/monitoring/data-quality', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.large', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::849121223812:role/LabRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 1800}}\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:51,711 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:51,711 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:51,711 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:51,711 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:51,711 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:52,030 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:52,031 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:52,031 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.large', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.large', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:52,043 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:52,044 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:52,044 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:53,715 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.100.50\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:\u001b[0m\n",
      "\u001b[34m/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:53,734 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:53,747 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-8f86a2a0-33de-4870-90be-4395fb9a45eb\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,746 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,767 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,769 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,773 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,792 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,792 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,792 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,792 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,849 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,873 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,873 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,878 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,881 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Oct 07 22:47:54\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,883 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,883 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,886 INFO util.GSet: 2.0% max memory 1.4 GB = 28.4 MB\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,887 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,942 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,946 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,946 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,946 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,946 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,946 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,946 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,946 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,946 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,946 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,946 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,946 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,985 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,985 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,985 INFO util.GSet: 1.0% max memory 1.4 GB = 14.2 MB\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,985 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,986 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,986 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,986 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,987 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,991 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,995 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,995 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,996 INFO util.GSet: 0.25% max memory 1.4 GB = 3.6 MB\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:54,996 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,004 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,004 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,004 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,008 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,008 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,011 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,011 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,012 INFO util.GSet: 0.029999999329447746% max memory 1.4 GB = 436.4 KB\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,012 INFO util.GSet: capacity      = 2^16 = 65536 entries\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,041 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1759127296-10.2.100.50-1759877275033\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,058 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,071 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,179 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,199 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,206 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.100.50\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:55,215 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:57,280 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:57,281 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:59,405 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-07 22:47:59,405 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:01,692 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:01,692 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:03,949 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:03,950 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:06,418 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:06,419 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:16,427 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:19,508 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:20,103 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:20,152 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:20,175 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:20,999 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,027 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,028 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,028 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,029 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,077 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 5664, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,095 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,097 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,187 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,188 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,188 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,189 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,189 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,788 INFO util.Utils: Successfully started service 'sparkDriver' on port 34095.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,865 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,936 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,973 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:21,974 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:22,020 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:22,078 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-5378c2b8-fdd0-4e2a-abf4-a0ceb171b825\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:22,102 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:22,164 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:22,212 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.2.100.50:34095/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1759877300993\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:23,016 INFO client.RMProxy: Connecting to ResourceManager at /10.2.100.50:8032\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:23,855 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:23,856 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:23,862 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (7724 MB per container)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:23,863 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:23,864 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:23,864 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:23,871 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:23,968 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:26,503 INFO yarn.Client: Uploading resource file:/tmp/spark-7b5a5377-b171-4c95-8131-9ed45d345bd9/__spark_libs__7938405398005054245.zip -> hdfs://10.2.100.50/user/root/.sparkStaging/application_1759877283102_0001/__spark_libs__7938405398005054245.zip\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:28,863 INFO yarn.Client: Uploading resource file:/tmp/spark-7b5a5377-b171-4c95-8131-9ed45d345bd9/__spark_conf__8757236601936956483.zip -> hdfs://10.2.100.50/user/root/.sparkStaging/application_1759877283102_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:28,935 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:28,936 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:28,937 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:28,937 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:28,939 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:28,979 INFO yarn.Client: Submitting application application_1759877283102_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:29,203 INFO impl.YarnClientImpl: Submitted application application_1759877283102_0001\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:30,207 INFO yarn.Client: Application report for application_1759877283102_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:30,212 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Tue Oct 07 22:48:29 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1759877309089\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1759877283102_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:31,215 INFO yarn.Client: Application report for application_1759877283102_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:32,222 INFO yarn.Client: Application report for application_1759877283102_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:33,225 INFO yarn.Client: Application report for application_1759877283102_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:34,235 INFO yarn.Client: Application report for application_1759877283102_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:35,240 INFO yarn.Client: Application report for application_1759877283102_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:36,251 INFO yarn.Client: Application report for application_1759877283102_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:37,255 INFO yarn.Client: Application report for application_1759877283102_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:37,256 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.100.50\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1759877309089\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1759877283102_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:37,258 INFO cluster.YarnClientSchedulerBackend: Application application_1759877283102_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:37,311 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39489.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:37,311 INFO netty.NettyBlockTransferService: Server created on 10.2.100.50:39489\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:37,313 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:37,323 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.100.50, 39489, None)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:37,333 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.100.50:39489 with 1458.6 MiB RAM, BlockManagerId(driver, 10.2.100.50, 39489, None)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:37,337 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.100.50, 39489, None)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:37,349 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.100.50, 39489, None)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:37,586 INFO util.log: Logging initialized @20921ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:37,668 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1759877283102_0001), /proxy/application_1759877283102_0001\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:39,795 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:44,824 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.100.50:43466) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:45,155 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:36609 with 2.8 GiB RAM, BlockManagerId(1, algo-1, 36609, None)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:52,901 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:53,237 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:53,353 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:53,358 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:55,306 INFO datasources.InMemoryFileIndex: It took 96 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:55,634 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,253 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,258 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.100.50:39489 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,263 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,770 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,777 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,781 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 57479\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,865 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,884 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,885 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,886 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,888 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,903 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,977 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,986 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,988 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.100.50:39489 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:56,991 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:57,015 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:57,017 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:57,069 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4629 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:57,453 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:36609 (size: 4.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:58,577 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:36609 (size: 39.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:59,086 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2034 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:59,089 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:59,098 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 2.145 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:59,103 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:59,104 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:59,107 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 2.241626 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:59,371 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.2.100.50:39489 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:48:59,381 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:36609 in memory (size: 4.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:02,798 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:02,800 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:02,803 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 16 more fields>\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,151 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,188 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,189 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.100.50:39489 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,190 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,209 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,267 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,269 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,269 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,270 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,272 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,275 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,382 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,398 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,399 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.100.50:39489 (size: 8.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,401 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,403 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,403 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,410 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:03,514 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:36609 (size: 8.4 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:04,772 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:36609 (size: 39.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:04,951 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:36609 (size: 19.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:05,122 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1716 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:05,124 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 1.846 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:05,128 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:05,129 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:05,129 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:05,130 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 1.861893 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:05,272 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.100.50:39489 in memory (size: 8.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:05,273 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:36609 in memory (size: 8.4 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:05,578 INFO codegen.CodeGenerator: Code generated in 299.651434 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,242 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,489 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,505 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,505 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,506 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,510 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,521 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,568 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 115.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,573 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,577 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.100.50:39489 (size: 35.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,580 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,584 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,584 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,600 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:06,653 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:36609 (size: 35.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,077 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2478 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,080 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,082 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 2.557 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,089 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,089 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,090 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,090 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,183 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,186 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,186 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,186 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,186 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,187 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,205 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 167.6 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,208 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,209 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.100.50:39489 (size: 46.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,210 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,211 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,211 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,213 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,241 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:36609 (size: 46.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,310 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.100.50:43466\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,804 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 591 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,806 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.609 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,806 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,806 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,807 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,807 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.623511 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:09,854 INFO codegen.CodeGenerator: Code generated in 36.971393 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,478 INFO codegen.CodeGenerator: Code generated in 90.957967 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,648 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,655 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,656 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,656 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,656 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,658 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,725 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 38.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,731 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,735 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.100.50:39489 (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,740 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,746 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,750 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,756 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:10,800 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:36609 (size: 16.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:11,543 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 788 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:11,547 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:11,548 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.877 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:11,549 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:11,550 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:11,550 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.901616 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,307 INFO codegen.CodeGenerator: Code generated in 144.514583 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,321 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,321 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,321 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,321 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,322 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,324 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,329 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 75.8 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,332 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,332 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.100.50:39489 (size: 24.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,333 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,335 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,335 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,337 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,350 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:36609 (size: 24.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,535 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 199 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,536 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,537 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.212 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,537 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,537 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,537 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,537 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,904 INFO codegen.CodeGenerator: Code generated in 159.251289 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,938 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,945 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,945 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,945 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,945 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,946 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,949 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 67.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,952 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,953 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.100.50:39489 (size: 19.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,959 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,959 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,959 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,961 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,975 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:36609 (size: 19.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:12,985 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.100.50:43466\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,109 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 148 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,110 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,111 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.164 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,112 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,112 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,113 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.174725 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,281 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:36609 in memory (size: 35.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,310 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.100.50:39489 in memory (size: 35.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,346 INFO codegen.CodeGenerator: Code generated in 204.352145 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,363 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:36609 in memory (size: 16.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,364 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.2.100.50:39489 in memory (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,429 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:36609 in memory (size: 19.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,434 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.2.100.50:39489 in memory (size: 19.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,511 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:36609 in memory (size: 46.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,513 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.100.50:39489 in memory (size: 46.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,591 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.100.50:39489 in memory (size: 24.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,593 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:36609 in memory (size: 24.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,684 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,693 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,694 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,694 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,694 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,694 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,697 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,712 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 31.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,716 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,717 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.100.50:39489 (size: 14.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,717 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,718 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,721 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,723 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:13,734 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:36609 (size: 14.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,896 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 2173 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,896 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,898 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 2.199 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,907 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,907 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,907 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,907 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,908 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,915 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,917 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,918 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.2.100.50:39489 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,920 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,921 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,922 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,925 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,943 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:36609 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:15,951 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.2.100.50:43466\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,008 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 83 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,008 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,009 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.098 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,015 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,016 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,016 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 2.331545 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,304 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,304 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,305 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,305 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,306 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,308 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,319 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 84.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,321 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,321 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.2.100.50:39489 (size: 27.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,323 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,323 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,324 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,325 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,336 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:36609 (size: 27.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,592 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 267 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,593 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.283 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,594 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,594 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,594 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,595 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,594 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,681 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,682 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,682 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,683 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,683 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,684 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,714 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 168.7 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,719 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 46.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,720 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.2.100.50:39489 (size: 46.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,722 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,723 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,723 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,725 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,737 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:36609 (size: 46.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,750 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.2.100.50:43466\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,936 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 212 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,936 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,938 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.252 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,938 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,938 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:16,940 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.258682 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,276 INFO codegen.CodeGenerator: Code generated in 65.084622 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,332 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,333 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,334 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,334 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,337 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,342 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,355 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 38.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,358 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,359 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.2.100.50:39489 (size: 16.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,360 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,360 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,361 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,363 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,375 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:36609 (size: 16.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,443 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 81 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,444 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.098 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,445 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,446 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,446 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:17,447 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.115016 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,094 INFO codegen.CodeGenerator: Code generated in 186.446317 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,112 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,115 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,115 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,115 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,116 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,117 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,122 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 76.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,126 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 24.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,127 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.2.100.50:39489 (size: 24.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,127 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,128 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,128 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,130 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,144 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:36609 (size: 24.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,259 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 129 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,261 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,264 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.144 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,265 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,265 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,265 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,266 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,524 INFO codegen.CodeGenerator: Code generated in 166.334929 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,543 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,551 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,552 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,552 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,552 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,552 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,555 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 67.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,556 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,557 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.2.100.50:39489 (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,558 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,558 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,558 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,563 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,574 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:36609 (size: 19.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,587 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.2.100.50:43466\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,731 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 168 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,731 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,732 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.179 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,732 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,732 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,733 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.182665 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,860 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,862 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,863 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,863 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,863 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,863 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,866 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,887 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 31.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,891 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 14.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,891 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.2.100.50:39489 (size: 14.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,892 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,893 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,893 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,895 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,911 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:36609 (size: 14.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,950 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 55 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,951 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,952 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.083 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,952 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,952 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,952 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,952 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,955 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,957 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,959 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,963 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.2.100.50:39489 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,963 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,964 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,965 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,966 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,979 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:36609 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:18,987 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.2.100.50:43466\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,026 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 60 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,026 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,027 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.071 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,028 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,028 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,028 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.168164 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,275 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,276 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,277 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,277 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,282 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,282 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,286 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 84.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,293 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,295 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.2.100.50:39489 (size: 27.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,296 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,296 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,297 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,298 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,313 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:36609 (size: 27.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,514 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 216 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,514 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,515 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.232 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,515 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,516 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,516 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,516 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,575 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,577 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,579 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,580 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,580 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,581 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,590 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 168.8 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,592 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 46.5 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,593 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.2.100.50:39489 (size: 46.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,593 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,594 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,594 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,596 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,611 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:36609 (size: 46.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,629 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.2.100.50:43466\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,776 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 181 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,776 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,777 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.195 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,782 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,782 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,782 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.206729 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,957 INFO codegen.CodeGenerator: Code generated in 16.334035 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,992 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,993 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,994 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,994 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,995 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:19,995 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,004 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 38.9 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,006 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,006 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.2.100.50:39489 (size: 16.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,007 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,008 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,008 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,011 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,023 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:36609 (size: 16.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,102 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 91 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,105 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,106 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.107 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,107 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,107 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,108 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.115149 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,486 INFO codegen.CodeGenerator: Code generated in 96.149538 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,497 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,498 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,498 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,498 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,499 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,500 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,508 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 74.9 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,512 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,519 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.2.100.50:39489 (size: 24.0 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,525 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,529 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,529 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,531 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,543 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:36609 (size: 24.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,650 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 118 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,650 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,651 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.149 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,652 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,653 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,653 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,653 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,769 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:36609 in memory (size: 14.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,774 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.2.100.50:39489 in memory (size: 14.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,811 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.2.100.50:39489 in memory (size: 46.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,816 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:36609 in memory (size: 46.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,866 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:36609 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,871 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.2.100.50:39489 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,920 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:36609 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,925 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.2.100.50:39489 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,940 INFO codegen.CodeGenerator: Code generated in 185.536449 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,980 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,994 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,994 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,995 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,995 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,995 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:20,999 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 66.5 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,001 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,002 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.2.100.50:39489 (size: 19.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,003 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,003 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,003 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,005 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,006 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:36609 in memory (size: 24.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,018 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:36609 (size: 19.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,022 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.2.100.50:43466\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,031 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.2.100.50:39489 in memory (size: 24.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,097 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:36609 in memory (size: 24.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,103 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.2.100.50:39489 in memory (size: 24.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,169 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 164 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,170 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,171 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.173 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,172 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,173 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,173 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.180744 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,224 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:36609 in memory (size: 14.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,275 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.2.100.50:39489 in memory (size: 14.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,391 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.2.100.50:39489 in memory (size: 16.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,404 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:36609 in memory (size: 16.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,429 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,431 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,432 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,432 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,432 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,432 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,435 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,447 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 31.3 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,449 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 14.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,450 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.2.100.50:39489 (size: 14.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,451 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,452 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,452 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,454 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,473 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:36609 (size: 14.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,512 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.2.100.50:39489 in memory (size: 46.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,516 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:36609 in memory (size: 46.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,581 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.2.100.50:39489 in memory (size: 27.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,597 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 143 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,598 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,598 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:36609 in memory (size: 27.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,599 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.161 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,602 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,602 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,602 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,602 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,603 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,605 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,607 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,608 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.2.100.50:39489 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,608 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,609 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,609 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,610 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,650 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:36609 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,655 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.2.100.50:43466\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,712 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:36609 in memory (size: 27.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,731 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 121 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,732 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,732 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.128 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,733 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,734 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,734 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.304636 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,738 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.2.100.50:39489 in memory (size: 27.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,794 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.2.100.50:39489 in memory (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,802 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:36609 in memory (size: 19.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,872 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:36609 in memory (size: 16.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:21,875 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.2.100.50:39489 in memory (size: 16.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,210 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,216 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,216 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,216 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,222 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,222 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,234 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 63.6 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,245 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 22.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,251 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.2.100.50:39489 (size: 22.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,256 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,256 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,256 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,258 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,275 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:36609 (size: 22.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,490 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 232 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,491 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,491 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.265 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,492 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,492 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,492 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,492 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,532 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,533 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,534 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,534 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,534 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,534 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,543 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 115.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,546 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,546 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.2.100.50:39489 (size: 35.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,550 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,550 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,551 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,552 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,565 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:36609 (size: 35.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,580 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.2.100.50:43466\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,687 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 134 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,687 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,695 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.151 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,695 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,695 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,696 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.164030 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,714 INFO codegen.CodeGenerator: Code generated in 15.828586 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,871 INFO codegen.CodeGenerator: Code generated in 12.692123 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,906 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,907 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,908 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,908 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,909 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,910 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,920 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 36.8 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,922 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,923 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.2.100.50:39489 (size: 16.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,924 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,924 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,925 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,926 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:22,938 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:36609 (size: 16.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,026 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 100 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,027 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,028 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 0.114 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,028 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,028 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,029 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 0.122393 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,239 INFO codegen.CodeGenerator: Code generated in 60.766477 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,248 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,248 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,248 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,248 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,249 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,250 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,264 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 53.8 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,266 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 18.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,266 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.2.100.50:39489 (size: 18.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,268 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,268 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,269 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,271 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,283 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:36609 (size: 18.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,366 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 95 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,366 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,367 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.116 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,368 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,368 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,368 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,368 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,537 INFO codegen.CodeGenerator: Code generated in 93.38029 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,559 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,560 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,560 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,560 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,562 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,562 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,564 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 43.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,568 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,569 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.2.100.50:39489 (size: 14.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,569 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,570 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,571 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,573 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,589 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:36609 (size: 14.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,593 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.2.100.50:43466\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,642 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 70 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,642 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,643 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.080 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,644 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,644 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,645 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.085616 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,671 INFO codegen.CodeGenerator: Code generated in 18.482026 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,730 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,731 INFO scheduler.DAGScheduler: Registering RDD 150 (countByKey at ColumnProfiler.scala:592) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,732 INFO scheduler.DAGScheduler: Got job 25 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,732 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,732 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,732 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,735 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,743 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 31.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,745 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 14.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,748 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.2.100.50:39489 (size: 14.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,748 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,749 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,749 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,750 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,762 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:36609 (size: 14.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,839 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 89 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,840 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,841 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (countByKey at ColumnProfiler.scala:592) finished in 0.106 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,844 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,845 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,845 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 37)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,845 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,845 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,847 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,849 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,850 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.2.100.50:39489 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,851 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,851 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,851 INFO cluster.YarnScheduler: Adding task set 37.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,853 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,864 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:36609 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,871 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.2.100.50:43466\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,889 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 29) in 37 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,889 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,889 INFO scheduler.DAGScheduler: ResultStage 37 (countByKey at ColumnProfiler.scala:592) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,890 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,891 INFO cluster.YarnScheduler: Killing all running tasks in stage 37: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:23,891 INFO scheduler.DAGScheduler: Job 25 finished: countByKey at ColumnProfiler.scala:592, took 0.161072 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,299 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,355 INFO codegen.CodeGenerator: Code generated in 13.070415 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,372 INFO scheduler.DAGScheduler: Registering RDD 156 (count at StatsGenerator.scala:66) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,373 INFO scheduler.DAGScheduler: Got map stage job 26 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,373 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 38 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,373 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,374 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,374 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[156] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,379 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 23.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,381 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 10.6 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,381 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.2.100.50:39489 (size: 10.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,382 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,382 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[156] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,383 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,384 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,393 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:36609 (size: 10.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,441 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 30) in 57 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,441 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,442 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (count at StatsGenerator.scala:66) finished in 0.066 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,442 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,443 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,443 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,443 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,475 INFO codegen.CodeGenerator: Code generated in 20.245317 ms\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,492 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,496 INFO scheduler.DAGScheduler: Got job 27 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,496 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,496 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,496 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,497 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[159] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,500 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 11.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,502 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,502 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.2.100.50:39489 (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,503 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,503 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[159] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,503 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,505 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,514 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:36609 (size: 5.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,524 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.2.100.50:43466\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,545 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 41 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,545 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,546 INFO scheduler.DAGScheduler: ResultStage 40 (count at StatsGenerator.scala:66) finished in 0.048 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,547 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,548 INFO cluster.YarnScheduler: Killing all running tasks in stage 40: Stage finished\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,548 INFO scheduler.DAGScheduler: Job 27 finished: count at StatsGenerator.scala:66, took 0.053160 s\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,909 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:24,935 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,029 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,030 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,040 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,063 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,112 WARN nio.NioEventLoop: Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@66797887.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,115 INFO nio.NioEventLoop: Migrated 1 channel(s) to the new Selector.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,132 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,137 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,152 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,162 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,222 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,222 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,222 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,260 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,270 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7b5a5377-b171-4c95-8131-9ed45d345bd9\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,288 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-16f62141-8d90-4f54-b828-16e9903795e3\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,512 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2025-10-07 22:49:25,513 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: mhr-dq-sched-20251007-224036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality schedule: mhr-dq-sched-20251007-224036\n"
     ]
    }
   ],
   "source": [
    "# Baseline from Week-4 train CSV (drop label column 0 to reflect inference inputs)\n",
    "resp = s3c.list_objects_v2(Bucket=bucket, Prefix=f\"{WEEK4_PREFIX}/xgb/\")\n",
    "train_keys = [o[\"Key\"] for o in resp.get(\"Contents\", []) if o[\"Key\"].endswith(\"w4_xgb_train.csv\")]\n",
    "assert train_keys, \"Week-4 XGB train CSV not found.\"\n",
    "xgb_train_key = train_keys[0]\n",
    "\n",
    "raw = pd.read_csv(io.BytesIO(s3c.get_object(Bucket=bucket, Key=xgb_train_key)[\"Body\"].read()), header=None)\n",
    "raw.drop(columns=[0]).to_csv(\"xgb_train_inputs.csv\", index=False, header=False)\n",
    "baseline_inputs_key = f\"{W5_PREFIX}/baselines/xgb_train_inputs.csv\"\n",
    "s3c.upload_file(\"xgb_train_inputs.csv\", bucket, baseline_inputs_key)\n",
    "BASELINE_INPUTS_S3 = f\"s3://{bucket}/{baseline_inputs_key}\"\n",
    "\n",
    "dq_output = f\"s3://{bucket}/{W5_PREFIX}/monitoring/data-quality\"\n",
    "dqm = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,                 # < 1 hour cadence\n",
    "    sagemaker_session=sm_sess,\n",
    ")\n",
    "dqm.suggest_baseline(\n",
    "    baseline_dataset=BASELINE_INPUTS_S3,\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=dq_output,\n",
    "    wait=True,\n",
    ")\n",
    "\n",
    "DQ_SCHEDULE = f\"mhr-dq-sched-{RUN_ID}\"\n",
    "dqm.create_monitoring_schedule(\n",
    "    monitor_schedule_name=DQ_SCHEDULE,\n",
    "    endpoint_input=ENDPOINT_NAME,\n",
    "    output_s3_uri=dq_output,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")\n",
    "print(\"Data Quality schedule:\", DQ_SCHEDULE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9818e3f2-4808-40a8-ac8b-cc31d5078b71",
   "metadata": {},
   "source": [
    "#### Model Quality monitor - schedule hourly on previous full hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98d10283-b224-4577-a185-e81333f8399c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:50:45.973636Z",
     "iopub.status.busy": "2025-10-07T22:50:45.970857Z",
     "iopub.status.idle": "2025-10-07T22:50:46.801117Z",
     "shell.execute_reply": "2025-10-07T22:50:46.800099Z",
     "shell.execute_reply.started": "2025-10-07T22:50:45.973599Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: mhr-mq-sched-20251007-224036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Quality schedule: mhr-mq-sched-20251007-224036\n"
     ]
    }
   ],
   "source": [
    "mq_output = f\"s3://{bucket}/{W5_PREFIX}/monitoring/model-quality\"\n",
    "mqm = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,                 # < 1 hour cadence\n",
    "    sagemaker_session=sm_sess,\n",
    ")\n",
    "\n",
    "ENDPOINT_DEST = \"/opt/ml/processing/input/endpoint\"\n",
    "\n",
    "endpoint_input = EndpointInput(\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    destination=ENDPOINT_DEST,\n",
    "    # previous full hour window (works reliably in us-east-1)\n",
    "    start_time_offset=\"-PT2H\",\n",
    "    end_time_offset=\"-PT1H\",\n",
    "    # model output is a single probability in CSV column 0\n",
    "    probability_attribute=\"0\",\n",
    "    probability_threshold_attribute=0.5,\n",
    ")\n",
    "\n",
    "MQ_SCHEDULE = f\"mhr-mq-sched-{RUN_ID}\"\n",
    "mqm.create_monitoring_schedule(\n",
    "    monitor_schedule_name=MQ_SCHEDULE,\n",
    "    endpoint_input=endpoint_input,\n",
    "    ground_truth_input=GROUND_TRUTH_S3,\n",
    "    problem_type=\"BinaryClassification\",\n",
    "    output_s3_uri=mq_output,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")\n",
    "print(\"Model Quality schedule:\", MQ_SCHEDULE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eed816-ac7a-4d86-a194-b3d4bffd1726",
   "metadata": {},
   "source": [
    "#### Infrastructure monitors  CloudWatch alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05c08725-9290-4c6d-85fc-5d9d21f934f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:50:55.278374Z",
     "iopub.status.busy": "2025-10-07T22:50:55.277885Z",
     "iopub.status.idle": "2025-10-07T22:50:55.912107Z",
     "shell.execute_reply": "2025-10-07T22:50:55.910868Z",
     "shell.execute_reply.started": "2025-10-07T22:50:55.278347Z"
    }
   },
   "outputs": [],
   "source": [
    "ALARM_PREFIX = f\"MHR-W5-{RUN_ID}\"\n",
    "\n",
    "def put_alarm(metric, stat, comp, thresh, period=60, evals=1, unit=None):\n",
    "    dims=[{\"Name\":\"EndpointName\",\"Value\":ENDPOINT_NAME},\n",
    "          {\"Name\":\"VariantName\",\"Value\":\"AllTraffic\"}]\n",
    "    params=dict(\n",
    "        AlarmName=f\"{ALARM_PREFIX}-{metric}\",\n",
    "        Namespace=\"AWS/SageMaker\",\n",
    "        MetricName=metric,\n",
    "        Dimensions=dims,\n",
    "        Statistic=stat,\n",
    "        Period=period,\n",
    "        EvaluationPeriods=evals,\n",
    "        ComparisonOperator=comp,\n",
    "        Threshold=thresh,\n",
    "        ActionsEnabled=False,                # flip True + SNS if we want emails\n",
    "        TreatMissingData=\"notBreaching\",\n",
    "    )\n",
    "    if unit: params[\"Unit\"]=unit\n",
    "    cw.put_metric_alarm(**params)\n",
    "\n",
    "put_alarm(\"ModelLatency\",    \"Average\", \"GreaterThanThreshold\", 60000)\n",
    "put_alarm(\"OverheadLatency\", \"Average\", \"GreaterThanThreshold\", 60000)\n",
    "put_alarm(\"5XXErrors\",       \"Sum\",     \"GreaterThanThreshold\", 0)\n",
    "put_alarm(\"Invocations\",     \"Sum\",     \"GreaterThanThreshold\", 500)\n",
    "put_alarm(\"CPUUtilization\",  \"Average\", \"GreaterThanThreshold\", 95, unit=\"Percent\")\n",
    "put_alarm(\"MemoryUtilization\",\"Average\",\"GreaterThanThreshold\", 95, unit=\"Percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67671689-4bed-420f-987e-fa85c8a2a2d6",
   "metadata": {},
   "source": [
    "#### CloudWatch dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65fc69d3-c918-4cd9-9372-32c636ba0874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:50:59.357053Z",
     "iopub.status.busy": "2025-10-07T22:50:59.356746Z",
     "iopub.status.idle": "2025-10-07T22:50:59.480656Z",
     "shell.execute_reply": "2025-10-07T22:50:59.479749Z",
     "shell.execute_reply.started": "2025-10-07T22:50:59.357030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CloudWatch dashboard: MHR-W5-Dashboard-20251007-224036\n"
     ]
    }
   ],
   "source": [
    "DASHBOARD = f\"MHR-W5-Dashboard-{RUN_ID}\"\n",
    "widgets = [\n",
    "  {\"type\":\"metric\",\"width\":12,\"height\":6,\"properties\":{\n",
    "      \"region\":region,\"title\":\"Latency (ms)\",\n",
    "      \"metrics\":[[\"AWS/SageMaker\",\"ModelLatency\",\"EndpointName\",ENDPOINT_NAME,\"VariantName\",\"AllTraffic\"],\n",
    "                 [\".\",\"OverheadLatency\",\".\",\".\",\".\",\".\"]],\n",
    "      \"period\":60,\"stat\":\"Average\",\"view\":\"timeSeries\"}},\n",
    "  {\"type\":\"metric\",\"width\":12,\"height\":6,\"properties\":{\n",
    "      \"region\":region,\"title\":\"Invocations & 5XX\",\n",
    "      \"metrics\":[[\"AWS/SageMaker\",\"Invocations\",\"EndpointName\",ENDPOINT_NAME,\"VariantName\",\"AllTraffic\"],\n",
    "                 [\".\",\"5XXErrors\",\".\",\".\",\".\",\".\"]],\n",
    "      \"period\":60,\"stat\":\"Sum\",\"view\":\"timeSeries\"}}\n",
    "]\n",
    "cw.put_dashboard(DashboardName=DASHBOARD, DashboardBody=json.dumps({\"widgets\":widgets}))\n",
    "print(\"CloudWatch dashboard:\", DASHBOARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c56d5-282d-4ee2-9865-164a53ebf20f",
   "metadata": {},
   "source": [
    "#### Week-5 tracker (JSON + MD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "459e8ed4-fb2c-45c9-806e-25a7353107b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:51:01.239128Z",
     "iopub.status.busy": "2025-10-07T22:51:01.238820Z",
     "iopub.status.idle": "2025-10-07T22:51:01.415540Z",
     "shell.execute_reply": "2025-10-07T22:51:01.414466Z",
     "shell.execute_reply.started": "2025-10-07T22:51:01.239102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week-5 monitoring setup complete.\n"
     ]
    }
   ],
   "source": [
    "tracker = {\n",
    "    \"week\": 5,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"week4_prefix\": f\"s3://{bucket}/{WEEK4_PREFIX}\",\n",
    "    \"week5_prefix\": f\"s3://{bucket}/{W5_PREFIX}\",\n",
    "    \"endpoint\": ENDPOINT_NAME,\n",
    "    \"datacapture_s3\": f\"s3://{bucket}/{data_capture_prefix}\",\n",
    "    \"schedules\": {\"data_quality\": DQ_SCHEDULE, \"model_quality\": MQ_SCHEDULE},\n",
    "    \"outputs\": {\"data_quality\": dq_output, \"model_quality\": mq_output, \"ground_truth\": GROUND_TRUTH_S3},\n",
    "    \"cloudwatch\": {\"alarms_prefix\": ALARM_PREFIX, \"dashboard\": DASHBOARD}\n",
    "}\n",
    "Path(\"w5_tracker\").mkdir(exist_ok=True)\n",
    "json.dump(tracker, open(\"w5_tracker/team_tracker_update_week5.json\",\"w\"), indent=2)\n",
    "open(\"w5_tracker/team_tracker_update_week5.md\",\"w\").write(\n",
    "    f\"# Week 5 Tracker – Maternal Health Risk (RUN: {RUN_ID})\\n\\n\"\n",
    "    f\"**Week-4:** s3://{bucket}/{WEEK4_PREFIX}\\n\"\n",
    "    f\"**Week-5:** s3://{bucket}/{W5_PREFIX}\\n\\n\"\n",
    "    f\"## Endpoint\\n- {ENDPOINT_NAME}\\n- Data Capture: s3://{bucket}/{data_capture_prefix}\\n\\n\"\n",
    "    f\"## Monitoring Schedules\\n- Data Quality: {DQ_SCHEDULE}\\n- Model Quality: {MQ_SCHEDULE}\\n\\n\"\n",
    "    f\"## Outputs\\n- DQ: {dq_output}\\n- MQ: {mq_output}\\n- GT: {GROUND_TRUTH_S3}\\n\\n\"\n",
    "    f\"## CloudWatch\\n- Alarms prefix: {ALARM_PREFIX}\\n- Dashboard: {DASHBOARD}\\n\"\n",
    ")\n",
    "s3c.upload_file(\"w5_tracker/team_tracker_update_week5.json\", bucket, f\"{W5_PREFIX}/team_tracker_update_week5.json\")\n",
    "s3c.upload_file(\"w5_tracker/team_tracker_update_week5.md\",   bucket, f\"{W5_PREFIX}/team_tracker_update_week5.md\")\n",
    "\n",
    "print(\"Week-5 monitoring setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd0818-60e0-4b84-87f8-0366d3dbf1bd",
   "metadata": {},
   "source": [
    "### week5_generate_reports.py <-- We'll run this after at least one schedule execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "606f0377-71b7-4350-81fc-bc473ed8baea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:51:44.407928Z",
     "iopub.status.busy": "2025-10-07T22:51:44.407221Z",
     "iopub.status.idle": "2025-10-07T22:51:44.925296Z",
     "shell.execute_reply": "2025-10-07T22:51:44.924572Z",
     "shell.execute_reply.started": "2025-10-07T22:51:44.407892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report written --> s3://sagemaker-us-east-1-849121223812/aai540/maternal-risk/week5/20251007-224036/monitoring_report.md\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AAI-540 — Week 5 Reporting (student version)\n",
    "- Finds the latest Week-5 run\n",
    "- Lists Data/Model Quality outputs in S3\n",
    "- Writes compact MD + JSON report for Team Update\n",
    "\"\"\"\n",
    "\n",
    "import json, io\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "\n",
    "s3  = boto3.client(\"s3\")\n",
    "bkt = Session().default_bucket()\n",
    "\n",
    "# Find latest Week-5 folder\n",
    "base=\"aai540/maternal-risk/week5/\"\n",
    "resp=s3.list_objects_v2(Bucket=bkt, Prefix=base, Delimiter=\"/\")\n",
    "runs=[cp[\"Prefix\"].rstrip(\"/\") for cp in resp.get(\"CommonPrefixes\",[])]\n",
    "assert runs, f\"No Week-5 runs under s3://{bkt}/{base}\"\n",
    "w5=sorted(runs)[-1]\n",
    "\n",
    "# Load Week-5 tracker\n",
    "tracker=json.loads(s3.get_object(Bucket=bkt, Key=f\"{w5}/team_tracker_update_week5.json\")[\"Body\"].read())\n",
    "dq_out=tracker[\"outputs\"][\"data_quality\"]; mq_out=tracker[\"outputs\"][\"model_quality\"]\n",
    "\n",
    "def list_all(prefix_uri):\n",
    "    \"\"\"List all keys under a given s3://bucket/prefix and return as full s3 URIs.\"\"\"\n",
    "    prefix=prefix_uri.replace(f\"s3://{bkt}/\",\"\")\n",
    "    out=[]; token=None\n",
    "    while True:\n",
    "        kw=dict(Bucket=bkt, Prefix=prefix)\n",
    "        if token: kw[\"ContinuationToken\"]=token\n",
    "        r=s3.list_objects_v2(**kw)\n",
    "        out += [o[\"Key\"] for o in r.get(\"Contents\",[])]\n",
    "        token=r.get(\"NextContinuationToken\")\n",
    "        if not token: break\n",
    "    return [f\"s3://{bkt}/{k}\" for k in out]\n",
    "\n",
    "dq_files=list_all(dq_out)\n",
    "mq_files=list_all(mq_out)\n",
    "\n",
    "report={\n",
    "  \"week\":5,\n",
    "  \"run\":w5,\n",
    "  \"data_quality\":{\n",
    "    \"statistics\": next((f for f in dq_files if f.endswith(\"statistics.json\")), \"N/A\"),\n",
    "    \"constraints\": next((f for f in dq_files if f.endswith(\"constraints.json\")), \"N/A\"),\n",
    "    \"violations\": next((f for f in dq_files if \"constraint_violations.json\" in f), \"N/A\"),\n",
    "  },\n",
    "  \"model_quality\":{\n",
    "    \"files\": [f for f in mq_files if f.endswith((\".json\",\".csv\"))][-10:]\n",
    "  }\n",
    "}\n",
    "\n",
    "Path(\"w5_reports\").mkdir(exist_ok=True)\n",
    "open(\"w5_reports/monitoring_report.md\",\"w\").write(\n",
    "  \"# Week 5 Monitoring Report\\n\\n\"\n",
    "  f\"**Run:** {w5}\\n\\n\"\n",
    "  \"## Data Quality\\n\"\n",
    "  f\"- Statistics: {report['data_quality']['statistics']}\\n\"\n",
    "  f\"- Constraints: {report['data_quality']['constraints']}\\n\"\n",
    "  f\"- Violations: {report['data_quality']['violations']}\\n\\n\"\n",
    "  \"## Model Quality (latest files)\\n\" + \"\\n\".join(f\"- {p}\" for p in report[\"model_quality\"][\"files\"])\n",
    ")\n",
    "open(\"w5_reports/monitoring_report.json\",\"w\").write(json.dumps(report, indent=2))\n",
    "\n",
    "s3.upload_file(\"w5_reports/monitoring_report.md\",   bkt, f\"{w5}/monitoring_report.md\")\n",
    "s3.upload_file(\"w5_reports/monitoring_report.json\", bkt, f\"{w5}/monitoring_report.json\")\n",
    "\n",
    "print(\"Report written -->\", f\"s3://{bkt}/{w5}/monitoring_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047650ec-8836-4caa-afd2-85434a053941",
   "metadata": {},
   "source": [
    "### Week 5 cleanup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60b51900-0587-4bef-a922-5b5daffec9c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T22:53:28.597022Z",
     "iopub.status.busy": "2025-10-07T22:53:28.596748Z",
     "iopub.status.idle": "2025-10-07T22:53:28.602244Z",
     "shell.execute_reply": "2025-10-07T22:53:28.601310Z",
     "shell.execute_reply.started": "2025-10-07T22:53:28.597001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAAI-540 — Week 5 Cleanup\\nDeletes: monitoring schedules, endpoint (+config), CW dashboard & alarms.\\nArtifacts remain in S3 for grading.\\n\\n\\nimport json, boto3\\nfrom sagemaker.session import Session\\n\\ns3 = boto3.client(\"s3\"); sm = boto3.client(\"sagemaker\"); cw=boto3.client(\"cloudwatch\")\\nbkt=Session().default_bucket()\\n\\n# latest Week-5 run\\nbase=\"aai540/maternal-risk/week5/\"\\nresp=s3.list_objects_v2(Bucket=bkt, Prefix=base, Delimiter=\"/\")\\nw5=sorted([cp[\"Prefix\"].rstrip(\"/\") for cp in resp.get(\"CommonPrefixes\",[])])[-1]\\n\\ntrk=json.loads(s3.get_object(Bucket=bkt, Key=f\"{w5}/team_tracker_update_week5.json\")[\"Body\"].read())\\nep=trk[\"endpoint\"]; dq=trk[\"schedules\"][\"data_quality\"]; mq=trk[\"schedules\"][\"model_quality\"]\\ndash=trk[\"cloudwatch\"][\"dashboard\"]; prefix=trk[\"cloudwatch\"][\"alarms_prefix\"]\\n\\n# stop & delete schedules\\nfor name in [dq, mq]:\\n    try: sm.stop_monitoring_schedule(MonitoringScheduleName=name)\\n    except: pass\\n    try: sm.delete_monitoring_schedule(MonitoringScheduleName=name)\\n    except: pass\\n\\n# delete endpoint (+ config)\\ntry: sm.delete_endpoint(EndpointName=ep)\\nexcept: pass\\ntry: sm.delete_endpoint_config(EndpointConfigName=ep)\\nexcept: pass\\n\\n# delete dashboard\\ntry: cw.delete_dashboards(DashboardNames=[dash])\\nexcept: pass\\n\\n# delete alarms with our prefix\\nalarms=cw.describe_alarms(AlarmNamePrefix=prefix).get(\"MetricAlarms\",[])\\nif alarms:\\n    cw.delete_alarms(AlarmNames=[a[\"AlarmName\"] for a in alarms])\\n\\nprint(\"🧹 Cleanup complete for run:\", w5)  \\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "AAI-540 — Week 5 Cleanup\n",
    "Deletes: monitoring schedules, endpoint (+config), CW dashboard & alarms.\n",
    "Artifacts remain in S3 for grading.\n",
    "\n",
    "\n",
    "import json, boto3\n",
    "from sagemaker.session import Session\n",
    "\n",
    "s3 = boto3.client(\"s3\"); sm = boto3.client(\"sagemaker\"); cw=boto3.client(\"cloudwatch\")\n",
    "bkt=Session().default_bucket()\n",
    "\n",
    "# latest Week-5 run\n",
    "base=\"aai540/maternal-risk/week5/\"\n",
    "resp=s3.list_objects_v2(Bucket=bkt, Prefix=base, Delimiter=\"/\")\n",
    "w5=sorted([cp[\"Prefix\"].rstrip(\"/\") for cp in resp.get(\"CommonPrefixes\",[])])[-1]\n",
    "\n",
    "trk=json.loads(s3.get_object(Bucket=bkt, Key=f\"{w5}/team_tracker_update_week5.json\")[\"Body\"].read())\n",
    "ep=trk[\"endpoint\"]; dq=trk[\"schedules\"][\"data_quality\"]; mq=trk[\"schedules\"][\"model_quality\"]\n",
    "dash=trk[\"cloudwatch\"][\"dashboard\"]; prefix=trk[\"cloudwatch\"][\"alarms_prefix\"]\n",
    "\n",
    "# stop & delete schedules\n",
    "for name in [dq, mq]:\n",
    "    try: sm.stop_monitoring_schedule(MonitoringScheduleName=name)\n",
    "    except: pass\n",
    "    try: sm.delete_monitoring_schedule(MonitoringScheduleName=name)\n",
    "    except: pass\n",
    "\n",
    "# delete endpoint (+ config)\n",
    "try: sm.delete_endpoint(EndpointName=ep)\n",
    "except: pass\n",
    "try: sm.delete_endpoint_config(EndpointConfigName=ep)\n",
    "except: pass\n",
    "\n",
    "# delete dashboard\n",
    "try: cw.delete_dashboards(DashboardNames=[dash])\n",
    "except: pass\n",
    "\n",
    "# delete alarms with our prefix\n",
    "alarms=cw.describe_alarms(AlarmNamePrefix=prefix).get(\"MetricAlarms\",[])\n",
    "if alarms:\n",
    "    cw.delete_alarms(AlarmNames=[a[\"AlarmName\"] for a in alarms])\n",
    "\n",
    "print(\"🧹 Cleanup complete for run:\", w5)  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1d5575-bb4a-4c90-8e4f-1202bc504bc8",
   "metadata": {},
   "source": [
    "## Week 6: Implement CI/CD Pipeline to automate training, evaluation, and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc13734-1e9f-43d2-9ea4-f17f00b48271",
   "metadata": {},
   "source": [
    "#### Setup + dataset resolver (auto-detect the latest CSV on S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fa15bcb5-f1f3-495b-894a-6ee175eff267",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T02:55:59.148419Z",
     "iopub.status.busy": "2025-10-08T02:55:59.148149Z",
     "iopub.status.idle": "2025-10-08T02:55:59.673335Z",
     "shell.execute_reply": "2025-10-08T02:55:59.672613Z",
     "shell.execute_reply.started": "2025-10-08T02:55:59.148398Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "# 1) Resolves latest Week-3 splits from S3 (train/val/test CSV URIs).\n",
    "# 2) Processing V4 (robust) normalizes schemas -> writes \"train.csv\" for each split.\n",
    "# 3) Trains XGBoost on train, validates on val.\n",
    "# 4) Evaluates the trained model on the test split inside an XGBoost ScriptProcessor.\n",
    "# 5) Reads AUC from evaluation.json; quality gate passes if AUC >= threshold.\n",
    "# 6) If pass, registers model into Model Package Group \"MaternalHealthRisk\".\n",
    "#\n",
    "# Key debugging upgrades:\n",
    "# - explicit LabRole used everywhere (jobs + pipeline resource).\n",
    "# - uses Join() for S3 paths (no string concat on pipeline variables).\n",
    "# - evaluation step runs in XGBoost image (so xgboost is importable).\n",
    "# - processing/evaluate scripts print helpful context to CloudWatch.\n",
    "# - caching disabled + V4 step names to force upload of fresh code.\n",
    "\n",
    "import os, time, boto3\n",
    "from pathlib import Path\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterString, ParameterFloat, ParameterInteger\n",
    ")\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.functions import JsonGet, Join\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "# environment\n",
    "boto_sess  = boto3.session.Session()\n",
    "sm_sess    = Session(boto_sess)\n",
    "region     = boto_sess.region_name\n",
    "bucket     = sm_sess.default_bucket()\n",
    "ROLE_ARN   = \"arn:aws:iam::849121223812:role/LabRole\"\n",
    "\n",
    "RUN_ID        = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "PIPELINE_NAME = \"MaternalHealthRisk-CICD\"\n",
    "BASE_PREFIX   = f\"aai540/maternal-risk/week6/{RUN_ID}\"\n",
    "\n",
    "Path(\"week6_code\").mkdir(parents=True, exist_ok=True)\n",
    "assert Path(\"week6_code/processing_v4.py\").exists()\n",
    "assert Path(\"week6_code/evaluate.py\").exists()\n",
    "\n",
    "# dataset resolver\n",
    "def resolve_week3_csv():\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    base = \"aai540/maternal-risk/week3/\"\n",
    "    resp = s3.list_objects_v2(Bucket=bucket, Prefix=base, Delimiter=\"/\")\n",
    "    runs = sorted(cp[\"Prefix\"].rstrip(\"/\") for cp in resp.get(\"CommonPrefixes\", []))\n",
    "    assert runs, \"No Week-3 runs found. Please run Week 3 prep first.\"\n",
    "    latest = runs[-1]\n",
    "    return (f\"s3://{bucket}/{latest}/train.csv\",\n",
    "            f\"s3://{bucket}/{latest}/val.csv\",\n",
    "            f\"s3://{bucket}/{latest}/test.csv\")\n",
    "\n",
    "TRAIN_S3, VAL_S3, TEST_S3 = resolve_week3_csv()\n",
    "\n",
    "# pipeline parameters (adds instance types/counts)\n",
    "p_base_prefix   = ParameterString(\"BasePrefix\", default_value=BASE_PREFIX)\n",
    "p_auc_gate      = ParameterFloat(\"AUCThreshold\", default_value=0.90)\n",
    "p_max_depth     = ParameterInteger(\"MaxDepth\", default_value=5)\n",
    "p_eta           = ParameterFloat(\"Eta\", default_value=0.2)\n",
    "p_rounds        = ParameterInteger(\"NumRounds\", default_value=200)\n",
    "\n",
    "# define the parameters that the error complained about (and wire them)\n",
    "p_proc_instance_type  = ParameterString(\"ProcInstanceType\",  default_value=\"ml.m5.large\")\n",
    "p_eval_instance_type  = ParameterString(\"EvalInstanceType\",  default_value=\"ml.m5.large\")\n",
    "p_train_instance_type = ParameterString(\"TrainInstanceType\", default_value=\"ml.m5.large\")\n",
    "p_train_instance_cnt  = ParameterInteger(\"TrainInstanceCount\", default_value=1)\n",
    "\n",
    "# images\n",
    "sk_image  = retrieve(\"sklearn\", region, version=\"1.2-1\")\n",
    "xgb_image = retrieve(\"xgboost\", region, version=\"1.7-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c676d6-ddb8-409c-87c0-83b8c831db0d",
   "metadata": {},
   "source": [
    "#### PROCESS V4 (no cache; V4 names; robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "861ef18f-d72f-4027-856f-1c163ee6785c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T02:56:34.719941Z",
     "iopub.status.busy": "2025-10-08T02:56:34.719533Z",
     "iopub.status.idle": "2025-10-08T02:56:34.726161Z",
     "shell.execute_reply": "2025-10-08T02:56:34.725177Z",
     "shell.execute_reply.started": "2025-10-08T02:56:34.719913Z"
    }
   },
   "outputs": [],
   "source": [
    "# PROCESS (v4)\n",
    "proc = ScriptProcessor(\n",
    "    image_uri=sk_image,\n",
    "    role=ROLE_ARN,\n",
    "    instance_type=p_proc_instance_type,   # wired to parameter\n",
    "    instance_count=1,\n",
    "    command=[\"python3\"],\n",
    "    sagemaker_session=sm_sess\n",
    ")\n",
    "\n",
    "def proc_step(name, s3_uri, split_name):\n",
    "    return ProcessingStep(\n",
    "        name=name,\n",
    "        processor=proc,\n",
    "        code=\"week6_code/processing_v4.py\",\n",
    "        inputs=[ProcessingInput(source=s3_uri, destination=\"/opt/ml/processing/input\")],\n",
    "        outputs=[ProcessingOutput(\n",
    "            output_name=\"out\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"s3://{bucket}/{BASE_PREFIX}/data/{split_name}\"\n",
    "        )]\n",
    "    )\n",
    "\n",
    "step_proc_train = proc_step(\"ProcessTrainV4\", TRAIN_S3, \"train\")\n",
    "step_proc_val   = proc_step(\"ProcessValV4\",   VAL_S3,   \"val\")\n",
    "step_proc_test  = proc_step(\"ProcessTestV4\",  TEST_S3,  \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6a980-e930-4ee0-8389-d45037c4e77f",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "429d28d8-95d0-4323-a756-a67e5873ba39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T02:57:18.052553Z",
     "iopub.status.busy": "2025-10-08T02:57:18.052279Z",
     "iopub.status.idle": "2025-10-08T02:57:18.065700Z",
     "shell.execute_reply": "2025-10-08T02:57:18.064928Z",
     "shell.execute_reply.started": "2025-10-08T02:57:18.052531Z"
    }
   },
   "outputs": [],
   "source": [
    "est = Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=ROLE_ARN,\n",
    "    instance_count=p_train_instance_cnt,      \n",
    "    instance_type=p_train_instance_type,      \n",
    "    sagemaker_session=sm_sess,\n",
    "    hyperparameters={\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"max_depth\": p_max_depth,\n",
    "        \"eta\": p_eta,\n",
    "        \"num_round\": p_rounds,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8\n",
    "    }\n",
    ")\n",
    "\n",
    "train_file_uri = Join(on=\"/\", values=[\n",
    "    step_proc_train.properties.ProcessingOutputConfig.Outputs[\"out\"].S3Output.S3Uri, \"train.csv\"\n",
    "])\n",
    "val_file_uri = Join(on=\"/\", values=[\n",
    "    step_proc_val.properties.ProcessingOutputConfig.Outputs[\"out\"].S3Output.S3Uri, \"train.csv\"\n",
    "])\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"TrainXGB\",\n",
    "    estimator=est,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(s3_data=train_file_uri, content_type=\"text/csv\"),\n",
    "        \"validation\": TrainingInput(s3_data=val_file_uri, content_type=\"text/csv\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718f2a1-20dc-43bf-92f8-b10b2d8b4068",
   "metadata": {},
   "source": [
    "#### EVALUATE (runs in XGBoost image; looks for train.csv first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "75284ea7-861c-4fc0-835e-61c19436da7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T02:58:40.373599Z",
     "iopub.status.busy": "2025-10-08T02:58:40.373313Z",
     "iopub.status.idle": "2025-10-08T02:58:40.380249Z",
     "shell.execute_reply": "2025-10-08T02:58:40.378959Z",
     "shell.execute_reply.started": "2025-10-08T02:58:40.373575Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_proc = ScriptProcessor(\n",
    "    image_uri=xgb_image,\n",
    "    role=ROLE_ARN,\n",
    "    instance_type=p_eval_instance_type,   # ← wired to parameter\n",
    "    instance_count=1,\n",
    "    command=[\"python3\"],\n",
    "    sagemaker_session=sm_sess\n",
    ")\n",
    "\n",
    "test_file_uri = Join(on=\"/\", values=[\n",
    "    step_proc_test.properties.ProcessingOutputConfig.Outputs[\"out\"].S3Output.S3Uri, \"train.csv\"\n",
    "])\n",
    "\n",
    "eval_out_s3  = f\"s3://{bucket}/{BASE_PREFIX}/evaluation\"\n",
    "eval_report  = PropertyFile(name=\"EvalReport\", output_name=\"eval\", path=\"evaluation.json\")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"EvaluateOnTestV2\",\n",
    "    processor=eval_proc,\n",
    "    code=\"week6_code/evaluate.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=test_file_uri,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(\n",
    "        output_name=\"eval\",\n",
    "        source=\"/opt/ml/processing/evaluation\",\n",
    "        destination=eval_out_s3\n",
    "    )],\n",
    "    property_files=[eval_report]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa3b0c6-4071-4fc1-81b9-7cbda7713c8e",
   "metadata": {},
   "source": [
    "#### QUALITY GATE + REGISTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "79b3d40b-d4ff-4179-926b-695cfe0072ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T02:59:01.379035Z",
     "iopub.status.busy": "2025-10-08T02:59:01.378223Z",
     "iopub.status.idle": "2025-10-08T02:59:01.393536Z",
     "shell.execute_reply": "2025-10-08T02:59:01.391709Z",
     "shell.execute_reply.started": "2025-10-08T02:59:01.379004Z"
    }
   },
   "outputs": [],
   "source": [
    "cond_ok = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(step_name=step_eval.name, property_file=eval_report,\n",
    "                 json_path=\"binary_classification_metrics.roc_auc\"),\n",
    "    right=p_auc_gate\n",
    ")\n",
    "\n",
    "metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=Join(on=\"/\", values=[eval_out_s3, \"evaluation.json\"]),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "register = RegisterModel(\n",
    "    name=\"RegisterBestXGB\",\n",
    "    estimator=est,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=\"MaternalHealthRisk\",\n",
    "    model_metrics=metrics\n",
    ")\n",
    "\n",
    "step_gate = ConditionStep(name=\"QualityGate\", conditions=[cond_ok], if_steps=[register], else_steps=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be711be0-f47a-47dc-a8e4-3cd87965edfa",
   "metadata": {},
   "source": [
    "#### assemble, upsert, start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5b9861ef-0d3b-4146-bb75-f0dc53fb2b64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T02:59:17.621256Z",
     "iopub.status.busy": "2025-10-08T02:59:17.620959Z",
     "iopub.status.idle": "2025-10-08T02:59:19.214170Z",
     "shell.execute_reply": "2025-10-08T02:59:19.213327Z",
     "shell.execute_reply.started": "2025-10-08T02:59:17.621234Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling pipeline definition…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting pipeline with LabRole…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting execution…\n",
      "Execution ARN: arn:aws:sagemaker:us-east-1:849121223812:pipeline/MaternalHealthRisk-CICD/execution/lniyz00otftm\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    parameters=[\n",
    "        p_base_prefix, p_auc_gate, p_max_depth, p_eta, p_rounds,\n",
    "        p_proc_instance_type, p_eval_instance_type,\n",
    "        p_train_instance_type, p_train_instance_cnt\n",
    "    ],\n",
    "    steps=[step_proc_train, step_proc_val, step_proc_test, step_train, step_eval, step_gate],\n",
    "    sagemaker_session=sm_sess\n",
    ")\n",
    "\n",
    "print(\"Compiling pipeline definition…\")\n",
    "_ = pipeline.definition()\n",
    "\n",
    "print(\"Upserting pipeline with LabRole…\")\n",
    "pipeline.upsert(role_arn=ROLE_ARN)\n",
    "\n",
    "print(\"Starting execution…\")\n",
    "execution = pipeline.start(\n",
    "    parameters={\n",
    "        \"BasePrefix\": BASE_PREFIX,\n",
    "        \"AUCThreshold\": 0.90,\n",
    "        \"MaxDepth\": 5,\n",
    "        \"Eta\": 0.2,\n",
    "        \"NumRounds\": 200,\n",
    "        \"ProcInstanceType\":  \"ml.m5.large\",\n",
    "        \"EvalInstanceType\":  \"ml.m5.large\",\n",
    "        \"TrainInstanceType\": \"ml.m5.large\",\n",
    "        \"TrainInstanceCount\": 1\n",
    "    },\n",
    "    execution_display_name=f\"run-{RUN_ID}\"\n",
    ")\n",
    "print(\"Execution ARN:\", execution.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93611095-87f2-49b2-b8f4-eac795fad5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bff55e-ca25-4142-8deb-fb81deb7f38e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
